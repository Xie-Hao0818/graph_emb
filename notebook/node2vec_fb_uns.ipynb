{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(899, device='cuda:1')\n",
      "torch.Size([2, 21026])\n",
      "Epoch: 001, Loss: 8.4387,AUC: 0.5571,AP: 0.5289\n",
      "Epoch: 002, Loss: 7.4364,AUC: 0.6068,AP: 0.5561\n",
      "Epoch: 003, Loss: 6.6292,AUC: 0.6467,AP: 0.5827\n",
      "Epoch: 004, Loss: 5.9091,AUC: 0.6784,AP: 0.6073\n",
      "Epoch: 005, Loss: 5.5818,AUC: 0.7061,AP: 0.6314\n",
      "Epoch: 006, Loss: 4.9642,AUC: 0.7309,AP: 0.6576\n",
      "Epoch: 007, Loss: 4.5740,AUC: 0.7521,AP: 0.6816\n",
      "Epoch: 008, Loss: 4.3000,AUC: 0.7700,AP: 0.7032\n",
      "Epoch: 009, Loss: 3.9761,AUC: 0.7844,AP: 0.7208\n",
      "Epoch: 010, Loss: 3.6946,AUC: 0.7954,AP: 0.7354\n",
      "Epoch: 011, Loss: 3.4465,AUC: 0.8031,AP: 0.7457\n",
      "Epoch: 012, Loss: 3.2752,AUC: 0.8082,AP: 0.7521\n",
      "Epoch: 013, Loss: 3.0655,AUC: 0.8128,AP: 0.7591\n",
      "Epoch: 014, Loss: 2.9085,AUC: 0.8179,AP: 0.7682\n",
      "Epoch: 015, Loss: 2.6947,AUC: 0.8224,AP: 0.7758\n",
      "Epoch: 016, Loss: 2.5518,AUC: 0.8262,AP: 0.7816\n",
      "Epoch: 017, Loss: 2.4428,AUC: 0.8299,AP: 0.7868\n",
      "Epoch: 018, Loss: 2.3172,AUC: 0.8334,AP: 0.7927\n",
      "Epoch: 019, Loss: 2.1957,AUC: 0.8368,AP: 0.7972\n",
      "Epoch: 020, Loss: 2.1144,AUC: 0.8403,AP: 0.8026\n",
      "Epoch: 021, Loss: 2.0216,AUC: 0.8445,AP: 0.8077\n",
      "Epoch: 022, Loss: 1.9339,AUC: 0.8493,AP: 0.8144\n",
      "Epoch: 023, Loss: 1.8383,AUC: 0.8547,AP: 0.8217\n",
      "Epoch: 024, Loss: 1.7757,AUC: 0.8595,AP: 0.8282\n",
      "Epoch: 025, Loss: 1.7103,AUC: 0.8640,AP: 0.8347\n",
      "Epoch: 026, Loss: 1.6621,AUC: 0.8686,AP: 0.8410\n",
      "Epoch: 027, Loss: 1.5925,AUC: 0.8737,AP: 0.8476\n",
      "Epoch: 028, Loss: 1.5446,AUC: 0.8779,AP: 0.8533\n",
      "Epoch: 029, Loss: 1.4890,AUC: 0.8821,AP: 0.8590\n",
      "Epoch: 030, Loss: 1.4571,AUC: 0.8863,AP: 0.8653\n",
      "Epoch: 031, Loss: 1.3978,AUC: 0.8909,AP: 0.8718\n",
      "Epoch: 032, Loss: 1.3890,AUC: 0.8951,AP: 0.8771\n",
      "Epoch: 033, Loss: 1.3535,AUC: 0.8989,AP: 0.8823\n",
      "Epoch: 034, Loss: 1.3058,AUC: 0.9028,AP: 0.8877\n",
      "Epoch: 035, Loss: 1.2896,AUC: 0.9057,AP: 0.8917\n",
      "Epoch: 036, Loss: 1.2357,AUC: 0.9086,AP: 0.8959\n",
      "Epoch: 037, Loss: 1.2195,AUC: 0.9109,AP: 0.8995\n",
      "Epoch: 038, Loss: 1.2106,AUC: 0.9128,AP: 0.9027\n",
      "Epoch: 039, Loss: 1.1963,AUC: 0.9157,AP: 0.9076\n",
      "Epoch: 040, Loss: 1.1831,AUC: 0.9194,AP: 0.9125\n",
      "Epoch: 041, Loss: 1.1565,AUC: 0.9218,AP: 0.9158\n",
      "Epoch: 042, Loss: 1.1432,AUC: 0.9237,AP: 0.9182\n",
      "Epoch: 043, Loss: 1.1341,AUC: 0.9252,AP: 0.9202\n",
      "Epoch: 044, Loss: 1.1129,AUC: 0.9266,AP: 0.9225\n",
      "Epoch: 045, Loss: 1.1086,AUC: 0.9285,AP: 0.9254\n",
      "Epoch: 046, Loss: 1.0897,AUC: 0.9305,AP: 0.9281\n",
      "Epoch: 047, Loss: 1.0725,AUC: 0.9321,AP: 0.9306\n",
      "Epoch: 048, Loss: 1.0719,AUC: 0.9332,AP: 0.9328\n",
      "Epoch: 049, Loss: 1.0702,AUC: 0.9347,AP: 0.9351\n",
      "Epoch: 050, Loss: 1.0506,AUC: 0.9366,AP: 0.9377\n",
      "Epoch: 051, Loss: 1.0492,AUC: 0.9383,AP: 0.9400\n",
      "Epoch: 052, Loss: 1.0493,AUC: 0.9398,AP: 0.9420\n",
      "Epoch: 053, Loss: 1.0305,AUC: 0.9408,AP: 0.9437\n",
      "Epoch: 054, Loss: 1.0266,AUC: 0.9417,AP: 0.9451\n",
      "Epoch: 055, Loss: 1.0222,AUC: 0.9423,AP: 0.9462\n",
      "Epoch: 056, Loss: 1.0073,AUC: 0.9427,AP: 0.9470\n",
      "Epoch: 057, Loss: 1.0137,AUC: 0.9436,AP: 0.9481\n",
      "Epoch: 058, Loss: 1.0121,AUC: 0.9445,AP: 0.9489\n",
      "Epoch: 059, Loss: 0.9951,AUC: 0.9458,AP: 0.9509\n",
      "Epoch: 060, Loss: 0.9805,AUC: 0.9468,AP: 0.9522\n",
      "Epoch: 061, Loss: 0.9924,AUC: 0.9477,AP: 0.9531\n",
      "Epoch: 062, Loss: 0.9973,AUC: 0.9484,AP: 0.9537\n",
      "Epoch: 063, Loss: 0.9993,AUC: 0.9495,AP: 0.9547\n",
      "Epoch: 064, Loss: 0.9844,AUC: 0.9498,AP: 0.9552\n",
      "Epoch: 065, Loss: 0.9814,AUC: 0.9501,AP: 0.9556\n",
      "Epoch: 066, Loss: 0.9738,AUC: 0.9507,AP: 0.9564\n",
      "Epoch: 067, Loss: 0.9798,AUC: 0.9512,AP: 0.9571\n",
      "Epoch: 068, Loss: 0.9719,AUC: 0.9517,AP: 0.9577\n",
      "Epoch: 069, Loss: 0.9765,AUC: 0.9516,AP: 0.9579\n",
      "Epoch: 070, Loss: 0.9778,AUC: 0.9518,AP: 0.9585\n",
      "Epoch: 071, Loss: 0.9608,AUC: 0.9521,AP: 0.9589\n",
      "Epoch: 072, Loss: 0.9647,AUC: 0.9523,AP: 0.9593\n",
      "Epoch: 073, Loss: 0.9764,AUC: 0.9525,AP: 0.9597\n",
      "Epoch: 074, Loss: 0.9643,AUC: 0.9531,AP: 0.9601\n",
      "Epoch: 075, Loss: 0.9645,AUC: 0.9535,AP: 0.9605\n",
      "Epoch: 076, Loss: 0.9698,AUC: 0.9540,AP: 0.9610\n",
      "Epoch: 077, Loss: 0.9592,AUC: 0.9545,AP: 0.9617\n",
      "Epoch: 078, Loss: 0.9572,AUC: 0.9546,AP: 0.9624\n",
      "Epoch: 079, Loss: 0.9540,AUC: 0.9547,AP: 0.9625\n",
      "Epoch: 080, Loss: 0.9552,AUC: 0.9550,AP: 0.9627\n",
      "Epoch: 081, Loss: 0.9572,AUC: 0.9554,AP: 0.9633\n",
      "Epoch: 082, Loss: 0.9462,AUC: 0.9556,AP: 0.9637\n",
      "Epoch: 083, Loss: 0.9496,AUC: 0.9554,AP: 0.9635\n",
      "Epoch: 084, Loss: 0.9506,AUC: 0.9555,AP: 0.9637\n",
      "Epoch: 085, Loss: 0.9517,AUC: 0.9560,AP: 0.9645\n",
      "Epoch: 086, Loss: 0.9441,AUC: 0.9566,AP: 0.9652\n",
      "Epoch: 087, Loss: 0.9542,AUC: 0.9569,AP: 0.9655\n",
      "Epoch: 088, Loss: 0.9508,AUC: 0.9566,AP: 0.9653\n",
      "Epoch: 089, Loss: 0.9519,AUC: 0.9564,AP: 0.9654\n",
      "Epoch: 090, Loss: 0.9527,AUC: 0.9565,AP: 0.9655\n",
      "Epoch: 091, Loss: 0.9545,AUC: 0.9564,AP: 0.9653\n",
      "Epoch: 092, Loss: 0.9495,AUC: 0.9562,AP: 0.9650\n",
      "Epoch: 093, Loss: 0.9433,AUC: 0.9563,AP: 0.9652\n",
      "Epoch: 094, Loss: 0.9505,AUC: 0.9561,AP: 0.9652\n",
      "Epoch: 095, Loss: 0.9516,AUC: 0.9560,AP: 0.9652\n",
      "Epoch: 096, Loss: 0.9452,AUC: 0.9563,AP: 0.9658\n",
      "Epoch: 097, Loss: 0.9560,AUC: 0.9564,AP: 0.9660\n",
      "Epoch: 098, Loss: 0.9398,AUC: 0.9564,AP: 0.9661\n",
      "Epoch: 099, Loss: 0.9440,AUC: 0.9564,AP: 0.9661\n",
      "Epoch: 100, Loss: 0.9462,AUC: 0.9569,AP: 0.9666\n",
      "Epoch: 101, Loss: 0.9436,AUC: 0.9574,AP: 0.9671\n",
      "Epoch: 102, Loss: 0.9437,AUC: 0.9577,AP: 0.9672\n",
      "Epoch: 103, Loss: 0.9387,AUC: 0.9576,AP: 0.9668\n",
      "Epoch: 104, Loss: 0.9415,AUC: 0.9571,AP: 0.9665\n",
      "Epoch: 105, Loss: 0.9439,AUC: 0.9568,AP: 0.9666\n",
      "Epoch: 106, Loss: 0.9415,AUC: 0.9569,AP: 0.9670\n",
      "Epoch: 107, Loss: 0.9433,AUC: 0.9571,AP: 0.9672\n",
      "Epoch: 108, Loss: 0.9435,AUC: 0.9571,AP: 0.9670\n",
      "Epoch: 109, Loss: 0.9389,AUC: 0.9569,AP: 0.9671\n",
      "Epoch: 110, Loss: 0.9374,AUC: 0.9571,AP: 0.9674\n",
      "Epoch: 111, Loss: 0.9511,AUC: 0.9574,AP: 0.9677\n",
      "Epoch: 112, Loss: 0.9358,AUC: 0.9575,AP: 0.9676\n",
      "Epoch: 113, Loss: 0.9432,AUC: 0.9578,AP: 0.9676\n",
      "Epoch: 114, Loss: 0.9346,AUC: 0.9576,AP: 0.9673\n",
      "Epoch: 115, Loss: 0.9325,AUC: 0.9569,AP: 0.9668\n",
      "Epoch: 116, Loss: 0.9376,AUC: 0.9569,AP: 0.9671\n",
      "Epoch: 117, Loss: 0.9216,AUC: 0.9571,AP: 0.9673\n",
      "Epoch: 118, Loss: 0.9350,AUC: 0.9569,AP: 0.9671\n",
      "Epoch: 119, Loss: 0.9383,AUC: 0.9571,AP: 0.9673\n",
      "Epoch: 120, Loss: 0.9484,AUC: 0.9568,AP: 0.9669\n",
      "Epoch: 121, Loss: 0.9352,AUC: 0.9572,AP: 0.9669\n",
      "Epoch: 122, Loss: 0.9442,AUC: 0.9569,AP: 0.9667\n",
      "Epoch: 123, Loss: 0.9332,AUC: 0.9568,AP: 0.9669\n",
      "Epoch: 124, Loss: 0.9328,AUC: 0.9566,AP: 0.9667\n",
      "Epoch: 125, Loss: 0.9302,AUC: 0.9564,AP: 0.9667\n",
      "Epoch: 126, Loss: 0.9415,AUC: 0.9562,AP: 0.9668\n",
      "Epoch: 127, Loss: 0.9399,AUC: 0.9568,AP: 0.9671\n",
      "Epoch: 128, Loss: 0.9389,AUC: 0.9568,AP: 0.9670\n",
      "Epoch: 129, Loss: 0.9362,AUC: 0.9567,AP: 0.9670\n",
      "Epoch: 130, Loss: 0.9384,AUC: 0.9571,AP: 0.9670\n",
      "Epoch: 131, Loss: 0.9305,AUC: 0.9567,AP: 0.9667\n",
      "Epoch: 132, Loss: 0.9428,AUC: 0.9572,AP: 0.9672\n",
      "Epoch: 133, Loss: 0.9347,AUC: 0.9582,AP: 0.9680\n",
      "Epoch: 134, Loss: 0.9461,AUC: 0.9590,AP: 0.9686\n",
      "Epoch: 135, Loss: 0.9403,AUC: 0.9585,AP: 0.9681\n",
      "Epoch: 136, Loss: 0.9354,AUC: 0.9573,AP: 0.9672\n",
      "Epoch: 137, Loss: 0.9405,AUC: 0.9567,AP: 0.9668\n",
      "Epoch: 138, Loss: 0.9426,AUC: 0.9573,AP: 0.9673\n",
      "Epoch: 139, Loss: 0.9338,AUC: 0.9571,AP: 0.9672\n",
      "Epoch: 140, Loss: 0.9422,AUC: 0.9568,AP: 0.9670\n",
      "Epoch: 141, Loss: 0.9291,AUC: 0.9561,AP: 0.9664\n",
      "Epoch: 142, Loss: 0.9350,AUC: 0.9554,AP: 0.9660\n",
      "Epoch: 143, Loss: 0.9371,AUC: 0.9551,AP: 0.9657\n",
      "Epoch: 144, Loss: 0.9301,AUC: 0.9559,AP: 0.9663\n",
      "Epoch: 145, Loss: 0.9388,AUC: 0.9565,AP: 0.9667\n",
      "Epoch: 146, Loss: 0.9337,AUC: 0.9572,AP: 0.9674\n",
      "Epoch: 147, Loss: 0.9316,AUC: 0.9565,AP: 0.9668\n",
      "Epoch: 148, Loss: 0.9338,AUC: 0.9566,AP: 0.9672\n",
      "Epoch: 149, Loss: 0.9417,AUC: 0.9578,AP: 0.9678\n",
      "Epoch: 150, Loss: 0.9426,AUC: 0.9590,AP: 0.9688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m1.pt')\n",
    "data = data.to(device)\n",
    "data.num_nodes = 899\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "model = Node2Vec(\n",
    "    edge_index=train_data.edge_index,\n",
    "    embedding_dim=128,\n",
    "    walk_length=20,\n",
    "    context_size=10,\n",
    "    walks_per_node=10,\n",
    "    num_negative_samples=1,\n",
    "    p=0.5,  # p=q=1时，退化为DeepWalk\n",
    "    q=1.0,\n",
    "    sparse=False,  # 该数据集为密集数据集\n",
    ").to(device)\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "print(torch.max(train_data.edge_index[0]))\n",
    "print(train_data.edge_index.shape)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data\n",
    "    z = model()  # 输出潜在空间的嵌入向量\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = model()\n",
    "z1.shape\n",
    "# 节点数实际是899，这里是由于图节点编号从1开始，而列表从0开始，所以对于索引为0的特征向量应该不会被用到"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.2721,  0.0995, -0.1786, -0.0347, -0.1345,  0.3340,  0.1330,  0.2713,\n         0.2525, -0.0428,  0.0860, -0.1148, -0.1609,  0.0208, -0.1687, -0.3123,\n         0.2585, -0.3620,  0.0265, -0.0585,  0.2145,  0.0852, -0.0614,  0.1562,\n        -0.0185,  0.1677,  0.0349, -0.3967, -0.0485, -0.0065, -0.0039,  0.0877,\n        -0.5729, -0.4422, -0.1665, -0.1857, -0.1774, -0.1686,  0.2383, -0.1294,\n         0.1029, -0.0528, -0.2724, -0.1697,  0.1196,  0.2347,  0.1430,  0.1387,\n        -0.1743, -0.2277,  0.1404,  0.2274, -0.0846,  0.0153,  0.0104,  0.1668,\n         0.0357,  0.3507, -0.0098, -0.1247, -0.0062, -0.0745, -0.0884, -0.0454,\n        -0.0654, -0.1711,  0.0819,  0.2387,  0.1617,  0.0998,  0.1661,  0.1250,\n         0.1807, -0.1691,  0.2505,  0.0108,  0.3105,  0.0381, -0.0101,  0.5680,\n         0.0203, -0.0370, -0.3498, -0.5207,  0.0874, -0.0792,  0.2913,  0.0314,\n         0.3772, -0.1924, -0.0456,  0.0973,  0.0257,  0.1601,  0.0082, -0.3676,\n        -0.1476,  0.4315, -0.2282, -0.1975,  0.0679,  0.1498, -0.1657, -0.1015,\n         0.2491,  0.2847, -0.1142, -0.1348, -0.0022, -0.0016,  0.7028,  0.1719,\n        -0.0972,  0.2447,  0.1432, -0.0581,  0.1115, -0.0862,  0.0292,  0.0138,\n        -0.0873, -0.2496, -0.0386, -0.1221,  0.0356,  0.2890, -0.0236, -0.2173],\n       device='cuda:1', grad_fn=<SelectBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(897, device='cuda:1')\n",
      "torch.Size([2, 4762])\n",
      "Epoch: 001, Loss: 7.4532,AUC: 0.6031,AP: 0.5572\n",
      "Epoch: 002, Loss: 6.2507,AUC: 0.6653,AP: 0.5956\n",
      "Epoch: 003, Loss: 5.5973,AUC: 0.7076,AP: 0.6283\n",
      "Epoch: 004, Loss: 5.3504,AUC: 0.7382,AP: 0.6556\n",
      "Epoch: 005, Loss: 4.9243,AUC: 0.7594,AP: 0.6737\n",
      "Epoch: 006, Loss: 4.6742,AUC: 0.7752,AP: 0.6903\n",
      "Epoch: 007, Loss: 4.2765,AUC: 0.7859,AP: 0.7047\n",
      "Epoch: 008, Loss: 4.0335,AUC: 0.7958,AP: 0.7180\n",
      "Epoch: 009, Loss: 3.7483,AUC: 0.8051,AP: 0.7322\n",
      "Epoch: 010, Loss: 3.5175,AUC: 0.8146,AP: 0.7444\n",
      "Epoch: 011, Loss: 3.2648,AUC: 0.8209,AP: 0.7530\n",
      "Epoch: 012, Loss: 3.1859,AUC: 0.8272,AP: 0.7625\n",
      "Epoch: 013, Loss: 2.9542,AUC: 0.8338,AP: 0.7720\n",
      "Epoch: 014, Loss: 2.8077,AUC: 0.8396,AP: 0.7796\n",
      "Epoch: 015, Loss: 2.6489,AUC: 0.8434,AP: 0.7865\n",
      "Epoch: 016, Loss: 2.5250,AUC: 0.8481,AP: 0.7920\n",
      "Epoch: 017, Loss: 2.4336,AUC: 0.8523,AP: 0.7981\n",
      "Epoch: 018, Loss: 2.3137,AUC: 0.8572,AP: 0.8042\n",
      "Epoch: 019, Loss: 2.2464,AUC: 0.8619,AP: 0.8110\n",
      "Epoch: 020, Loss: 2.1378,AUC: 0.8667,AP: 0.8170\n",
      "Epoch: 021, Loss: 2.0454,AUC: 0.8701,AP: 0.8230\n",
      "Epoch: 022, Loss: 1.9616,AUC: 0.8740,AP: 0.8273\n",
      "Epoch: 023, Loss: 1.8635,AUC: 0.8768,AP: 0.8316\n",
      "Epoch: 024, Loss: 1.8228,AUC: 0.8790,AP: 0.8343\n",
      "Epoch: 025, Loss: 1.7628,AUC: 0.8822,AP: 0.8394\n",
      "Epoch: 026, Loss: 1.6804,AUC: 0.8864,AP: 0.8458\n",
      "Epoch: 027, Loss: 1.6285,AUC: 0.8900,AP: 0.8499\n",
      "Epoch: 028, Loss: 1.5972,AUC: 0.8912,AP: 0.8514\n",
      "Epoch: 029, Loss: 1.5397,AUC: 0.8927,AP: 0.8534\n",
      "Epoch: 030, Loss: 1.4862,AUC: 0.8946,AP: 0.8557\n",
      "Epoch: 031, Loss: 1.4498,AUC: 0.8963,AP: 0.8589\n",
      "Epoch: 032, Loss: 1.3996,AUC: 0.8984,AP: 0.8651\n",
      "Epoch: 033, Loss: 1.3693,AUC: 0.8998,AP: 0.8701\n",
      "Epoch: 034, Loss: 1.3475,AUC: 0.9011,AP: 0.8729\n",
      "Epoch: 035, Loss: 1.2977,AUC: 0.9032,AP: 0.8773\n",
      "Epoch: 036, Loss: 1.2749,AUC: 0.9051,AP: 0.8806\n",
      "Epoch: 037, Loss: 1.2301,AUC: 0.9072,AP: 0.8842\n",
      "Epoch: 038, Loss: 1.2106,AUC: 0.9074,AP: 0.8856\n",
      "Epoch: 039, Loss: 1.2065,AUC: 0.9088,AP: 0.8887\n",
      "Epoch: 040, Loss: 1.1805,AUC: 0.9120,AP: 0.8929\n",
      "Epoch: 041, Loss: 1.1497,AUC: 0.9151,AP: 0.8980\n",
      "Epoch: 042, Loss: 1.1335,AUC: 0.9180,AP: 0.9030\n",
      "Epoch: 043, Loss: 1.1331,AUC: 0.9228,AP: 0.9075\n",
      "Epoch: 044, Loss: 1.0858,AUC: 0.9245,AP: 0.9107\n",
      "Epoch: 045, Loss: 1.0807,AUC: 0.9256,AP: 0.9121\n",
      "Epoch: 046, Loss: 1.0539,AUC: 0.9264,AP: 0.9134\n",
      "Epoch: 047, Loss: 1.0491,AUC: 0.9274,AP: 0.9149\n",
      "Epoch: 048, Loss: 1.0447,AUC: 0.9283,AP: 0.9163\n",
      "Epoch: 049, Loss: 1.0190,AUC: 0.9295,AP: 0.9179\n",
      "Epoch: 050, Loss: 1.0167,AUC: 0.9306,AP: 0.9191\n",
      "Epoch: 051, Loss: 1.0128,AUC: 0.9318,AP: 0.9217\n",
      "Epoch: 052, Loss: 0.9973,AUC: 0.9319,AP: 0.9229\n",
      "Epoch: 053, Loss: 0.9723,AUC: 0.9313,AP: 0.9231\n",
      "Epoch: 054, Loss: 0.9766,AUC: 0.9310,AP: 0.9234\n",
      "Epoch: 055, Loss: 0.9665,AUC: 0.9317,AP: 0.9246\n",
      "Epoch: 056, Loss: 0.9570,AUC: 0.9324,AP: 0.9263\n",
      "Epoch: 057, Loss: 0.9497,AUC: 0.9324,AP: 0.9266\n",
      "Epoch: 058, Loss: 0.9526,AUC: 0.9327,AP: 0.9270\n",
      "Epoch: 059, Loss: 0.9388,AUC: 0.9336,AP: 0.9280\n",
      "Epoch: 060, Loss: 0.9390,AUC: 0.9353,AP: 0.9287\n",
      "Epoch: 061, Loss: 0.9201,AUC: 0.9355,AP: 0.9299\n",
      "Epoch: 062, Loss: 0.9203,AUC: 0.9353,AP: 0.9301\n",
      "Epoch: 063, Loss: 0.9305,AUC: 0.9355,AP: 0.9304\n",
      "Epoch: 064, Loss: 0.9204,AUC: 0.9370,AP: 0.9316\n",
      "Epoch: 065, Loss: 0.9031,AUC: 0.9377,AP: 0.9327\n",
      "Epoch: 066, Loss: 0.9013,AUC: 0.9377,AP: 0.9335\n",
      "Epoch: 067, Loss: 0.8948,AUC: 0.9380,AP: 0.9346\n",
      "Epoch: 068, Loss: 0.9043,AUC: 0.9371,AP: 0.9342\n",
      "Epoch: 069, Loss: 0.8886,AUC: 0.9364,AP: 0.9338\n",
      "Epoch: 070, Loss: 0.8864,AUC: 0.9353,AP: 0.9333\n",
      "Epoch: 071, Loss: 0.8715,AUC: 0.9357,AP: 0.9339\n",
      "Epoch: 072, Loss: 0.8862,AUC: 0.9353,AP: 0.9344\n",
      "Epoch: 073, Loss: 0.8904,AUC: 0.9357,AP: 0.9352\n",
      "Epoch: 074, Loss: 0.8657,AUC: 0.9364,AP: 0.9363\n",
      "Epoch: 075, Loss: 0.8594,AUC: 0.9367,AP: 0.9370\n",
      "Epoch: 076, Loss: 0.8585,AUC: 0.9373,AP: 0.9375\n",
      "Epoch: 077, Loss: 0.8837,AUC: 0.9375,AP: 0.9376\n",
      "Epoch: 078, Loss: 0.8670,AUC: 0.9377,AP: 0.9382\n",
      "Epoch: 079, Loss: 0.8550,AUC: 0.9368,AP: 0.9379\n",
      "Epoch: 080, Loss: 0.8602,AUC: 0.9370,AP: 0.9403\n",
      "Epoch: 081, Loss: 0.8509,AUC: 0.9362,AP: 0.9406\n",
      "Epoch: 082, Loss: 0.8641,AUC: 0.9354,AP: 0.9406\n",
      "Epoch: 083, Loss: 0.8672,AUC: 0.9363,AP: 0.9432\n",
      "Epoch: 084, Loss: 0.8647,AUC: 0.9367,AP: 0.9433\n",
      "Epoch: 085, Loss: 0.8629,AUC: 0.9369,AP: 0.9444\n",
      "Epoch: 086, Loss: 0.8431,AUC: 0.9367,AP: 0.9453\n",
      "Epoch: 087, Loss: 0.8611,AUC: 0.9364,AP: 0.9459\n",
      "Epoch: 088, Loss: 0.8541,AUC: 0.9366,AP: 0.9478\n",
      "Epoch: 089, Loss: 0.8401,AUC: 0.9367,AP: 0.9482\n",
      "Epoch: 090, Loss: 0.8485,AUC: 0.9376,AP: 0.9491\n",
      "Epoch: 091, Loss: 0.8385,AUC: 0.9378,AP: 0.9495\n",
      "Epoch: 092, Loss: 0.8541,AUC: 0.9382,AP: 0.9495\n",
      "Epoch: 093, Loss: 0.8432,AUC: 0.9392,AP: 0.9505\n",
      "Epoch: 094, Loss: 0.8550,AUC: 0.9402,AP: 0.9533\n",
      "Epoch: 095, Loss: 0.8391,AUC: 0.9402,AP: 0.9541\n",
      "Epoch: 096, Loss: 0.8528,AUC: 0.9400,AP: 0.9542\n",
      "Epoch: 097, Loss: 0.8488,AUC: 0.9397,AP: 0.9544\n",
      "Epoch: 098, Loss: 0.8488,AUC: 0.9397,AP: 0.9551\n",
      "Epoch: 099, Loss: 0.8309,AUC: 0.9407,AP: 0.9564\n",
      "Epoch: 100, Loss: 0.8271,AUC: 0.9410,AP: 0.9569\n",
      "Epoch: 101, Loss: 0.8358,AUC: 0.9417,AP: 0.9582\n",
      "Epoch: 102, Loss: 0.8277,AUC: 0.9420,AP: 0.9587\n",
      "Epoch: 103, Loss: 0.8356,AUC: 0.9417,AP: 0.9585\n",
      "Epoch: 104, Loss: 0.8337,AUC: 0.9417,AP: 0.9590\n",
      "Epoch: 105, Loss: 0.8354,AUC: 0.9420,AP: 0.9594\n",
      "Epoch: 106, Loss: 0.8375,AUC: 0.9428,AP: 0.9601\n",
      "Epoch: 107, Loss: 0.8409,AUC: 0.9430,AP: 0.9604\n",
      "Epoch: 108, Loss: 0.8309,AUC: 0.9419,AP: 0.9599\n",
      "Epoch: 109, Loss: 0.8345,AUC: 0.9420,AP: 0.9600\n",
      "Epoch: 110, Loss: 0.8241,AUC: 0.9425,AP: 0.9604\n",
      "Epoch: 111, Loss: 0.8382,AUC: 0.9425,AP: 0.9604\n",
      "Epoch: 112, Loss: 0.8233,AUC: 0.9422,AP: 0.9604\n",
      "Epoch: 113, Loss: 0.8371,AUC: 0.9430,AP: 0.9608\n",
      "Epoch: 114, Loss: 0.8325,AUC: 0.9430,AP: 0.9610\n",
      "Epoch: 115, Loss: 0.8274,AUC: 0.9437,AP: 0.9616\n",
      "Epoch: 116, Loss: 0.8285,AUC: 0.9444,AP: 0.9622\n",
      "Epoch: 117, Loss: 0.8323,AUC: 0.9448,AP: 0.9626\n",
      "Epoch: 118, Loss: 0.8187,AUC: 0.9452,AP: 0.9630\n",
      "Epoch: 119, Loss: 0.8239,AUC: 0.9447,AP: 0.9628\n",
      "Epoch: 120, Loss: 0.8207,AUC: 0.9448,AP: 0.9630\n",
      "Epoch: 121, Loss: 0.8174,AUC: 0.9451,AP: 0.9632\n",
      "Epoch: 122, Loss: 0.8277,AUC: 0.9452,AP: 0.9634\n",
      "Epoch: 123, Loss: 0.8202,AUC: 0.9448,AP: 0.9631\n",
      "Epoch: 124, Loss: 0.8415,AUC: 0.9440,AP: 0.9626\n",
      "Epoch: 125, Loss: 0.8327,AUC: 0.9425,AP: 0.9619\n",
      "Epoch: 126, Loss: 0.8247,AUC: 0.9416,AP: 0.9615\n",
      "Epoch: 127, Loss: 0.8327,AUC: 0.9414,AP: 0.9614\n",
      "Epoch: 128, Loss: 0.8386,AUC: 0.9400,AP: 0.9603\n",
      "Epoch: 129, Loss: 0.8201,AUC: 0.9387,AP: 0.9594\n",
      "Epoch: 130, Loss: 0.8334,AUC: 0.9381,AP: 0.9590\n",
      "Epoch: 131, Loss: 0.8225,AUC: 0.9388,AP: 0.9595\n",
      "Epoch: 132, Loss: 0.8291,AUC: 0.9396,AP: 0.9600\n",
      "Epoch: 133, Loss: 0.8339,AUC: 0.9409,AP: 0.9607\n",
      "Epoch: 134, Loss: 0.8259,AUC: 0.9411,AP: 0.9610\n",
      "Epoch: 135, Loss: 0.8339,AUC: 0.9400,AP: 0.9604\n",
      "Epoch: 136, Loss: 0.8268,AUC: 0.9415,AP: 0.9611\n",
      "Epoch: 137, Loss: 0.8364,AUC: 0.9412,AP: 0.9607\n",
      "Epoch: 138, Loss: 0.8260,AUC: 0.9406,AP: 0.9602\n",
      "Epoch: 139, Loss: 0.8194,AUC: 0.9409,AP: 0.9602\n",
      "Epoch: 140, Loss: 0.8153,AUC: 0.9403,AP: 0.9601\n",
      "Epoch: 141, Loss: 0.8130,AUC: 0.9396,AP: 0.9596\n",
      "Epoch: 142, Loss: 0.8227,AUC: 0.9373,AP: 0.9586\n",
      "Epoch: 143, Loss: 0.8205,AUC: 0.9356,AP: 0.9578\n",
      "Epoch: 144, Loss: 0.8292,AUC: 0.9332,AP: 0.9566\n",
      "Epoch: 145, Loss: 0.8198,AUC: 0.9319,AP: 0.9559\n",
      "Epoch: 146, Loss: 0.8309,AUC: 0.9319,AP: 0.9560\n",
      "Epoch: 147, Loss: 0.8223,AUC: 0.9321,AP: 0.9562\n",
      "Epoch: 148, Loss: 0.8163,AUC: 0.9327,AP: 0.9566\n",
      "Epoch: 149, Loss: 0.8291,AUC: 0.9340,AP: 0.9573\n",
      "Epoch: 150, Loss: 0.8301,AUC: 0.9352,AP: 0.9581\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m2.pt')\n",
    "data = data.to(device)\n",
    "data.num_nodes = 899\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "model = Node2Vec(\n",
    "    edge_index=train_data.edge_index,\n",
    "    embedding_dim=128,\n",
    "    walk_length=20,\n",
    "    context_size=10,\n",
    "    walks_per_node=10,\n",
    "    num_negative_samples=1,\n",
    "    p=0.5,  # p=q=1时，退化为DeepWalk\n",
    "    q=1.0,\n",
    "    sparse=False,  # 该数据集为密集数据集\n",
    ").to(device)\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "print(torch.max(train_data.edge_index[0]))\n",
    "print(train_data.edge_index.shape)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data\n",
    "    z = model()  # 输出潜在空间的嵌入向量\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([898, 128])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = model()\n",
    "z2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_2 = torch.zeros((2, 128)).to(device)\n",
    "z2 = torch.cat([z2, zero_2], 0)\n",
    "z2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(897, device='cuda:1')\n",
      "torch.Size([2, 3220])\n",
      "Epoch: 001, Loss: 6.7476,AUC: 0.5529,AP: 0.5215\n",
      "Epoch: 002, Loss: 5.7288,AUC: 0.6123,AP: 0.5529\n",
      "Epoch: 003, Loss: 5.2171,AUC: 0.6597,AP: 0.5801\n",
      "Epoch: 004, Loss: 4.8983,AUC: 0.6985,AP: 0.6097\n",
      "Epoch: 005, Loss: 4.4788,AUC: 0.7264,AP: 0.6338\n",
      "Epoch: 006, Loss: 4.2856,AUC: 0.7464,AP: 0.6522\n",
      "Epoch: 007, Loss: 3.9368,AUC: 0.7595,AP: 0.6689\n",
      "Epoch: 008, Loss: 3.7693,AUC: 0.7711,AP: 0.6836\n",
      "Epoch: 009, Loss: 3.4885,AUC: 0.7800,AP: 0.6935\n",
      "Epoch: 010, Loss: 3.2874,AUC: 0.7864,AP: 0.7038\n",
      "Epoch: 011, Loss: 3.0603,AUC: 0.7916,AP: 0.7118\n",
      "Epoch: 012, Loss: 2.9724,AUC: 0.7944,AP: 0.7174\n",
      "Epoch: 013, Loss: 2.7783,AUC: 0.8022,AP: 0.7301\n",
      "Epoch: 014, Loss: 2.6616,AUC: 0.8094,AP: 0.7403\n",
      "Epoch: 015, Loss: 2.4965,AUC: 0.8142,AP: 0.7492\n",
      "Epoch: 016, Loss: 2.3796,AUC: 0.8187,AP: 0.7556\n",
      "Epoch: 017, Loss: 2.2688,AUC: 0.8214,AP: 0.7615\n",
      "Epoch: 018, Loss: 2.1738,AUC: 0.8248,AP: 0.7687\n",
      "Epoch: 019, Loss: 2.1052,AUC: 0.8302,AP: 0.7768\n",
      "Epoch: 020, Loss: 2.0102,AUC: 0.8371,AP: 0.7881\n",
      "Epoch: 021, Loss: 1.9183,AUC: 0.8418,AP: 0.7938\n",
      "Epoch: 022, Loss: 1.8396,AUC: 0.8444,AP: 0.7977\n",
      "Epoch: 023, Loss: 1.7508,AUC: 0.8453,AP: 0.8009\n",
      "Epoch: 024, Loss: 1.7047,AUC: 0.8477,AP: 0.8059\n",
      "Epoch: 025, Loss: 1.6587,AUC: 0.8504,AP: 0.8117\n",
      "Epoch: 026, Loss: 1.5818,AUC: 0.8553,AP: 0.8171\n",
      "Epoch: 027, Loss: 1.5286,AUC: 0.8584,AP: 0.8238\n",
      "Epoch: 028, Loss: 1.4822,AUC: 0.8588,AP: 0.8285\n",
      "Epoch: 029, Loss: 1.4464,AUC: 0.8593,AP: 0.8308\n",
      "Epoch: 030, Loss: 1.3993,AUC: 0.8608,AP: 0.8366\n",
      "Epoch: 031, Loss: 1.3507,AUC: 0.8615,AP: 0.8385\n",
      "Epoch: 032, Loss: 1.3201,AUC: 0.8647,AP: 0.8449\n",
      "Epoch: 033, Loss: 1.2893,AUC: 0.8675,AP: 0.8555\n",
      "Epoch: 034, Loss: 1.2492,AUC: 0.8699,AP: 0.8580\n",
      "Epoch: 035, Loss: 1.2091,AUC: 0.8726,AP: 0.8632\n",
      "Epoch: 036, Loss: 1.1811,AUC: 0.8731,AP: 0.8662\n",
      "Epoch: 037, Loss: 1.1523,AUC: 0.8732,AP: 0.8686\n",
      "Epoch: 038, Loss: 1.1324,AUC: 0.8742,AP: 0.8708\n",
      "Epoch: 039, Loss: 1.1293,AUC: 0.8766,AP: 0.8742\n",
      "Epoch: 040, Loss: 1.1084,AUC: 0.8812,AP: 0.8810\n",
      "Epoch: 041, Loss: 1.0703,AUC: 0.8837,AP: 0.8835\n",
      "Epoch: 042, Loss: 1.0619,AUC: 0.8856,AP: 0.8858\n",
      "Epoch: 043, Loss: 1.0470,AUC: 0.8865,AP: 0.8881\n",
      "Epoch: 044, Loss: 1.0244,AUC: 0.8872,AP: 0.8898\n",
      "Epoch: 045, Loss: 1.0095,AUC: 0.8875,AP: 0.8912\n",
      "Epoch: 046, Loss: 0.9868,AUC: 0.8883,AP: 0.8928\n",
      "Epoch: 047, Loss: 0.9743,AUC: 0.8875,AP: 0.8926\n",
      "Epoch: 048, Loss: 0.9792,AUC: 0.8875,AP: 0.8945\n",
      "Epoch: 049, Loss: 0.9540,AUC: 0.8887,AP: 0.8970\n",
      "Epoch: 050, Loss: 0.9498,AUC: 0.8902,AP: 0.8998\n",
      "Epoch: 051, Loss: 0.9302,AUC: 0.8900,AP: 0.9012\n",
      "Epoch: 052, Loss: 0.9221,AUC: 0.8885,AP: 0.9016\n",
      "Epoch: 053, Loss: 0.9166,AUC: 0.8882,AP: 0.9028\n",
      "Epoch: 054, Loss: 0.9074,AUC: 0.8871,AP: 0.9035\n",
      "Epoch: 055, Loss: 0.9089,AUC: 0.8859,AP: 0.9038\n",
      "Epoch: 056, Loss: 0.8917,AUC: 0.8870,AP: 0.9059\n",
      "Epoch: 057, Loss: 0.8868,AUC: 0.8872,AP: 0.9064\n",
      "Epoch: 058, Loss: 0.8872,AUC: 0.8874,AP: 0.9074\n",
      "Epoch: 059, Loss: 0.8749,AUC: 0.8881,AP: 0.9087\n",
      "Epoch: 060, Loss: 0.8660,AUC: 0.8896,AP: 0.9107\n",
      "Epoch: 061, Loss: 0.8579,AUC: 0.8896,AP: 0.9102\n",
      "Epoch: 062, Loss: 0.8590,AUC: 0.8881,AP: 0.9087\n",
      "Epoch: 063, Loss: 0.8581,AUC: 0.8881,AP: 0.9099\n",
      "Epoch: 064, Loss: 0.8573,AUC: 0.8894,AP: 0.9117\n",
      "Epoch: 065, Loss: 0.8412,AUC: 0.8909,AP: 0.9140\n",
      "Epoch: 066, Loss: 0.8395,AUC: 0.8916,AP: 0.9162\n",
      "Epoch: 067, Loss: 0.8331,AUC: 0.8916,AP: 0.9172\n",
      "Epoch: 068, Loss: 0.8478,AUC: 0.8904,AP: 0.9176\n",
      "Epoch: 069, Loss: 0.8321,AUC: 0.8910,AP: 0.9199\n",
      "Epoch: 070, Loss: 0.8348,AUC: 0.8922,AP: 0.9212\n",
      "Epoch: 071, Loss: 0.8193,AUC: 0.8920,AP: 0.9215\n",
      "Epoch: 072, Loss: 0.8229,AUC: 0.8923,AP: 0.9223\n",
      "Epoch: 073, Loss: 0.8206,AUC: 0.8938,AP: 0.9241\n",
      "Epoch: 074, Loss: 0.8124,AUC: 0.8936,AP: 0.9245\n",
      "Epoch: 075, Loss: 0.8096,AUC: 0.8936,AP: 0.9248\n",
      "Epoch: 076, Loss: 0.8116,AUC: 0.8916,AP: 0.9245\n",
      "Epoch: 077, Loss: 0.8248,AUC: 0.8912,AP: 0.9244\n",
      "Epoch: 078, Loss: 0.8115,AUC: 0.8918,AP: 0.9250\n",
      "Epoch: 079, Loss: 0.8058,AUC: 0.8938,AP: 0.9272\n",
      "Epoch: 080, Loss: 0.8028,AUC: 0.8958,AP: 0.9294\n",
      "Epoch: 081, Loss: 0.8018,AUC: 0.8976,AP: 0.9308\n",
      "Epoch: 082, Loss: 0.8093,AUC: 0.8997,AP: 0.9323\n",
      "Epoch: 083, Loss: 0.8043,AUC: 0.9018,AP: 0.9342\n",
      "Epoch: 084, Loss: 0.8118,AUC: 0.9023,AP: 0.9345\n",
      "Epoch: 085, Loss: 0.8001,AUC: 0.9030,AP: 0.9355\n",
      "Epoch: 086, Loss: 0.7940,AUC: 0.9041,AP: 0.9375\n",
      "Epoch: 087, Loss: 0.7999,AUC: 0.9037,AP: 0.9371\n",
      "Epoch: 088, Loss: 0.7974,AUC: 0.9033,AP: 0.9370\n",
      "Epoch: 089, Loss: 0.7923,AUC: 0.9042,AP: 0.9380\n",
      "Epoch: 090, Loss: 0.8038,AUC: 0.9058,AP: 0.9390\n",
      "Epoch: 091, Loss: 0.7916,AUC: 0.9078,AP: 0.9406\n",
      "Epoch: 092, Loss: 0.7970,AUC: 0.9071,AP: 0.9407\n",
      "Epoch: 093, Loss: 0.7921,AUC: 0.9075,AP: 0.9411\n",
      "Epoch: 094, Loss: 0.7941,AUC: 0.9077,AP: 0.9414\n",
      "Epoch: 095, Loss: 0.7942,AUC: 0.9082,AP: 0.9417\n",
      "Epoch: 096, Loss: 0.7944,AUC: 0.9086,AP: 0.9420\n",
      "Epoch: 097, Loss: 0.7947,AUC: 0.9083,AP: 0.9417\n",
      "Epoch: 098, Loss: 0.7962,AUC: 0.9098,AP: 0.9427\n",
      "Epoch: 099, Loss: 0.7813,AUC: 0.9102,AP: 0.9431\n",
      "Epoch: 100, Loss: 0.7859,AUC: 0.9103,AP: 0.9432\n",
      "Epoch: 101, Loss: 0.7824,AUC: 0.9089,AP: 0.9423\n",
      "Epoch: 102, Loss: 0.7831,AUC: 0.9084,AP: 0.9422\n",
      "Epoch: 103, Loss: 0.7857,AUC: 0.9084,AP: 0.9423\n",
      "Epoch: 104, Loss: 0.7874,AUC: 0.9090,AP: 0.9427\n",
      "Epoch: 105, Loss: 0.7842,AUC: 0.9088,AP: 0.9424\n",
      "Epoch: 106, Loss: 0.7813,AUC: 0.9105,AP: 0.9433\n",
      "Epoch: 107, Loss: 0.7876,AUC: 0.9118,AP: 0.9442\n",
      "Epoch: 108, Loss: 0.7838,AUC: 0.9125,AP: 0.9449\n",
      "Epoch: 109, Loss: 0.7826,AUC: 0.9133,AP: 0.9456\n",
      "Epoch: 110, Loss: 0.7798,AUC: 0.9138,AP: 0.9461\n",
      "Epoch: 111, Loss: 0.7799,AUC: 0.9120,AP: 0.9450\n",
      "Epoch: 112, Loss: 0.7773,AUC: 0.9116,AP: 0.9449\n",
      "Epoch: 113, Loss: 0.7857,AUC: 0.9106,AP: 0.9445\n",
      "Epoch: 114, Loss: 0.7799,AUC: 0.9107,AP: 0.9447\n",
      "Epoch: 115, Loss: 0.7819,AUC: 0.9099,AP: 0.9445\n",
      "Epoch: 116, Loss: 0.7800,AUC: 0.9100,AP: 0.9447\n",
      "Epoch: 117, Loss: 0.7767,AUC: 0.9097,AP: 0.9446\n",
      "Epoch: 118, Loss: 0.7734,AUC: 0.9095,AP: 0.9444\n",
      "Epoch: 119, Loss: 0.7781,AUC: 0.9090,AP: 0.9442\n",
      "Epoch: 120, Loss: 0.7770,AUC: 0.9088,AP: 0.9439\n",
      "Epoch: 121, Loss: 0.7762,AUC: 0.9089,AP: 0.9439\n",
      "Epoch: 122, Loss: 0.7742,AUC: 0.9085,AP: 0.9437\n",
      "Epoch: 123, Loss: 0.7726,AUC: 0.9082,AP: 0.9435\n",
      "Epoch: 124, Loss: 0.7746,AUC: 0.9078,AP: 0.9435\n",
      "Epoch: 125, Loss: 0.7769,AUC: 0.9073,AP: 0.9435\n",
      "Epoch: 126, Loss: 0.7806,AUC: 0.9067,AP: 0.9432\n",
      "Epoch: 127, Loss: 0.7822,AUC: 0.9071,AP: 0.9430\n",
      "Epoch: 128, Loss: 0.7862,AUC: 0.9058,AP: 0.9422\n",
      "Epoch: 129, Loss: 0.7739,AUC: 0.9035,AP: 0.9411\n",
      "Epoch: 130, Loss: 0.7775,AUC: 0.9030,AP: 0.9410\n",
      "Epoch: 131, Loss: 0.7734,AUC: 0.9038,AP: 0.9413\n",
      "Epoch: 132, Loss: 0.7730,AUC: 0.9058,AP: 0.9422\n",
      "Epoch: 133, Loss: 0.7774,AUC: 0.9075,AP: 0.9429\n",
      "Epoch: 134, Loss: 0.7774,AUC: 0.9077,AP: 0.9429\n",
      "Epoch: 135, Loss: 0.7793,AUC: 0.9083,AP: 0.9429\n",
      "Epoch: 136, Loss: 0.7783,AUC: 0.9118,AP: 0.9444\n",
      "Epoch: 137, Loss: 0.7794,AUC: 0.9129,AP: 0.9449\n",
      "Epoch: 138, Loss: 0.7803,AUC: 0.9135,AP: 0.9454\n",
      "Epoch: 139, Loss: 0.7762,AUC: 0.9125,AP: 0.9448\n",
      "Epoch: 140, Loss: 0.7698,AUC: 0.9130,AP: 0.9450\n",
      "Epoch: 141, Loss: 0.7706,AUC: 0.9130,AP: 0.9449\n",
      "Epoch: 142, Loss: 0.7833,AUC: 0.9119,AP: 0.9442\n",
      "Epoch: 143, Loss: 0.7740,AUC: 0.9128,AP: 0.9449\n",
      "Epoch: 144, Loss: 0.7739,AUC: 0.9121,AP: 0.9447\n",
      "Epoch: 145, Loss: 0.7745,AUC: 0.9114,AP: 0.9442\n",
      "Epoch: 146, Loss: 0.7784,AUC: 0.9111,AP: 0.9441\n",
      "Epoch: 147, Loss: 0.7740,AUC: 0.9093,AP: 0.9428\n",
      "Epoch: 148, Loss: 0.7723,AUC: 0.9084,AP: 0.9424\n",
      "Epoch: 149, Loss: 0.7747,AUC: 0.9090,AP: 0.9429\n",
      "Epoch: 150, Loss: 0.7744,AUC: 0.9096,AP: 0.9433\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m3.pt')\n",
    "data = data.to(device)\n",
    "data.num_nodes = 899\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "model = Node2Vec(\n",
    "    edge_index=train_data.edge_index,\n",
    "    embedding_dim=128,\n",
    "    walk_length=20,\n",
    "    context_size=10,\n",
    "    walks_per_node=10,\n",
    "    num_negative_samples=1,\n",
    "    p=0.5,  # p=q=1时，退化为DeepWalk\n",
    "    q=1.0,\n",
    "    sparse=False,  # 该数据集为密集数据集\n",
    ").to(device)\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "print(torch.max(train_data.edge_index[0]))\n",
    "print(train_data.edge_index.shape)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data\n",
    "    z = model()  # 输出潜在空间的嵌入向量\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([898, 128])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3 = model()\n",
    "z3.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_2 = torch.zeros((2, 128)).to(device)\n",
    "z3 = torch.cat([z3, zero_2], 0)\n",
    "z3.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(897, device='cuda:1')\n",
      "torch.Size([2, 3052])\n",
      "Epoch: 001, Loss: 6.1907,AUC: 0.6365,AP: 0.5775\n",
      "Epoch: 002, Loss: 5.4508,AUC: 0.6958,AP: 0.6120\n",
      "Epoch: 003, Loss: 4.9338,AUC: 0.7324,AP: 0.6397\n",
      "Epoch: 004, Loss: 4.7292,AUC: 0.7558,AP: 0.6606\n",
      "Epoch: 005, Loss: 4.3182,AUC: 0.7802,AP: 0.6844\n",
      "Epoch: 006, Loss: 4.0732,AUC: 0.7961,AP: 0.7016\n",
      "Epoch: 007, Loss: 3.7962,AUC: 0.8058,AP: 0.7147\n",
      "Epoch: 008, Loss: 3.6290,AUC: 0.8114,AP: 0.7211\n",
      "Epoch: 009, Loss: 3.3760,AUC: 0.8209,AP: 0.7356\n",
      "Epoch: 010, Loss: 3.1670,AUC: 0.8257,AP: 0.7411\n",
      "Epoch: 011, Loss: 2.9480,AUC: 0.8303,AP: 0.7487\n",
      "Epoch: 012, Loss: 2.8597,AUC: 0.8346,AP: 0.7542\n",
      "Epoch: 013, Loss: 2.6334,AUC: 0.8413,AP: 0.7708\n",
      "Epoch: 014, Loss: 2.5459,AUC: 0.8459,AP: 0.7763\n",
      "Epoch: 015, Loss: 2.4017,AUC: 0.8505,AP: 0.7854\n",
      "Epoch: 016, Loss: 2.2838,AUC: 0.8540,AP: 0.7938\n",
      "Epoch: 017, Loss: 2.1770,AUC: 0.8575,AP: 0.8020\n",
      "Epoch: 018, Loss: 2.0756,AUC: 0.8615,AP: 0.8115\n",
      "Epoch: 019, Loss: 2.0033,AUC: 0.8664,AP: 0.8204\n",
      "Epoch: 020, Loss: 1.9084,AUC: 0.8719,AP: 0.8307\n",
      "Epoch: 021, Loss: 1.8262,AUC: 0.8763,AP: 0.8398\n",
      "Epoch: 022, Loss: 1.7573,AUC: 0.8789,AP: 0.8430\n",
      "Epoch: 023, Loss: 1.6740,AUC: 0.8828,AP: 0.8487\n",
      "Epoch: 024, Loss: 1.6104,AUC: 0.8859,AP: 0.8552\n",
      "Epoch: 025, Loss: 1.5723,AUC: 0.8887,AP: 0.8599\n",
      "Epoch: 026, Loss: 1.5160,AUC: 0.8932,AP: 0.8673\n",
      "Epoch: 027, Loss: 1.4537,AUC: 0.8966,AP: 0.8714\n",
      "Epoch: 028, Loss: 1.4048,AUC: 0.8967,AP: 0.8723\n",
      "Epoch: 029, Loss: 1.3740,AUC: 0.8984,AP: 0.8761\n",
      "Epoch: 030, Loss: 1.3339,AUC: 0.9010,AP: 0.8871\n",
      "Epoch: 031, Loss: 1.2783,AUC: 0.9027,AP: 0.8905\n",
      "Epoch: 032, Loss: 1.2504,AUC: 0.9069,AP: 0.8964\n",
      "Epoch: 033, Loss: 1.2247,AUC: 0.9096,AP: 0.9004\n",
      "Epoch: 034, Loss: 1.1857,AUC: 0.9129,AP: 0.9051\n",
      "Epoch: 035, Loss: 1.1518,AUC: 0.9151,AP: 0.9083\n",
      "Epoch: 036, Loss: 1.1245,AUC: 0.9162,AP: 0.9102\n",
      "Epoch: 037, Loss: 1.1002,AUC: 0.9181,AP: 0.9134\n",
      "Epoch: 038, Loss: 1.0795,AUC: 0.9196,AP: 0.9160\n",
      "Epoch: 039, Loss: 1.0704,AUC: 0.9226,AP: 0.9217\n",
      "Epoch: 040, Loss: 1.0593,AUC: 0.9251,AP: 0.9255\n",
      "Epoch: 041, Loss: 1.0219,AUC: 0.9269,AP: 0.9288\n",
      "Epoch: 042, Loss: 1.0038,AUC: 0.9295,AP: 0.9323\n",
      "Epoch: 043, Loss: 1.0032,AUC: 0.9300,AP: 0.9335\n",
      "Epoch: 044, Loss: 0.9735,AUC: 0.9303,AP: 0.9346\n",
      "Epoch: 045, Loss: 0.9578,AUC: 0.9301,AP: 0.9351\n",
      "Epoch: 046, Loss: 0.9436,AUC: 0.9310,AP: 0.9374\n",
      "Epoch: 047, Loss: 0.9366,AUC: 0.9319,AP: 0.9403\n",
      "Epoch: 048, Loss: 0.9302,AUC: 0.9339,AP: 0.9428\n",
      "Epoch: 049, Loss: 0.9154,AUC: 0.9348,AP: 0.9444\n",
      "Epoch: 050, Loss: 0.8992,AUC: 0.9362,AP: 0.9473\n",
      "Epoch: 051, Loss: 0.8914,AUC: 0.9365,AP: 0.9484\n",
      "Epoch: 052, Loss: 0.8863,AUC: 0.9370,AP: 0.9490\n",
      "Epoch: 053, Loss: 0.8762,AUC: 0.9384,AP: 0.9504\n",
      "Epoch: 054, Loss: 0.8675,AUC: 0.9378,AP: 0.9505\n",
      "Epoch: 055, Loss: 0.8714,AUC: 0.9375,AP: 0.9508\n",
      "Epoch: 056, Loss: 0.8651,AUC: 0.9376,AP: 0.9514\n",
      "Epoch: 057, Loss: 0.8470,AUC: 0.9376,AP: 0.9516\n",
      "Epoch: 058, Loss: 0.8487,AUC: 0.9374,AP: 0.9515\n",
      "Epoch: 059, Loss: 0.8359,AUC: 0.9374,AP: 0.9516\n",
      "Epoch: 060, Loss: 0.8340,AUC: 0.9380,AP: 0.9524\n",
      "Epoch: 061, Loss: 0.8262,AUC: 0.9382,AP: 0.9527\n",
      "Epoch: 062, Loss: 0.8337,AUC: 0.9388,AP: 0.9534\n",
      "Epoch: 063, Loss: 0.8306,AUC: 0.9382,AP: 0.9528\n",
      "Epoch: 064, Loss: 0.8255,AUC: 0.9380,AP: 0.9532\n",
      "Epoch: 065, Loss: 0.8142,AUC: 0.9383,AP: 0.9533\n",
      "Epoch: 066, Loss: 0.8129,AUC: 0.9381,AP: 0.9534\n",
      "Epoch: 067, Loss: 0.8091,AUC: 0.9384,AP: 0.9546\n",
      "Epoch: 068, Loss: 0.8125,AUC: 0.9383,AP: 0.9548\n",
      "Epoch: 069, Loss: 0.8043,AUC: 0.9390,AP: 0.9560\n",
      "Epoch: 070, Loss: 0.8051,AUC: 0.9403,AP: 0.9570\n",
      "Epoch: 071, Loss: 0.7953,AUC: 0.9411,AP: 0.9578\n",
      "Epoch: 072, Loss: 0.7994,AUC: 0.9419,AP: 0.9586\n",
      "Epoch: 073, Loss: 0.7965,AUC: 0.9429,AP: 0.9591\n",
      "Epoch: 074, Loss: 0.7888,AUC: 0.9435,AP: 0.9594\n",
      "Epoch: 075, Loss: 0.7848,AUC: 0.9436,AP: 0.9593\n",
      "Epoch: 076, Loss: 0.7855,AUC: 0.9440,AP: 0.9598\n",
      "Epoch: 077, Loss: 0.8032,AUC: 0.9448,AP: 0.9605\n",
      "Epoch: 078, Loss: 0.7856,AUC: 0.9466,AP: 0.9611\n",
      "Epoch: 079, Loss: 0.7841,AUC: 0.9464,AP: 0.9618\n",
      "Epoch: 080, Loss: 0.7794,AUC: 0.9471,AP: 0.9629\n",
      "Epoch: 081, Loss: 0.7831,AUC: 0.9474,AP: 0.9632\n",
      "Epoch: 082, Loss: 0.7811,AUC: 0.9477,AP: 0.9634\n",
      "Epoch: 083, Loss: 0.7825,AUC: 0.9481,AP: 0.9643\n",
      "Epoch: 084, Loss: 0.7855,AUC: 0.9474,AP: 0.9642\n",
      "Epoch: 085, Loss: 0.7803,AUC: 0.9466,AP: 0.9643\n",
      "Epoch: 086, Loss: 0.7737,AUC: 0.9457,AP: 0.9641\n",
      "Epoch: 087, Loss: 0.7811,AUC: 0.9458,AP: 0.9644\n",
      "Epoch: 088, Loss: 0.7734,AUC: 0.9462,AP: 0.9649\n",
      "Epoch: 089, Loss: 0.7725,AUC: 0.9463,AP: 0.9651\n",
      "Epoch: 090, Loss: 0.7739,AUC: 0.9463,AP: 0.9653\n",
      "Epoch: 091, Loss: 0.7711,AUC: 0.9476,AP: 0.9662\n",
      "Epoch: 092, Loss: 0.7754,AUC: 0.9475,AP: 0.9660\n",
      "Epoch: 093, Loss: 0.7712,AUC: 0.9466,AP: 0.9658\n",
      "Epoch: 094, Loss: 0.7696,AUC: 0.9460,AP: 0.9654\n",
      "Epoch: 095, Loss: 0.7714,AUC: 0.9461,AP: 0.9653\n",
      "Epoch: 096, Loss: 0.7776,AUC: 0.9449,AP: 0.9646\n",
      "Epoch: 097, Loss: 0.7736,AUC: 0.9445,AP: 0.9644\n",
      "Epoch: 098, Loss: 0.7685,AUC: 0.9449,AP: 0.9649\n",
      "Epoch: 099, Loss: 0.7637,AUC: 0.9463,AP: 0.9659\n",
      "Epoch: 100, Loss: 0.7671,AUC: 0.9465,AP: 0.9661\n",
      "Epoch: 101, Loss: 0.7647,AUC: 0.9473,AP: 0.9666\n",
      "Epoch: 102, Loss: 0.7654,AUC: 0.9477,AP: 0.9666\n",
      "Epoch: 103, Loss: 0.7633,AUC: 0.9468,AP: 0.9661\n",
      "Epoch: 104, Loss: 0.7682,AUC: 0.9465,AP: 0.9659\n",
      "Epoch: 105, Loss: 0.7616,AUC: 0.9464,AP: 0.9659\n",
      "Epoch: 106, Loss: 0.7618,AUC: 0.9457,AP: 0.9656\n",
      "Epoch: 107, Loss: 0.7691,AUC: 0.9452,AP: 0.9654\n",
      "Epoch: 108, Loss: 0.7654,AUC: 0.9451,AP: 0.9655\n",
      "Epoch: 109, Loss: 0.7628,AUC: 0.9461,AP: 0.9661\n",
      "Epoch: 110, Loss: 0.7635,AUC: 0.9473,AP: 0.9666\n",
      "Epoch: 111, Loss: 0.7664,AUC: 0.9476,AP: 0.9667\n",
      "Epoch: 112, Loss: 0.7588,AUC: 0.9477,AP: 0.9668\n",
      "Epoch: 113, Loss: 0.7644,AUC: 0.9476,AP: 0.9667\n",
      "Epoch: 114, Loss: 0.7622,AUC: 0.9474,AP: 0.9667\n",
      "Epoch: 115, Loss: 0.7585,AUC: 0.9471,AP: 0.9664\n",
      "Epoch: 116, Loss: 0.7653,AUC: 0.9473,AP: 0.9665\n",
      "Epoch: 117, Loss: 0.7604,AUC: 0.9475,AP: 0.9667\n",
      "Epoch: 118, Loss: 0.7565,AUC: 0.9463,AP: 0.9662\n",
      "Epoch: 119, Loss: 0.7620,AUC: 0.9461,AP: 0.9661\n",
      "Epoch: 120, Loss: 0.7585,AUC: 0.9462,AP: 0.9662\n",
      "Epoch: 121, Loss: 0.7632,AUC: 0.9457,AP: 0.9659\n",
      "Epoch: 122, Loss: 0.7593,AUC: 0.9455,AP: 0.9659\n",
      "Epoch: 123, Loss: 0.7562,AUC: 0.9460,AP: 0.9663\n",
      "Epoch: 124, Loss: 0.7583,AUC: 0.9464,AP: 0.9666\n",
      "Epoch: 125, Loss: 0.7595,AUC: 0.9466,AP: 0.9668\n",
      "Epoch: 126, Loss: 0.7563,AUC: 0.9467,AP: 0.9669\n",
      "Epoch: 127, Loss: 0.7599,AUC: 0.9478,AP: 0.9674\n",
      "Epoch: 128, Loss: 0.7576,AUC: 0.9482,AP: 0.9675\n",
      "Epoch: 129, Loss: 0.7559,AUC: 0.9488,AP: 0.9678\n",
      "Epoch: 130, Loss: 0.7600,AUC: 0.9486,AP: 0.9677\n",
      "Epoch: 131, Loss: 0.7545,AUC: 0.9472,AP: 0.9669\n",
      "Epoch: 132, Loss: 0.7559,AUC: 0.9459,AP: 0.9662\n",
      "Epoch: 133, Loss: 0.7607,AUC: 0.9450,AP: 0.9658\n",
      "Epoch: 134, Loss: 0.7544,AUC: 0.9449,AP: 0.9657\n",
      "Epoch: 135, Loss: 0.7593,AUC: 0.9457,AP: 0.9662\n",
      "Epoch: 136, Loss: 0.7568,AUC: 0.9461,AP: 0.9665\n",
      "Epoch: 137, Loss: 0.7604,AUC: 0.9453,AP: 0.9661\n",
      "Epoch: 138, Loss: 0.7552,AUC: 0.9439,AP: 0.9656\n",
      "Epoch: 139, Loss: 0.7550,AUC: 0.9444,AP: 0.9658\n",
      "Epoch: 140, Loss: 0.7543,AUC: 0.9444,AP: 0.9657\n",
      "Epoch: 141, Loss: 0.7549,AUC: 0.9445,AP: 0.9656\n",
      "Epoch: 142, Loss: 0.7638,AUC: 0.9451,AP: 0.9658\n",
      "Epoch: 143, Loss: 0.7554,AUC: 0.9464,AP: 0.9662\n",
      "Epoch: 144, Loss: 0.7534,AUC: 0.9471,AP: 0.9664\n",
      "Epoch: 145, Loss: 0.7574,AUC: 0.9474,AP: 0.9663\n",
      "Epoch: 146, Loss: 0.7606,AUC: 0.9480,AP: 0.9665\n",
      "Epoch: 147, Loss: 0.7587,AUC: 0.9479,AP: 0.9664\n",
      "Epoch: 148, Loss: 0.7556,AUC: 0.9471,AP: 0.9659\n",
      "Epoch: 149, Loss: 0.7578,AUC: 0.9462,AP: 0.9654\n",
      "Epoch: 150, Loss: 0.7579,AUC: 0.9458,AP: 0.9653\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m4.pt')\n",
    "data = data.to(device)\n",
    "data.num_nodes = 899\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "model = Node2Vec(\n",
    "    edge_index=train_data.edge_index,\n",
    "    embedding_dim=128,\n",
    "    walk_length=20,\n",
    "    context_size=10,\n",
    "    walks_per_node=10,\n",
    "    num_negative_samples=1,\n",
    "    p=0.5,  # p=q=1时，退化为DeepWalk\n",
    "    q=1.0,\n",
    "    sparse=False,  # 该数据集为密集数据集\n",
    ").to(device)\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "print(torch.max(train_data.edge_index[0]))\n",
    "print(train_data.edge_index.shape)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data\n",
    "    z = model()  # 输出潜在空间的嵌入向量\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([898, 128])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z4 = model()\n",
    "z4.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_2 = torch.zeros((2, 128)).to(device)\n",
    "z4 = torch.cat([z4, zero_2], 0)\n",
    "z4.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(897, device='cuda:1')\n",
      "torch.Size([2, 1952])\n",
      "Epoch: 001, Loss: 6.3228,AUC: 0.5826,AP: 0.5433\n",
      "Epoch: 002, Loss: 5.5022,AUC: 0.6529,AP: 0.5834\n",
      "Epoch: 003, Loss: 4.9748,AUC: 0.7054,AP: 0.6203\n",
      "Epoch: 004, Loss: 4.7265,AUC: 0.7344,AP: 0.6478\n",
      "Epoch: 005, Loss: 4.3602,AUC: 0.7543,AP: 0.6685\n",
      "Epoch: 006, Loss: 4.1259,AUC: 0.7735,AP: 0.6916\n",
      "Epoch: 007, Loss: 3.8614,AUC: 0.7822,AP: 0.7011\n",
      "Epoch: 008, Loss: 3.6699,AUC: 0.7904,AP: 0.7124\n",
      "Epoch: 009, Loss: 3.4229,AUC: 0.7965,AP: 0.7244\n",
      "Epoch: 010, Loss: 3.2225,AUC: 0.8001,AP: 0.7314\n",
      "Epoch: 011, Loss: 2.9968,AUC: 0.8024,AP: 0.7365\n",
      "Epoch: 012, Loss: 2.9058,AUC: 0.8050,AP: 0.7413\n",
      "Epoch: 013, Loss: 2.6882,AUC: 0.8115,AP: 0.7568\n",
      "Epoch: 014, Loss: 2.5923,AUC: 0.8174,AP: 0.7670\n",
      "Epoch: 015, Loss: 2.4516,AUC: 0.8232,AP: 0.7759\n",
      "Epoch: 016, Loss: 2.3360,AUC: 0.8281,AP: 0.7817\n",
      "Epoch: 017, Loss: 2.2217,AUC: 0.8319,AP: 0.7872\n",
      "Epoch: 018, Loss: 2.1104,AUC: 0.8371,AP: 0.7963\n",
      "Epoch: 019, Loss: 2.0460,AUC: 0.8436,AP: 0.8072\n",
      "Epoch: 020, Loss: 1.9642,AUC: 0.8494,AP: 0.8192\n",
      "Epoch: 021, Loss: 1.8890,AUC: 0.8547,AP: 0.8267\n",
      "Epoch: 022, Loss: 1.7999,AUC: 0.8603,AP: 0.8332\n",
      "Epoch: 023, Loss: 1.7164,AUC: 0.8633,AP: 0.8365\n",
      "Epoch: 024, Loss: 1.6513,AUC: 0.8669,AP: 0.8448\n",
      "Epoch: 025, Loss: 1.6109,AUC: 0.8678,AP: 0.8460\n",
      "Epoch: 026, Loss: 1.5588,AUC: 0.8704,AP: 0.8536\n",
      "Epoch: 027, Loss: 1.4852,AUC: 0.8736,AP: 0.8586\n",
      "Epoch: 028, Loss: 1.4504,AUC: 0.8747,AP: 0.8608\n",
      "Epoch: 029, Loss: 1.4123,AUC: 0.8781,AP: 0.8633\n",
      "Epoch: 030, Loss: 1.3600,AUC: 0.8823,AP: 0.8702\n",
      "Epoch: 031, Loss: 1.3095,AUC: 0.8849,AP: 0.8749\n",
      "Epoch: 032, Loss: 1.2758,AUC: 0.8895,AP: 0.8833\n",
      "Epoch: 033, Loss: 1.2519,AUC: 0.8909,AP: 0.8861\n",
      "Epoch: 034, Loss: 1.2178,AUC: 0.8941,AP: 0.8892\n",
      "Epoch: 035, Loss: 1.1790,AUC: 0.8968,AP: 0.8927\n",
      "Epoch: 036, Loss: 1.1474,AUC: 0.8998,AP: 0.8979\n",
      "Epoch: 037, Loss: 1.1199,AUC: 0.9033,AP: 0.9015\n",
      "Epoch: 038, Loss: 1.1021,AUC: 0.9034,AP: 0.9019\n",
      "Epoch: 039, Loss: 1.0916,AUC: 0.9053,AP: 0.9050\n",
      "Epoch: 040, Loss: 1.0830,AUC: 0.9054,AP: 0.9061\n",
      "Epoch: 041, Loss: 1.0477,AUC: 0.9047,AP: 0.9070\n",
      "Epoch: 042, Loss: 1.0321,AUC: 0.9048,AP: 0.9078\n",
      "Epoch: 043, Loss: 1.0224,AUC: 0.9067,AP: 0.9150\n",
      "Epoch: 044, Loss: 0.9954,AUC: 0.9056,AP: 0.9148\n",
      "Epoch: 045, Loss: 0.9781,AUC: 0.9072,AP: 0.9159\n",
      "Epoch: 046, Loss: 0.9606,AUC: 0.9067,AP: 0.9161\n",
      "Epoch: 047, Loss: 0.9505,AUC: 0.9053,AP: 0.9157\n",
      "Epoch: 048, Loss: 0.9509,AUC: 0.9053,AP: 0.9159\n",
      "Epoch: 049, Loss: 0.9312,AUC: 0.9053,AP: 0.9183\n",
      "Epoch: 050, Loss: 0.9168,AUC: 0.9056,AP: 0.9204\n",
      "Epoch: 051, Loss: 0.9050,AUC: 0.9056,AP: 0.9205\n",
      "Epoch: 052, Loss: 0.8942,AUC: 0.9052,AP: 0.9204\n",
      "Epoch: 053, Loss: 0.8892,AUC: 0.9066,AP: 0.9213\n",
      "Epoch: 054, Loss: 0.8832,AUC: 0.9077,AP: 0.9246\n",
      "Epoch: 055, Loss: 0.8756,AUC: 0.9074,AP: 0.9242\n",
      "Epoch: 056, Loss: 0.8737,AUC: 0.9082,AP: 0.9274\n",
      "Epoch: 057, Loss: 0.8648,AUC: 0.9071,AP: 0.9282\n",
      "Epoch: 058, Loss: 0.8622,AUC: 0.9066,AP: 0.9295\n",
      "Epoch: 059, Loss: 0.8470,AUC: 0.9068,AP: 0.9299\n",
      "Epoch: 060, Loss: 0.8499,AUC: 0.9079,AP: 0.9304\n",
      "Epoch: 061, Loss: 0.8389,AUC: 0.9096,AP: 0.9314\n",
      "Epoch: 062, Loss: 0.8384,AUC: 0.9104,AP: 0.9338\n",
      "Epoch: 063, Loss: 0.8479,AUC: 0.9114,AP: 0.9344\n",
      "Epoch: 064, Loss: 0.8302,AUC: 0.9101,AP: 0.9312\n",
      "Epoch: 065, Loss: 0.8301,AUC: 0.9106,AP: 0.9334\n",
      "Epoch: 066, Loss: 0.8215,AUC: 0.9106,AP: 0.9306\n",
      "Epoch: 067, Loss: 0.8157,AUC: 0.9109,AP: 0.9335\n",
      "Epoch: 068, Loss: 0.8207,AUC: 0.9097,AP: 0.9333\n",
      "Epoch: 069, Loss: 0.8069,AUC: 0.9100,AP: 0.9337\n",
      "Epoch: 070, Loss: 0.8145,AUC: 0.9088,AP: 0.9341\n",
      "Epoch: 071, Loss: 0.8012,AUC: 0.9078,AP: 0.9333\n",
      "Epoch: 072, Loss: 0.8026,AUC: 0.9079,AP: 0.9333\n",
      "Epoch: 073, Loss: 0.8068,AUC: 0.9086,AP: 0.9348\n",
      "Epoch: 074, Loss: 0.7962,AUC: 0.9090,AP: 0.9348\n",
      "Epoch: 075, Loss: 0.7928,AUC: 0.9097,AP: 0.9360\n",
      "Epoch: 076, Loss: 0.7926,AUC: 0.9098,AP: 0.9369\n",
      "Epoch: 077, Loss: 0.8079,AUC: 0.9100,AP: 0.9369\n",
      "Epoch: 078, Loss: 0.8013,AUC: 0.9096,AP: 0.9362\n",
      "Epoch: 079, Loss: 0.7889,AUC: 0.9102,AP: 0.9397\n",
      "Epoch: 080, Loss: 0.7896,AUC: 0.9102,AP: 0.9404\n",
      "Epoch: 081, Loss: 0.7864,AUC: 0.9097,AP: 0.9408\n",
      "Epoch: 082, Loss: 0.7909,AUC: 0.9102,AP: 0.9413\n",
      "Epoch: 083, Loss: 0.7921,AUC: 0.9132,AP: 0.9434\n",
      "Epoch: 084, Loss: 0.7900,AUC: 0.9121,AP: 0.9434\n",
      "Epoch: 085, Loss: 0.7879,AUC: 0.9124,AP: 0.9434\n",
      "Epoch: 086, Loss: 0.7814,AUC: 0.9132,AP: 0.9455\n",
      "Epoch: 087, Loss: 0.7839,AUC: 0.9142,AP: 0.9461\n",
      "Epoch: 088, Loss: 0.7794,AUC: 0.9140,AP: 0.9464\n",
      "Epoch: 089, Loss: 0.7773,AUC: 0.9149,AP: 0.9471\n",
      "Epoch: 090, Loss: 0.7797,AUC: 0.9149,AP: 0.9473\n",
      "Epoch: 091, Loss: 0.7757,AUC: 0.9136,AP: 0.9466\n",
      "Epoch: 092, Loss: 0.7790,AUC: 0.9138,AP: 0.9468\n",
      "Epoch: 093, Loss: 0.7733,AUC: 0.9141,AP: 0.9472\n",
      "Epoch: 094, Loss: 0.7728,AUC: 0.9144,AP: 0.9477\n",
      "Epoch: 095, Loss: 0.7758,AUC: 0.9142,AP: 0.9474\n",
      "Epoch: 096, Loss: 0.7795,AUC: 0.9134,AP: 0.9466\n",
      "Epoch: 097, Loss: 0.7723,AUC: 0.9136,AP: 0.9466\n",
      "Epoch: 098, Loss: 0.7796,AUC: 0.9145,AP: 0.9470\n",
      "Epoch: 099, Loss: 0.7682,AUC: 0.9162,AP: 0.9488\n",
      "Epoch: 100, Loss: 0.7701,AUC: 0.9171,AP: 0.9498\n",
      "Epoch: 101, Loss: 0.7719,AUC: 0.9162,AP: 0.9494\n",
      "Epoch: 102, Loss: 0.7680,AUC: 0.9156,AP: 0.9491\n",
      "Epoch: 103, Loss: 0.7686,AUC: 0.9144,AP: 0.9484\n",
      "Epoch: 104, Loss: 0.7730,AUC: 0.9138,AP: 0.9483\n",
      "Epoch: 105, Loss: 0.7664,AUC: 0.9132,AP: 0.9480\n",
      "Epoch: 106, Loss: 0.7660,AUC: 0.9150,AP: 0.9489\n",
      "Epoch: 107, Loss: 0.7714,AUC: 0.9162,AP: 0.9499\n",
      "Epoch: 108, Loss: 0.7653,AUC: 0.9176,AP: 0.9507\n",
      "Epoch: 109, Loss: 0.7666,AUC: 0.9193,AP: 0.9512\n",
      "Epoch: 110, Loss: 0.7656,AUC: 0.9191,AP: 0.9511\n",
      "Epoch: 111, Loss: 0.7644,AUC: 0.9182,AP: 0.9508\n",
      "Epoch: 112, Loss: 0.7641,AUC: 0.9178,AP: 0.9506\n",
      "Epoch: 113, Loss: 0.7695,AUC: 0.9164,AP: 0.9500\n",
      "Epoch: 114, Loss: 0.7709,AUC: 0.9156,AP: 0.9498\n",
      "Epoch: 115, Loss: 0.7639,AUC: 0.9167,AP: 0.9500\n",
      "Epoch: 116, Loss: 0.7696,AUC: 0.9170,AP: 0.9500\n",
      "Epoch: 117, Loss: 0.7689,AUC: 0.9171,AP: 0.9503\n",
      "Epoch: 118, Loss: 0.7601,AUC: 0.9164,AP: 0.9498\n",
      "Epoch: 119, Loss: 0.7615,AUC: 0.9166,AP: 0.9502\n",
      "Epoch: 120, Loss: 0.7630,AUC: 0.9164,AP: 0.9502\n",
      "Epoch: 121, Loss: 0.7657,AUC: 0.9164,AP: 0.9502\n",
      "Epoch: 122, Loss: 0.7646,AUC: 0.9152,AP: 0.9494\n",
      "Epoch: 123, Loss: 0.7604,AUC: 0.9152,AP: 0.9494\n",
      "Epoch: 124, Loss: 0.7611,AUC: 0.9155,AP: 0.9495\n",
      "Epoch: 125, Loss: 0.7628,AUC: 0.9158,AP: 0.9497\n",
      "Epoch: 126, Loss: 0.7635,AUC: 0.9165,AP: 0.9502\n",
      "Epoch: 127, Loss: 0.7685,AUC: 0.9182,AP: 0.9513\n",
      "Epoch: 128, Loss: 0.7628,AUC: 0.9175,AP: 0.9514\n",
      "Epoch: 129, Loss: 0.7588,AUC: 0.9161,AP: 0.9508\n",
      "Epoch: 130, Loss: 0.7651,AUC: 0.9140,AP: 0.9498\n",
      "Epoch: 131, Loss: 0.7612,AUC: 0.9132,AP: 0.9495\n",
      "Epoch: 132, Loss: 0.7629,AUC: 0.9120,AP: 0.9488\n",
      "Epoch: 133, Loss: 0.7635,AUC: 0.9122,AP: 0.9489\n",
      "Epoch: 134, Loss: 0.7561,AUC: 0.9120,AP: 0.9488\n",
      "Epoch: 135, Loss: 0.7584,AUC: 0.9132,AP: 0.9495\n",
      "Epoch: 136, Loss: 0.7604,AUC: 0.9149,AP: 0.9504\n",
      "Epoch: 137, Loss: 0.7656,AUC: 0.9162,AP: 0.9512\n",
      "Epoch: 138, Loss: 0.7583,AUC: 0.9180,AP: 0.9525\n",
      "Epoch: 139, Loss: 0.7584,AUC: 0.9191,AP: 0.9529\n",
      "Epoch: 140, Loss: 0.7574,AUC: 0.9195,AP: 0.9531\n",
      "Epoch: 141, Loss: 0.7568,AUC: 0.9192,AP: 0.9532\n",
      "Epoch: 142, Loss: 0.7627,AUC: 0.9185,AP: 0.9528\n",
      "Epoch: 143, Loss: 0.7622,AUC: 0.9172,AP: 0.9519\n",
      "Epoch: 144, Loss: 0.7617,AUC: 0.9186,AP: 0.9519\n",
      "Epoch: 145, Loss: 0.7595,AUC: 0.9182,AP: 0.9514\n",
      "Epoch: 146, Loss: 0.7602,AUC: 0.9195,AP: 0.9518\n",
      "Epoch: 147, Loss: 0.7619,AUC: 0.9205,AP: 0.9522\n",
      "Epoch: 148, Loss: 0.7584,AUC: 0.9240,AP: 0.9530\n",
      "Epoch: 149, Loss: 0.7601,AUC: 0.9225,AP: 0.9523\n",
      "Epoch: 150, Loss: 0.7550,AUC: 0.9215,AP: 0.9518\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m5.pt')\n",
    "data = data.to(device)\n",
    "data.num_nodes = 899\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "model = Node2Vec(\n",
    "    edge_index=train_data.edge_index,\n",
    "    embedding_dim=128,\n",
    "    walk_length=20,\n",
    "    context_size=10,\n",
    "    walks_per_node=10,\n",
    "    num_negative_samples=1,\n",
    "    p=0.5,  # p=q=1时，退化为DeepWalk\n",
    "    q=1.0,\n",
    "    sparse=False,  # 该数据集为密集数据集\n",
    ").to(device)\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "print(torch.max(train_data.edge_index[0]))\n",
    "print(train_data.edge_index.shape)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data\n",
    "    z = model()  # 输出潜在空间的嵌入向量\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([898, 128])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z5 = model()\n",
    "z5.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_2 = torch.zeros((2, 128)).to(device)\n",
    "z5 = torch.cat([z5, zero_2], 0)\n",
    "z5.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 640])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将每个节点在不同快照学习到的特征拼接起来，使用最后一个快照进行测试\n",
    "z_1_5 = torch.cat([z1, z2, z3, z4, z5], 1)\n",
    "z_1_5.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# torch.save(z_1_5,'/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/z_1_5.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 584])\n",
      "AUC: 0.9570,AP: 0.9716\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "# z_1_5 = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/z_1_5.pt')\n",
    "pyg.seed_everything(3407)\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m6.pt')\n",
    "data = data.to(device)\n",
    "data.num_nodes = 899\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.0,\n",
    "                            is_undirected=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(train_data.edge_index.shape)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    data = train_data\n",
    "    z = z_1_5  # 输出潜在空间的嵌入向量\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "AUC, AP = test()\n",
    "print(f'AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
