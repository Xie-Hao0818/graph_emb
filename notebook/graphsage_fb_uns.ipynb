{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 37.4164,AUC: 0.6925,AP: 0.6403\n",
      "Epoch: 002, Loss: 29.4341,AUC: 0.7454,AP: 0.7017\n",
      "Epoch: 003, Loss: 27.5449,AUC: 0.7696,AP: 0.7369\n",
      "Epoch: 004, Loss: 26.6442,AUC: 0.7921,AP: 0.7667\n",
      "Epoch: 005, Loss: 26.1253,AUC: 0.8082,AP: 0.7890\n",
      "Epoch: 006, Loss: 25.5689,AUC: 0.8237,AP: 0.8091\n",
      "Epoch: 007, Loss: 25.1949,AUC: 0.8262,AP: 0.8146\n",
      "Epoch: 008, Loss: 24.9006,AUC: 0.8368,AP: 0.8293\n",
      "Epoch: 009, Loss: 24.5322,AUC: 0.8409,AP: 0.8358\n",
      "Epoch: 010, Loss: 24.6486,AUC: 0.8446,AP: 0.8396\n",
      "Epoch: 011, Loss: 24.4770,AUC: 0.8528,AP: 0.8480\n",
      "Epoch: 012, Loss: 24.2695,AUC: 0.8490,AP: 0.8448\n",
      "Epoch: 013, Loss: 24.1306,AUC: 0.8573,AP: 0.8523\n",
      "Epoch: 014, Loss: 24.0202,AUC: 0.8610,AP: 0.8558\n",
      "Epoch: 015, Loss: 23.9486,AUC: 0.8631,AP: 0.8588\n",
      "Epoch: 016, Loss: 23.6919,AUC: 0.8649,AP: 0.8616\n",
      "Epoch: 017, Loss: 23.6120,AUC: 0.8683,AP: 0.8668\n",
      "Epoch: 018, Loss: 23.7323,AUC: 0.8673,AP: 0.8674\n",
      "Epoch: 019, Loss: 23.5517,AUC: 0.8778,AP: 0.8774\n",
      "Epoch: 020, Loss: 23.4587,AUC: 0.8764,AP: 0.8771\n",
      "Epoch: 021, Loss: 23.4453,AUC: 0.8797,AP: 0.8810\n",
      "Epoch: 022, Loss: 23.3360,AUC: 0.8803,AP: 0.8831\n",
      "Epoch: 023, Loss: 23.1903,AUC: 0.8851,AP: 0.8878\n",
      "Epoch: 024, Loss: 23.1934,AUC: 0.8879,AP: 0.8892\n",
      "Epoch: 025, Loss: 23.2436,AUC: 0.8946,AP: 0.8963\n",
      "Epoch: 026, Loss: 23.1120,AUC: 0.8903,AP: 0.8933\n",
      "Epoch: 027, Loss: 23.0750,AUC: 0.9000,AP: 0.9028\n",
      "Epoch: 028, Loss: 22.9380,AUC: 0.9022,AP: 0.9043\n",
      "Epoch: 029, Loss: 22.9997,AUC: 0.9041,AP: 0.9069\n",
      "Epoch: 030, Loss: 22.8886,AUC: 0.9053,AP: 0.9092\n",
      "Epoch: 031, Loss: 23.0367,AUC: 0.9025,AP: 0.9066\n",
      "Epoch: 032, Loss: 22.8296,AUC: 0.9055,AP: 0.9097\n",
      "Epoch: 033, Loss: 22.8731,AUC: 0.9069,AP: 0.9108\n",
      "Epoch: 034, Loss: 22.6894,AUC: 0.9073,AP: 0.9107\n",
      "Epoch: 035, Loss: 22.6671,AUC: 0.9030,AP: 0.9072\n",
      "Epoch: 036, Loss: 22.8709,AUC: 0.9102,AP: 0.9147\n",
      "Epoch: 037, Loss: 22.8257,AUC: 0.9101,AP: 0.9148\n",
      "Epoch: 038, Loss: 22.6152,AUC: 0.9087,AP: 0.9132\n",
      "Epoch: 039, Loss: 22.5230,AUC: 0.9076,AP: 0.9119\n",
      "Epoch: 040, Loss: 22.8051,AUC: 0.9078,AP: 0.9107\n",
      "Epoch: 041, Loss: 22.8374,AUC: 0.9104,AP: 0.9145\n",
      "Epoch: 042, Loss: 22.8462,AUC: 0.9151,AP: 0.9194\n",
      "Epoch: 043, Loss: 22.7068,AUC: 0.9105,AP: 0.9153\n",
      "Epoch: 044, Loss: 22.8043,AUC: 0.9136,AP: 0.9184\n",
      "Epoch: 045, Loss: 22.6187,AUC: 0.9132,AP: 0.9182\n",
      "Epoch: 046, Loss: 22.5409,AUC: 0.9136,AP: 0.9189\n",
      "Epoch: 047, Loss: 22.5212,AUC: 0.9127,AP: 0.9174\n",
      "Epoch: 048, Loss: 22.5386,AUC: 0.9133,AP: 0.9178\n",
      "Epoch: 049, Loss: 22.5066,AUC: 0.9166,AP: 0.9194\n",
      "Epoch: 050, Loss: 22.3814,AUC: 0.9118,AP: 0.9161\n",
      "Epoch: 051, Loss: 22.4573,AUC: 0.9092,AP: 0.9143\n",
      "Epoch: 052, Loss: 22.5525,AUC: 0.9109,AP: 0.9160\n",
      "Epoch: 053, Loss: 22.5853,AUC: 0.9149,AP: 0.9199\n",
      "Epoch: 054, Loss: 22.4270,AUC: 0.9126,AP: 0.9174\n",
      "Epoch: 055, Loss: 22.4325,AUC: 0.9150,AP: 0.9183\n",
      "Epoch: 056, Loss: 22.4582,AUC: 0.9141,AP: 0.9191\n",
      "Epoch: 057, Loss: 22.3333,AUC: 0.9172,AP: 0.9218\n",
      "Epoch: 058, Loss: 22.5298,AUC: 0.9194,AP: 0.9252\n",
      "Epoch: 059, Loss: 22.3818,AUC: 0.9161,AP: 0.9212\n",
      "Epoch: 060, Loss: 22.3344,AUC: 0.9163,AP: 0.9215\n",
      "Epoch: 061, Loss: 22.3477,AUC: 0.9134,AP: 0.9182\n",
      "Epoch: 062, Loss: 22.4065,AUC: 0.9130,AP: 0.9187\n",
      "Epoch: 063, Loss: 22.3786,AUC: 0.9181,AP: 0.9221\n",
      "Epoch: 064, Loss: 22.4177,AUC: 0.9192,AP: 0.9238\n",
      "Epoch: 065, Loss: 22.3683,AUC: 0.9191,AP: 0.9232\n",
      "Epoch: 066, Loss: 22.3878,AUC: 0.9186,AP: 0.9226\n",
      "Epoch: 067, Loss: 22.2908,AUC: 0.9198,AP: 0.9239\n",
      "Epoch: 068, Loss: 22.4109,AUC: 0.9196,AP: 0.9233\n",
      "Epoch: 069, Loss: 22.3653,AUC: 0.9212,AP: 0.9235\n",
      "Epoch: 070, Loss: 22.3251,AUC: 0.9208,AP: 0.9249\n",
      "Epoch: 071, Loss: 22.2162,AUC: 0.9221,AP: 0.9252\n",
      "Epoch: 072, Loss: 22.1977,AUC: 0.9238,AP: 0.9266\n",
      "Epoch: 073, Loss: 22.2084,AUC: 0.9209,AP: 0.9236\n",
      "Epoch: 074, Loss: 22.3716,AUC: 0.9251,AP: 0.9276\n",
      "Epoch: 075, Loss: 22.2002,AUC: 0.9235,AP: 0.9269\n",
      "Epoch: 076, Loss: 22.2645,AUC: 0.9248,AP: 0.9282\n",
      "Epoch: 077, Loss: 22.2029,AUC: 0.9272,AP: 0.9305\n",
      "Epoch: 078, Loss: 22.2017,AUC: 0.9263,AP: 0.9292\n",
      "Epoch: 079, Loss: 22.2105,AUC: 0.9286,AP: 0.9308\n",
      "Epoch: 080, Loss: 22.3640,AUC: 0.9247,AP: 0.9274\n",
      "Epoch: 081, Loss: 22.2274,AUC: 0.9262,AP: 0.9283\n",
      "Epoch: 082, Loss: 22.2543,AUC: 0.9263,AP: 0.9288\n",
      "Epoch: 083, Loss: 22.2047,AUC: 0.9290,AP: 0.9318\n",
      "Epoch: 084, Loss: 22.1921,AUC: 0.9240,AP: 0.9279\n",
      "Epoch: 085, Loss: 22.1504,AUC: 0.9292,AP: 0.9330\n",
      "Epoch: 086, Loss: 22.1692,AUC: 0.9276,AP: 0.9315\n",
      "Epoch: 087, Loss: 22.1367,AUC: 0.9274,AP: 0.9312\n",
      "Epoch: 088, Loss: 22.1169,AUC: 0.9244,AP: 0.9282\n",
      "Epoch: 089, Loss: 22.3475,AUC: 0.9297,AP: 0.9333\n",
      "Epoch: 090, Loss: 22.0603,AUC: 0.9239,AP: 0.9284\n",
      "Epoch: 091, Loss: 22.2905,AUC: 0.9263,AP: 0.9297\n",
      "Epoch: 092, Loss: 22.1271,AUC: 0.9259,AP: 0.9283\n",
      "Epoch: 093, Loss: 22.1933,AUC: 0.9247,AP: 0.9282\n",
      "Epoch: 094, Loss: 22.1786,AUC: 0.9257,AP: 0.9306\n",
      "Epoch: 095, Loss: 22.1651,AUC: 0.9294,AP: 0.9342\n",
      "Epoch: 096, Loss: 22.3137,AUC: 0.9277,AP: 0.9320\n",
      "Epoch: 097, Loss: 22.0637,AUC: 0.9236,AP: 0.9276\n",
      "Epoch: 098, Loss: 22.1234,AUC: 0.9265,AP: 0.9309\n",
      "Epoch: 099, Loss: 22.0634,AUC: 0.9300,AP: 0.9334\n",
      "Epoch: 100, Loss: 22.0069,AUC: 0.9290,AP: 0.9311\n",
      "Epoch: 101, Loss: 22.0944,AUC: 0.9244,AP: 0.9279\n",
      "Epoch: 102, Loss: 22.1247,AUC: 0.9293,AP: 0.9313\n",
      "Epoch: 103, Loss: 22.1128,AUC: 0.9293,AP: 0.9316\n",
      "Epoch: 104, Loss: 21.9753,AUC: 0.9286,AP: 0.9313\n",
      "Epoch: 105, Loss: 22.0626,AUC: 0.9305,AP: 0.9325\n",
      "Epoch: 106, Loss: 22.1087,AUC: 0.9273,AP: 0.9307\n",
      "Epoch: 107, Loss: 22.3109,AUC: 0.9277,AP: 0.9305\n",
      "Epoch: 108, Loss: 22.1726,AUC: 0.9285,AP: 0.9321\n",
      "Epoch: 109, Loss: 22.2443,AUC: 0.9294,AP: 0.9324\n",
      "Epoch: 110, Loss: 22.1653,AUC: 0.9281,AP: 0.9310\n",
      "Epoch: 111, Loss: 22.3315,AUC: 0.9307,AP: 0.9330\n",
      "Epoch: 112, Loss: 22.1228,AUC: 0.9317,AP: 0.9340\n",
      "Epoch: 113, Loss: 22.1017,AUC: 0.9265,AP: 0.9305\n",
      "Epoch: 114, Loss: 22.0927,AUC: 0.9314,AP: 0.9337\n",
      "Epoch: 115, Loss: 22.1585,AUC: 0.9299,AP: 0.9329\n",
      "Epoch: 116, Loss: 22.0062,AUC: 0.9295,AP: 0.9322\n",
      "Epoch: 117, Loss: 21.9642,AUC: 0.9264,AP: 0.9303\n",
      "Epoch: 118, Loss: 22.0758,AUC: 0.9280,AP: 0.9325\n",
      "Epoch: 119, Loss: 22.1496,AUC: 0.9309,AP: 0.9345\n",
      "Epoch: 120, Loss: 21.9794,AUC: 0.9332,AP: 0.9373\n",
      "Epoch: 121, Loss: 22.0229,AUC: 0.9282,AP: 0.9323\n",
      "Epoch: 122, Loss: 21.9820,AUC: 0.9258,AP: 0.9306\n",
      "Epoch: 123, Loss: 22.0637,AUC: 0.9302,AP: 0.9341\n",
      "Epoch: 124, Loss: 22.0348,AUC: 0.9286,AP: 0.9331\n",
      "Epoch: 125, Loss: 22.1796,AUC: 0.9237,AP: 0.9277\n",
      "Epoch: 126, Loss: 22.0646,AUC: 0.9273,AP: 0.9305\n",
      "Epoch: 127, Loss: 21.9369,AUC: 0.9312,AP: 0.9345\n",
      "Epoch: 128, Loss: 21.9069,AUC: 0.9301,AP: 0.9336\n",
      "Epoch: 129, Loss: 21.9968,AUC: 0.9303,AP: 0.9336\n",
      "Epoch: 130, Loss: 21.8790,AUC: 0.9298,AP: 0.9340\n",
      "Epoch: 131, Loss: 21.9232,AUC: 0.9310,AP: 0.9346\n",
      "Epoch: 132, Loss: 22.0332,AUC: 0.9293,AP: 0.9326\n",
      "Epoch: 133, Loss: 21.9872,AUC: 0.9293,AP: 0.9326\n",
      "Epoch: 134, Loss: 21.8813,AUC: 0.9307,AP: 0.9347\n",
      "Epoch: 135, Loss: 22.2072,AUC: 0.9271,AP: 0.9309\n",
      "Epoch: 136, Loss: 22.0252,AUC: 0.9334,AP: 0.9355\n",
      "Epoch: 137, Loss: 22.0416,AUC: 0.9295,AP: 0.9322\n",
      "Epoch: 138, Loss: 22.1093,AUC: 0.9294,AP: 0.9322\n",
      "Epoch: 139, Loss: 22.0286,AUC: 0.9317,AP: 0.9332\n",
      "Epoch: 140, Loss: 21.9745,AUC: 0.9317,AP: 0.9323\n",
      "Epoch: 141, Loss: 21.8817,AUC: 0.9298,AP: 0.9304\n",
      "Epoch: 142, Loss: 22.0015,AUC: 0.9260,AP: 0.9274\n",
      "Epoch: 143, Loss: 22.0051,AUC: 0.9315,AP: 0.9335\n",
      "Epoch: 144, Loss: 21.8822,AUC: 0.9284,AP: 0.9302\n",
      "Epoch: 145, Loss: 22.0408,AUC: 0.9313,AP: 0.9344\n",
      "Epoch: 146, Loss: 21.9083,AUC: 0.9286,AP: 0.9332\n",
      "Epoch: 147, Loss: 21.9409,AUC: 0.9291,AP: 0.9338\n",
      "Epoch: 148, Loss: 21.9130,AUC: 0.9313,AP: 0.9350\n",
      "Epoch: 149, Loss: 22.1281,AUC: 0.9254,AP: 0.9297\n",
      "Epoch: 150, Loss: 22.0469,AUC: 0.9298,AP: 0.9325\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m1.pt')\n",
    "\n",
    "data.x = torch.randn((900, 100))\n",
    "train_data, val_data, test_data = RandomLinkSplit(num_val=0.0,\n",
    "                                                  num_test=0.1,\n",
    "                                                  is_undirected=True)(data)\n",
    "\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    "    num_workers=6, persistent_workers=True,\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        batch.edge_index = batch.edge_index % 900\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = model(train_data.x, train_data.edge_index)\n",
    "z1.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 10.2854,AUC: 0.8103,AP: 0.7534\n",
      "Epoch: 002, Loss: 6.4928,AUC: 0.8387,AP: 0.8034\n",
      "Epoch: 003, Loss: 5.8345,AUC: 0.8695,AP: 0.8503\n",
      "Epoch: 004, Loss: 5.5820,AUC: 0.8820,AP: 0.8732\n",
      "Epoch: 005, Loss: 5.3051,AUC: 0.8962,AP: 0.8912\n",
      "Epoch: 006, Loss: 5.2046,AUC: 0.9003,AP: 0.8992\n",
      "Epoch: 007, Loss: 5.1720,AUC: 0.9034,AP: 0.9076\n",
      "Epoch: 008, Loss: 4.9935,AUC: 0.9130,AP: 0.9182\n",
      "Epoch: 009, Loss: 4.9476,AUC: 0.9120,AP: 0.9179\n",
      "Epoch: 010, Loss: 4.8999,AUC: 0.9171,AP: 0.9268\n",
      "Epoch: 011, Loss: 4.7139,AUC: 0.9159,AP: 0.9273\n",
      "Epoch: 012, Loss: 4.7490,AUC: 0.9209,AP: 0.9312\n",
      "Epoch: 013, Loss: 4.7165,AUC: 0.9190,AP: 0.9310\n",
      "Epoch: 014, Loss: 4.6491,AUC: 0.9189,AP: 0.9331\n",
      "Epoch: 015, Loss: 4.5922,AUC: 0.9207,AP: 0.9341\n",
      "Epoch: 016, Loss: 4.5545,AUC: 0.9239,AP: 0.9370\n",
      "Epoch: 017, Loss: 4.5729,AUC: 0.9195,AP: 0.9350\n",
      "Epoch: 018, Loss: 4.5448,AUC: 0.9218,AP: 0.9379\n",
      "Epoch: 019, Loss: 4.5249,AUC: 0.9230,AP: 0.9399\n",
      "Epoch: 020, Loss: 4.5216,AUC: 0.9212,AP: 0.9402\n",
      "Epoch: 021, Loss: 4.5420,AUC: 0.9218,AP: 0.9408\n",
      "Epoch: 022, Loss: 4.4603,AUC: 0.9229,AP: 0.9424\n",
      "Epoch: 023, Loss: 4.4473,AUC: 0.9216,AP: 0.9423\n",
      "Epoch: 024, Loss: 4.3863,AUC: 0.9238,AP: 0.9444\n",
      "Epoch: 025, Loss: 4.4443,AUC: 0.9229,AP: 0.9441\n",
      "Epoch: 026, Loss: 4.3676,AUC: 0.9228,AP: 0.9444\n",
      "Epoch: 027, Loss: 4.4163,AUC: 0.9213,AP: 0.9435\n",
      "Epoch: 028, Loss: 4.3331,AUC: 0.9237,AP: 0.9440\n",
      "Epoch: 029, Loss: 4.4119,AUC: 0.9215,AP: 0.9436\n",
      "Epoch: 030, Loss: 4.3813,AUC: 0.9241,AP: 0.9466\n",
      "Epoch: 031, Loss: 4.3457,AUC: 0.9254,AP: 0.9474\n",
      "Epoch: 032, Loss: 4.3024,AUC: 0.9205,AP: 0.9458\n",
      "Epoch: 033, Loss: 4.3234,AUC: 0.9215,AP: 0.9467\n",
      "Epoch: 034, Loss: 4.2913,AUC: 0.9224,AP: 0.9468\n",
      "Epoch: 035, Loss: 4.3215,AUC: 0.9222,AP: 0.9466\n",
      "Epoch: 036, Loss: 4.3854,AUC: 0.9203,AP: 0.9463\n",
      "Epoch: 037, Loss: 4.3221,AUC: 0.9213,AP: 0.9471\n",
      "Epoch: 038, Loss: 4.3022,AUC: 0.9210,AP: 0.9471\n",
      "Epoch: 039, Loss: 4.2783,AUC: 0.9201,AP: 0.9470\n",
      "Epoch: 040, Loss: 4.2187,AUC: 0.9206,AP: 0.9472\n",
      "Epoch: 041, Loss: 4.3042,AUC: 0.9189,AP: 0.9462\n",
      "Epoch: 042, Loss: 4.2243,AUC: 0.9190,AP: 0.9463\n",
      "Epoch: 043, Loss: 4.2016,AUC: 0.9187,AP: 0.9464\n",
      "Epoch: 044, Loss: 4.2220,AUC: 0.9178,AP: 0.9462\n",
      "Epoch: 045, Loss: 4.2151,AUC: 0.9156,AP: 0.9456\n",
      "Epoch: 046, Loss: 4.2764,AUC: 0.9143,AP: 0.9453\n",
      "Epoch: 047, Loss: 4.2644,AUC: 0.9109,AP: 0.9429\n",
      "Epoch: 048, Loss: 4.1842,AUC: 0.9147,AP: 0.9449\n",
      "Epoch: 049, Loss: 4.2094,AUC: 0.9127,AP: 0.9437\n",
      "Epoch: 050, Loss: 4.1806,AUC: 0.9138,AP: 0.9443\n",
      "Epoch: 051, Loss: 4.1583,AUC: 0.9131,AP: 0.9439\n",
      "Epoch: 052, Loss: 4.1489,AUC: 0.9148,AP: 0.9448\n",
      "Epoch: 053, Loss: 4.1956,AUC: 0.9159,AP: 0.9453\n",
      "Epoch: 054, Loss: 4.1760,AUC: 0.9183,AP: 0.9464\n",
      "Epoch: 055, Loss: 4.1486,AUC: 0.9216,AP: 0.9485\n",
      "Epoch: 056, Loss: 4.1734,AUC: 0.9185,AP: 0.9469\n",
      "Epoch: 057, Loss: 4.2228,AUC: 0.9136,AP: 0.9443\n",
      "Epoch: 058, Loss: 4.1514,AUC: 0.9165,AP: 0.9454\n",
      "Epoch: 059, Loss: 4.2228,AUC: 0.9167,AP: 0.9462\n",
      "Epoch: 060, Loss: 4.2043,AUC: 0.9162,AP: 0.9463\n",
      "Epoch: 061, Loss: 4.1476,AUC: 0.9175,AP: 0.9467\n",
      "Epoch: 062, Loss: 4.1338,AUC: 0.9194,AP: 0.9479\n",
      "Epoch: 063, Loss: 4.1364,AUC: 0.9188,AP: 0.9475\n",
      "Epoch: 064, Loss: 4.2041,AUC: 0.9188,AP: 0.9477\n",
      "Epoch: 065, Loss: 4.1762,AUC: 0.9144,AP: 0.9456\n",
      "Epoch: 066, Loss: 4.1056,AUC: 0.9163,AP: 0.9469\n",
      "Epoch: 067, Loss: 4.1346,AUC: 0.9152,AP: 0.9463\n",
      "Epoch: 068, Loss: 4.2197,AUC: 0.9164,AP: 0.9469\n",
      "Epoch: 069, Loss: 4.1852,AUC: 0.9180,AP: 0.9481\n",
      "Epoch: 070, Loss: 4.1681,AUC: 0.9179,AP: 0.9481\n",
      "Epoch: 071, Loss: 4.0977,AUC: 0.9170,AP: 0.9474\n",
      "Epoch: 072, Loss: 4.1548,AUC: 0.9155,AP: 0.9464\n",
      "Epoch: 073, Loss: 4.1508,AUC: 0.9155,AP: 0.9463\n",
      "Epoch: 074, Loss: 4.0696,AUC: 0.9182,AP: 0.9478\n",
      "Epoch: 075, Loss: 4.1283,AUC: 0.9194,AP: 0.9481\n",
      "Epoch: 076, Loss: 4.1747,AUC: 0.9174,AP: 0.9473\n",
      "Epoch: 077, Loss: 4.1121,AUC: 0.9153,AP: 0.9466\n",
      "Epoch: 078, Loss: 4.0436,AUC: 0.9192,AP: 0.9489\n",
      "Epoch: 079, Loss: 4.1037,AUC: 0.9198,AP: 0.9494\n",
      "Epoch: 080, Loss: 4.1170,AUC: 0.9196,AP: 0.9494\n",
      "Epoch: 081, Loss: 4.1086,AUC: 0.9176,AP: 0.9484\n",
      "Epoch: 082, Loss: 4.1355,AUC: 0.9199,AP: 0.9499\n",
      "Epoch: 083, Loss: 4.1119,AUC: 0.9171,AP: 0.9486\n",
      "Epoch: 084, Loss: 4.0797,AUC: 0.9161,AP: 0.9475\n",
      "Epoch: 085, Loss: 4.0625,AUC: 0.9193,AP: 0.9492\n",
      "Epoch: 086, Loss: 4.0892,AUC: 0.9217,AP: 0.9502\n",
      "Epoch: 087, Loss: 4.1146,AUC: 0.9180,AP: 0.9481\n",
      "Epoch: 088, Loss: 4.0993,AUC: 0.9205,AP: 0.9493\n",
      "Epoch: 089, Loss: 4.0731,AUC: 0.9193,AP: 0.9488\n",
      "Epoch: 090, Loss: 4.1077,AUC: 0.9143,AP: 0.9462\n",
      "Epoch: 091, Loss: 4.1514,AUC: 0.9149,AP: 0.9463\n",
      "Epoch: 092, Loss: 4.0713,AUC: 0.9134,AP: 0.9453\n",
      "Epoch: 093, Loss: 4.0640,AUC: 0.9151,AP: 0.9461\n",
      "Epoch: 094, Loss: 4.1241,AUC: 0.9120,AP: 0.9445\n",
      "Epoch: 095, Loss: 4.0963,AUC: 0.9141,AP: 0.9459\n",
      "Epoch: 096, Loss: 4.0999,AUC: 0.9131,AP: 0.9449\n",
      "Epoch: 097, Loss: 4.0480,AUC: 0.9132,AP: 0.9448\n",
      "Epoch: 098, Loss: 4.0903,AUC: 0.9177,AP: 0.9465\n",
      "Epoch: 099, Loss: 4.0796,AUC: 0.9166,AP: 0.9461\n",
      "Epoch: 100, Loss: 4.0463,AUC: 0.9160,AP: 0.9457\n",
      "Epoch: 101, Loss: 4.0843,AUC: 0.9169,AP: 0.9460\n",
      "Epoch: 102, Loss: 4.1279,AUC: 0.9150,AP: 0.9448\n",
      "Epoch: 103, Loss: 4.0738,AUC: 0.9144,AP: 0.9441\n",
      "Epoch: 104, Loss: 4.0445,AUC: 0.9118,AP: 0.9433\n",
      "Epoch: 105, Loss: 4.0711,AUC: 0.9127,AP: 0.9443\n",
      "Epoch: 106, Loss: 4.0450,AUC: 0.9133,AP: 0.9447\n",
      "Epoch: 107, Loss: 4.1176,AUC: 0.9122,AP: 0.9441\n",
      "Epoch: 108, Loss: 4.0790,AUC: 0.9138,AP: 0.9452\n",
      "Epoch: 109, Loss: 4.1076,AUC: 0.9137,AP: 0.9447\n",
      "Epoch: 110, Loss: 4.1831,AUC: 0.9158,AP: 0.9455\n",
      "Epoch: 111, Loss: 4.0582,AUC: 0.9130,AP: 0.9440\n",
      "Epoch: 112, Loss: 4.0617,AUC: 0.9164,AP: 0.9462\n",
      "Epoch: 113, Loss: 4.0977,AUC: 0.9178,AP: 0.9464\n",
      "Epoch: 114, Loss: 4.0894,AUC: 0.9180,AP: 0.9464\n",
      "Epoch: 115, Loss: 4.0292,AUC: 0.9184,AP: 0.9468\n",
      "Epoch: 116, Loss: 4.0149,AUC: 0.9199,AP: 0.9471\n",
      "Epoch: 117, Loss: 4.0345,AUC: 0.9185,AP: 0.9467\n",
      "Epoch: 118, Loss: 4.0401,AUC: 0.9174,AP: 0.9461\n",
      "Epoch: 119, Loss: 4.0775,AUC: 0.9173,AP: 0.9464\n",
      "Epoch: 120, Loss: 4.0277,AUC: 0.9173,AP: 0.9464\n",
      "Epoch: 121, Loss: 4.0280,AUC: 0.9206,AP: 0.9483\n",
      "Epoch: 122, Loss: 4.0796,AUC: 0.9191,AP: 0.9475\n",
      "Epoch: 123, Loss: 4.0397,AUC: 0.9178,AP: 0.9468\n",
      "Epoch: 124, Loss: 4.0435,AUC: 0.9187,AP: 0.9472\n",
      "Epoch: 125, Loss: 4.0561,AUC: 0.9216,AP: 0.9494\n",
      "Epoch: 126, Loss: 3.9768,AUC: 0.9205,AP: 0.9492\n",
      "Epoch: 127, Loss: 4.0320,AUC: 0.9207,AP: 0.9493\n",
      "Epoch: 128, Loss: 4.0867,AUC: 0.9181,AP: 0.9479\n",
      "Epoch: 129, Loss: 4.0737,AUC: 0.9198,AP: 0.9485\n",
      "Epoch: 130, Loss: 4.0393,AUC: 0.9193,AP: 0.9484\n",
      "Epoch: 131, Loss: 4.0909,AUC: 0.9175,AP: 0.9477\n",
      "Epoch: 132, Loss: 4.0385,AUC: 0.9202,AP: 0.9490\n",
      "Epoch: 133, Loss: 4.0328,AUC: 0.9171,AP: 0.9468\n",
      "Epoch: 134, Loss: 4.0331,AUC: 0.9175,AP: 0.9465\n",
      "Epoch: 135, Loss: 4.0612,AUC: 0.9156,AP: 0.9462\n",
      "Epoch: 136, Loss: 4.0344,AUC: 0.9149,AP: 0.9461\n",
      "Epoch: 137, Loss: 4.0627,AUC: 0.9198,AP: 0.9486\n",
      "Epoch: 138, Loss: 4.0143,AUC: 0.9187,AP: 0.9478\n",
      "Epoch: 139, Loss: 3.9811,AUC: 0.9175,AP: 0.9474\n",
      "Epoch: 140, Loss: 4.0231,AUC: 0.9127,AP: 0.9446\n",
      "Epoch: 141, Loss: 4.0565,AUC: 0.9151,AP: 0.9459\n",
      "Epoch: 142, Loss: 4.0351,AUC: 0.9172,AP: 0.9475\n",
      "Epoch: 143, Loss: 4.0137,AUC: 0.9116,AP: 0.9448\n",
      "Epoch: 144, Loss: 4.0328,AUC: 0.9148,AP: 0.9464\n",
      "Epoch: 145, Loss: 4.0327,AUC: 0.9112,AP: 0.9446\n",
      "Epoch: 146, Loss: 3.9976,AUC: 0.9178,AP: 0.9478\n",
      "Epoch: 147, Loss: 4.0087,AUC: 0.9190,AP: 0.9487\n",
      "Epoch: 148, Loss: 4.0081,AUC: 0.9174,AP: 0.9480\n",
      "Epoch: 149, Loss: 4.0249,AUC: 0.9176,AP: 0.9478\n",
      "Epoch: 150, Loss: 3.9905,AUC: 0.9202,AP: 0.9493\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m2.pt')\n",
    "\n",
    "data.x = torch.randn((900, 100))\n",
    "train_data, val_data, test_data = RandomLinkSplit(num_val=0.0,\n",
    "                                                  num_test=0.1,\n",
    "                                                  is_undirected=True)(data)\n",
    "\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    "    num_workers=6, persistent_workers=True,\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        batch.edge_index = batch.edge_index % 900\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = model(train_data.x, train_data.edge_index)\n",
    "z2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 7.4458,AUC: 0.8248,AP: 0.7799\n",
      "Epoch: 002, Loss: 4.2932,AUC: 0.8691,AP: 0.8472\n",
      "Epoch: 003, Loss: 3.9417,AUC: 0.8697,AP: 0.8637\n",
      "Epoch: 004, Loss: 3.5727,AUC: 0.8925,AP: 0.8926\n",
      "Epoch: 005, Loss: 3.3667,AUC: 0.8917,AP: 0.8982\n",
      "Epoch: 006, Loss: 3.2432,AUC: 0.9049,AP: 0.9146\n",
      "Epoch: 007, Loss: 3.3137,AUC: 0.8995,AP: 0.9131\n",
      "Epoch: 008, Loss: 3.1570,AUC: 0.9072,AP: 0.9209\n",
      "Epoch: 009, Loss: 3.1824,AUC: 0.9062,AP: 0.9210\n",
      "Epoch: 010, Loss: 3.1550,AUC: 0.9145,AP: 0.9291\n",
      "Epoch: 011, Loss: 3.1284,AUC: 0.9058,AP: 0.9232\n",
      "Epoch: 012, Loss: 3.1092,AUC: 0.9095,AP: 0.9272\n",
      "Epoch: 013, Loss: 3.0678,AUC: 0.9063,AP: 0.9261\n",
      "Epoch: 014, Loss: 3.0919,AUC: 0.9110,AP: 0.9308\n",
      "Epoch: 015, Loss: 2.9214,AUC: 0.9118,AP: 0.9318\n",
      "Epoch: 016, Loss: 3.0100,AUC: 0.9136,AP: 0.9336\n",
      "Epoch: 017, Loss: 2.9838,AUC: 0.9118,AP: 0.9333\n",
      "Epoch: 018, Loss: 2.9792,AUC: 0.9137,AP: 0.9355\n",
      "Epoch: 019, Loss: 3.0149,AUC: 0.9077,AP: 0.9319\n",
      "Epoch: 020, Loss: 2.9759,AUC: 0.9129,AP: 0.9364\n",
      "Epoch: 021, Loss: 2.9680,AUC: 0.9089,AP: 0.9342\n",
      "Epoch: 022, Loss: 2.9969,AUC: 0.9085,AP: 0.9350\n",
      "Epoch: 023, Loss: 2.9282,AUC: 0.9092,AP: 0.9352\n",
      "Epoch: 024, Loss: 2.8991,AUC: 0.9079,AP: 0.9352\n",
      "Epoch: 025, Loss: 2.8480,AUC: 0.9087,AP: 0.9365\n",
      "Epoch: 026, Loss: 2.9998,AUC: 0.9039,AP: 0.9338\n",
      "Epoch: 027, Loss: 2.8799,AUC: 0.9100,AP: 0.9383\n",
      "Epoch: 028, Loss: 2.8236,AUC: 0.9139,AP: 0.9402\n",
      "Epoch: 029, Loss: 2.8749,AUC: 0.9084,AP: 0.9364\n",
      "Epoch: 030, Loss: 2.8654,AUC: 0.9091,AP: 0.9366\n",
      "Epoch: 031, Loss: 2.8402,AUC: 0.9098,AP: 0.9371\n",
      "Epoch: 032, Loss: 2.7995,AUC: 0.9132,AP: 0.9390\n",
      "Epoch: 033, Loss: 2.8380,AUC: 0.9139,AP: 0.9396\n",
      "Epoch: 034, Loss: 2.8354,AUC: 0.9099,AP: 0.9366\n",
      "Epoch: 035, Loss: 2.8687,AUC: 0.9124,AP: 0.9382\n",
      "Epoch: 036, Loss: 2.8103,AUC: 0.9056,AP: 0.9344\n",
      "Epoch: 037, Loss: 2.8576,AUC: 0.9106,AP: 0.9374\n",
      "Epoch: 038, Loss: 2.7616,AUC: 0.9088,AP: 0.9369\n",
      "Epoch: 039, Loss: 2.8728,AUC: 0.9078,AP: 0.9362\n",
      "Epoch: 040, Loss: 2.8113,AUC: 0.9065,AP: 0.9345\n",
      "Epoch: 041, Loss: 2.7671,AUC: 0.9141,AP: 0.9401\n",
      "Epoch: 042, Loss: 2.8162,AUC: 0.9112,AP: 0.9385\n",
      "Epoch: 043, Loss: 2.7936,AUC: 0.9092,AP: 0.9375\n",
      "Epoch: 044, Loss: 2.7971,AUC: 0.9092,AP: 0.9366\n",
      "Epoch: 045, Loss: 2.7905,AUC: 0.9071,AP: 0.9356\n",
      "Epoch: 046, Loss: 2.8000,AUC: 0.9120,AP: 0.9395\n",
      "Epoch: 047, Loss: 2.7828,AUC: 0.9110,AP: 0.9393\n",
      "Epoch: 048, Loss: 2.7597,AUC: 0.9101,AP: 0.9392\n",
      "Epoch: 049, Loss: 2.7649,AUC: 0.9081,AP: 0.9382\n",
      "Epoch: 050, Loss: 2.7762,AUC: 0.9079,AP: 0.9379\n",
      "Epoch: 051, Loss: 2.7789,AUC: 0.9100,AP: 0.9395\n",
      "Epoch: 052, Loss: 2.7737,AUC: 0.9064,AP: 0.9368\n",
      "Epoch: 053, Loss: 2.7436,AUC: 0.9076,AP: 0.9379\n",
      "Epoch: 054, Loss: 2.7143,AUC: 0.9075,AP: 0.9377\n",
      "Epoch: 055, Loss: 2.7187,AUC: 0.9074,AP: 0.9379\n",
      "Epoch: 056, Loss: 2.7324,AUC: 0.9093,AP: 0.9398\n",
      "Epoch: 057, Loss: 2.7648,AUC: 0.9051,AP: 0.9378\n",
      "Epoch: 058, Loss: 2.7362,AUC: 0.9127,AP: 0.9423\n",
      "Epoch: 059, Loss: 2.7238,AUC: 0.9100,AP: 0.9409\n",
      "Epoch: 060, Loss: 2.7250,AUC: 0.9124,AP: 0.9425\n",
      "Epoch: 061, Loss: 2.7397,AUC: 0.9075,AP: 0.9396\n",
      "Epoch: 062, Loss: 2.7140,AUC: 0.9094,AP: 0.9403\n",
      "Epoch: 063, Loss: 2.7584,AUC: 0.9076,AP: 0.9395\n",
      "Epoch: 064, Loss: 2.7300,AUC: 0.9055,AP: 0.9388\n",
      "Epoch: 065, Loss: 2.7384,AUC: 0.9087,AP: 0.9402\n",
      "Epoch: 066, Loss: 2.7547,AUC: 0.9055,AP: 0.9383\n",
      "Epoch: 067, Loss: 2.7171,AUC: 0.9082,AP: 0.9395\n",
      "Epoch: 068, Loss: 2.7381,AUC: 0.9073,AP: 0.9396\n",
      "Epoch: 069, Loss: 2.7328,AUC: 0.9084,AP: 0.9394\n",
      "Epoch: 070, Loss: 2.7421,AUC: 0.9092,AP: 0.9400\n",
      "Epoch: 071, Loss: 2.7183,AUC: 0.9087,AP: 0.9392\n",
      "Epoch: 072, Loss: 2.6695,AUC: 0.9131,AP: 0.9422\n",
      "Epoch: 073, Loss: 2.7143,AUC: 0.9093,AP: 0.9397\n",
      "Epoch: 074, Loss: 2.6969,AUC: 0.9086,AP: 0.9401\n",
      "Epoch: 075, Loss: 2.7336,AUC: 0.9131,AP: 0.9421\n",
      "Epoch: 076, Loss: 2.7406,AUC: 0.9152,AP: 0.9432\n",
      "Epoch: 077, Loss: 2.7025,AUC: 0.9112,AP: 0.9406\n",
      "Epoch: 078, Loss: 2.7104,AUC: 0.9179,AP: 0.9434\n",
      "Epoch: 079, Loss: 2.6624,AUC: 0.9122,AP: 0.9406\n",
      "Epoch: 080, Loss: 2.6946,AUC: 0.9124,AP: 0.9408\n",
      "Epoch: 081, Loss: 2.7082,AUC: 0.9107,AP: 0.9400\n",
      "Epoch: 082, Loss: 2.7446,AUC: 0.9173,AP: 0.9435\n",
      "Epoch: 083, Loss: 2.6673,AUC: 0.9165,AP: 0.9430\n",
      "Epoch: 084, Loss: 2.7234,AUC: 0.9131,AP: 0.9405\n",
      "Epoch: 085, Loss: 2.6712,AUC: 0.9208,AP: 0.9457\n",
      "Epoch: 086, Loss: 2.7139,AUC: 0.9147,AP: 0.9421\n",
      "Epoch: 087, Loss: 2.6944,AUC: 0.9117,AP: 0.9409\n",
      "Epoch: 088, Loss: 2.7000,AUC: 0.9124,AP: 0.9412\n",
      "Epoch: 089, Loss: 2.6947,AUC: 0.9078,AP: 0.9392\n",
      "Epoch: 090, Loss: 2.7125,AUC: 0.9091,AP: 0.9406\n",
      "Epoch: 091, Loss: 2.6997,AUC: 0.9059,AP: 0.9393\n",
      "Epoch: 092, Loss: 2.6898,AUC: 0.9078,AP: 0.9404\n",
      "Epoch: 093, Loss: 2.6940,AUC: 0.9116,AP: 0.9424\n",
      "Epoch: 094, Loss: 2.6450,AUC: 0.9054,AP: 0.9399\n",
      "Epoch: 095, Loss: 2.6476,AUC: 0.9111,AP: 0.9427\n",
      "Epoch: 096, Loss: 2.7258,AUC: 0.9055,AP: 0.9404\n",
      "Epoch: 097, Loss: 2.7291,AUC: 0.9089,AP: 0.9420\n",
      "Epoch: 098, Loss: 2.6766,AUC: 0.9111,AP: 0.9431\n",
      "Epoch: 099, Loss: 2.7050,AUC: 0.9098,AP: 0.9425\n",
      "Epoch: 100, Loss: 2.7078,AUC: 0.9111,AP: 0.9429\n",
      "Epoch: 101, Loss: 2.6940,AUC: 0.9118,AP: 0.9427\n",
      "Epoch: 102, Loss: 2.6666,AUC: 0.9167,AP: 0.9459\n",
      "Epoch: 103, Loss: 2.6669,AUC: 0.9138,AP: 0.9439\n",
      "Epoch: 104, Loss: 2.6777,AUC: 0.9092,AP: 0.9409\n",
      "Epoch: 105, Loss: 2.6772,AUC: 0.9081,AP: 0.9401\n",
      "Epoch: 106, Loss: 2.7214,AUC: 0.9097,AP: 0.9412\n",
      "Epoch: 107, Loss: 2.6551,AUC: 0.9059,AP: 0.9389\n",
      "Epoch: 108, Loss: 2.6671,AUC: 0.9101,AP: 0.9420\n",
      "Epoch: 109, Loss: 2.6571,AUC: 0.9047,AP: 0.9387\n",
      "Epoch: 110, Loss: 2.6629,AUC: 0.9120,AP: 0.9427\n",
      "Epoch: 111, Loss: 2.7190,AUC: 0.9065,AP: 0.9392\n",
      "Epoch: 112, Loss: 2.6861,AUC: 0.9101,AP: 0.9406\n",
      "Epoch: 113, Loss: 2.6811,AUC: 0.9101,AP: 0.9412\n",
      "Epoch: 114, Loss: 2.6787,AUC: 0.9068,AP: 0.9396\n",
      "Epoch: 115, Loss: 2.6731,AUC: 0.9065,AP: 0.9393\n",
      "Epoch: 116, Loss: 2.6848,AUC: 0.9012,AP: 0.9365\n",
      "Epoch: 117, Loss: 2.6306,AUC: 0.9088,AP: 0.9404\n",
      "Epoch: 118, Loss: 2.6240,AUC: 0.9066,AP: 0.9398\n",
      "Epoch: 119, Loss: 2.6548,AUC: 0.9060,AP: 0.9388\n",
      "Epoch: 120, Loss: 2.7045,AUC: 0.9011,AP: 0.9362\n",
      "Epoch: 121, Loss: 2.6733,AUC: 0.9033,AP: 0.9373\n",
      "Epoch: 122, Loss: 2.6400,AUC: 0.9059,AP: 0.9393\n",
      "Epoch: 123, Loss: 2.6805,AUC: 0.9034,AP: 0.9379\n",
      "Epoch: 124, Loss: 2.6960,AUC: 0.9081,AP: 0.9412\n",
      "Epoch: 125, Loss: 2.6872,AUC: 0.9048,AP: 0.9395\n",
      "Epoch: 126, Loss: 2.6937,AUC: 0.9071,AP: 0.9409\n",
      "Epoch: 127, Loss: 2.7370,AUC: 0.9002,AP: 0.9372\n",
      "Epoch: 128, Loss: 2.6476,AUC: 0.9068,AP: 0.9408\n",
      "Epoch: 129, Loss: 2.6620,AUC: 0.9033,AP: 0.9392\n",
      "Epoch: 130, Loss: 2.6980,AUC: 0.9044,AP: 0.9401\n",
      "Epoch: 131, Loss: 2.6916,AUC: 0.9009,AP: 0.9380\n",
      "Epoch: 132, Loss: 2.7141,AUC: 0.9046,AP: 0.9397\n",
      "Epoch: 133, Loss: 2.6631,AUC: 0.9046,AP: 0.9399\n",
      "Epoch: 134, Loss: 2.7050,AUC: 0.9020,AP: 0.9381\n",
      "Epoch: 135, Loss: 2.6808,AUC: 0.9080,AP: 0.9416\n",
      "Epoch: 136, Loss: 2.6467,AUC: 0.9029,AP: 0.9383\n",
      "Epoch: 137, Loss: 2.6327,AUC: 0.9047,AP: 0.9391\n",
      "Epoch: 138, Loss: 2.6355,AUC: 0.8981,AP: 0.9359\n",
      "Epoch: 139, Loss: 2.6688,AUC: 0.8972,AP: 0.9361\n",
      "Epoch: 140, Loss: 2.6945,AUC: 0.8987,AP: 0.9361\n",
      "Epoch: 141, Loss: 2.6747,AUC: 0.9023,AP: 0.9382\n",
      "Epoch: 142, Loss: 2.6585,AUC: 0.8980,AP: 0.9355\n",
      "Epoch: 143, Loss: 2.6543,AUC: 0.9032,AP: 0.9387\n",
      "Epoch: 144, Loss: 2.6508,AUC: 0.9000,AP: 0.9355\n",
      "Epoch: 145, Loss: 2.6556,AUC: 0.9009,AP: 0.9362\n",
      "Epoch: 146, Loss: 2.6605,AUC: 0.8988,AP: 0.9345\n",
      "Epoch: 147, Loss: 2.6524,AUC: 0.8990,AP: 0.9352\n",
      "Epoch: 148, Loss: 2.6700,AUC: 0.9003,AP: 0.9357\n",
      "Epoch: 149, Loss: 2.6462,AUC: 0.9053,AP: 0.9386\n",
      "Epoch: 150, Loss: 2.6491,AUC: 0.9058,AP: 0.9387\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m3.pt')\n",
    "\n",
    "data.x = torch.randn((900, 100))\n",
    "train_data, val_data, test_data = RandomLinkSplit(num_val=0.0,\n",
    "                                                  num_test=0.1,\n",
    "                                                  is_undirected=True)(data)\n",
    "\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    "    num_workers=6, persistent_workers=True,\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        batch.edge_index = batch.edge_index % 900\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3 = model(train_data.x, train_data.edge_index)\n",
    "z3.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 6.8096,AUC: 0.8941,AP: 0.8626\n",
      "Epoch: 002, Loss: 3.7950,AUC: 0.9230,AP: 0.9242\n",
      "Epoch: 003, Loss: 3.3581,AUC: 0.9251,AP: 0.9348\n",
      "Epoch: 004, Loss: 3.1450,AUC: 0.9336,AP: 0.9481\n",
      "Epoch: 005, Loss: 3.1139,AUC: 0.9338,AP: 0.9504\n",
      "Epoch: 006, Loss: 2.9684,AUC: 0.9384,AP: 0.9561\n",
      "Epoch: 007, Loss: 2.9562,AUC: 0.9362,AP: 0.9548\n",
      "Epoch: 008, Loss: 2.9521,AUC: 0.9394,AP: 0.9581\n",
      "Epoch: 009, Loss: 2.9351,AUC: 0.9388,AP: 0.9581\n",
      "Epoch: 010, Loss: 2.8882,AUC: 0.9410,AP: 0.9604\n",
      "Epoch: 011, Loss: 2.8039,AUC: 0.9407,AP: 0.9601\n",
      "Epoch: 012, Loss: 2.8790,AUC: 0.9397,AP: 0.9590\n",
      "Epoch: 013, Loss: 2.7647,AUC: 0.9397,AP: 0.9595\n",
      "Epoch: 014, Loss: 2.8617,AUC: 0.9398,AP: 0.9602\n",
      "Epoch: 015, Loss: 2.7679,AUC: 0.9389,AP: 0.9598\n",
      "Epoch: 016, Loss: 2.7982,AUC: 0.9393,AP: 0.9597\n",
      "Epoch: 017, Loss: 2.7176,AUC: 0.9404,AP: 0.9607\n",
      "Epoch: 018, Loss: 2.8252,AUC: 0.9377,AP: 0.9594\n",
      "Epoch: 019, Loss: 2.7471,AUC: 0.9400,AP: 0.9608\n",
      "Epoch: 020, Loss: 2.7483,AUC: 0.9405,AP: 0.9610\n",
      "Epoch: 021, Loss: 2.7538,AUC: 0.9414,AP: 0.9618\n",
      "Epoch: 022, Loss: 2.7211,AUC: 0.9428,AP: 0.9625\n",
      "Epoch: 023, Loss: 2.7307,AUC: 0.9445,AP: 0.9637\n",
      "Epoch: 024, Loss: 2.7089,AUC: 0.9440,AP: 0.9634\n",
      "Epoch: 025, Loss: 2.7002,AUC: 0.9474,AP: 0.9653\n",
      "Epoch: 026, Loss: 2.7414,AUC: 0.9438,AP: 0.9632\n",
      "Epoch: 027, Loss: 2.6741,AUC: 0.9482,AP: 0.9660\n",
      "Epoch: 028, Loss: 2.6073,AUC: 0.9464,AP: 0.9650\n",
      "Epoch: 029, Loss: 2.6661,AUC: 0.9487,AP: 0.9664\n",
      "Epoch: 030, Loss: 2.6378,AUC: 0.9477,AP: 0.9656\n",
      "Epoch: 031, Loss: 2.6594,AUC: 0.9482,AP: 0.9655\n",
      "Epoch: 032, Loss: 2.6594,AUC: 0.9474,AP: 0.9650\n",
      "Epoch: 033, Loss: 2.6616,AUC: 0.9480,AP: 0.9651\n",
      "Epoch: 034, Loss: 2.5857,AUC: 0.9495,AP: 0.9661\n",
      "Epoch: 035, Loss: 2.6363,AUC: 0.9492,AP: 0.9658\n",
      "Epoch: 036, Loss: 2.6353,AUC: 0.9471,AP: 0.9646\n",
      "Epoch: 037, Loss: 2.6089,AUC: 0.9509,AP: 0.9671\n",
      "Epoch: 038, Loss: 2.5981,AUC: 0.9512,AP: 0.9672\n",
      "Epoch: 039, Loss: 2.6864,AUC: 0.9506,AP: 0.9666\n",
      "Epoch: 040, Loss: 2.5543,AUC: 0.9512,AP: 0.9671\n",
      "Epoch: 041, Loss: 2.6190,AUC: 0.9503,AP: 0.9663\n",
      "Epoch: 042, Loss: 2.5912,AUC: 0.9533,AP: 0.9685\n",
      "Epoch: 043, Loss: 2.6007,AUC: 0.9487,AP: 0.9653\n",
      "Epoch: 044, Loss: 2.5727,AUC: 0.9511,AP: 0.9667\n",
      "Epoch: 045, Loss: 2.6406,AUC: 0.9506,AP: 0.9661\n",
      "Epoch: 046, Loss: 2.5971,AUC: 0.9515,AP: 0.9672\n",
      "Epoch: 047, Loss: 2.5479,AUC: 0.9516,AP: 0.9673\n",
      "Epoch: 048, Loss: 2.5580,AUC: 0.9500,AP: 0.9663\n",
      "Epoch: 049, Loss: 2.5726,AUC: 0.9500,AP: 0.9664\n",
      "Epoch: 050, Loss: 2.6252,AUC: 0.9476,AP: 0.9645\n",
      "Epoch: 051, Loss: 2.5677,AUC: 0.9510,AP: 0.9666\n",
      "Epoch: 052, Loss: 2.5553,AUC: 0.9501,AP: 0.9661\n",
      "Epoch: 053, Loss: 2.5546,AUC: 0.9500,AP: 0.9667\n",
      "Epoch: 054, Loss: 2.6199,AUC: 0.9497,AP: 0.9663\n",
      "Epoch: 055, Loss: 2.5576,AUC: 0.9480,AP: 0.9657\n",
      "Epoch: 056, Loss: 2.5762,AUC: 0.9497,AP: 0.9668\n",
      "Epoch: 057, Loss: 2.5485,AUC: 0.9509,AP: 0.9673\n",
      "Epoch: 058, Loss: 2.5630,AUC: 0.9513,AP: 0.9672\n",
      "Epoch: 059, Loss: 2.5594,AUC: 0.9529,AP: 0.9693\n",
      "Epoch: 060, Loss: 2.5332,AUC: 0.9510,AP: 0.9677\n",
      "Epoch: 061, Loss: 2.5473,AUC: 0.9512,AP: 0.9679\n",
      "Epoch: 062, Loss: 2.5277,AUC: 0.9537,AP: 0.9696\n",
      "Epoch: 063, Loss: 2.5760,AUC: 0.9510,AP: 0.9679\n",
      "Epoch: 064, Loss: 2.5812,AUC: 0.9522,AP: 0.9682\n",
      "Epoch: 065, Loss: 2.5580,AUC: 0.9520,AP: 0.9683\n",
      "Epoch: 066, Loss: 2.5561,AUC: 0.9497,AP: 0.9666\n",
      "Epoch: 067, Loss: 2.5699,AUC: 0.9525,AP: 0.9684\n",
      "Epoch: 068, Loss: 2.5611,AUC: 0.9520,AP: 0.9675\n",
      "Epoch: 069, Loss: 2.5275,AUC: 0.9486,AP: 0.9651\n",
      "Epoch: 070, Loss: 2.5742,AUC: 0.9480,AP: 0.9652\n",
      "Epoch: 071, Loss: 2.5331,AUC: 0.9473,AP: 0.9652\n",
      "Epoch: 072, Loss: 2.4768,AUC: 0.9494,AP: 0.9669\n",
      "Epoch: 073, Loss: 2.5351,AUC: 0.9483,AP: 0.9665\n",
      "Epoch: 074, Loss: 2.5586,AUC: 0.9486,AP: 0.9663\n",
      "Epoch: 075, Loss: 2.5710,AUC: 0.9487,AP: 0.9662\n",
      "Epoch: 076, Loss: 2.5327,AUC: 0.9467,AP: 0.9648\n",
      "Epoch: 077, Loss: 2.5159,AUC: 0.9480,AP: 0.9656\n",
      "Epoch: 078, Loss: 2.5206,AUC: 0.9483,AP: 0.9662\n",
      "Epoch: 079, Loss: 2.4945,AUC: 0.9484,AP: 0.9663\n",
      "Epoch: 080, Loss: 2.5682,AUC: 0.9502,AP: 0.9668\n",
      "Epoch: 081, Loss: 2.5257,AUC: 0.9486,AP: 0.9657\n",
      "Epoch: 082, Loss: 2.5274,AUC: 0.9513,AP: 0.9674\n",
      "Epoch: 083, Loss: 2.5000,AUC: 0.9485,AP: 0.9659\n",
      "Epoch: 084, Loss: 2.5410,AUC: 0.9477,AP: 0.9654\n",
      "Epoch: 085, Loss: 2.5577,AUC: 0.9478,AP: 0.9662\n",
      "Epoch: 086, Loss: 2.5325,AUC: 0.9488,AP: 0.9668\n",
      "Epoch: 087, Loss: 2.5124,AUC: 0.9490,AP: 0.9670\n",
      "Epoch: 088, Loss: 2.5339,AUC: 0.9485,AP: 0.9664\n",
      "Epoch: 089, Loss: 2.4932,AUC: 0.9506,AP: 0.9677\n",
      "Epoch: 090, Loss: 2.5214,AUC: 0.9487,AP: 0.9671\n",
      "Epoch: 091, Loss: 2.5395,AUC: 0.9495,AP: 0.9679\n",
      "Epoch: 092, Loss: 2.5310,AUC: 0.9488,AP: 0.9677\n",
      "Epoch: 093, Loss: 2.4908,AUC: 0.9501,AP: 0.9686\n",
      "Epoch: 094, Loss: 2.5345,AUC: 0.9480,AP: 0.9673\n",
      "Epoch: 095, Loss: 2.5090,AUC: 0.9486,AP: 0.9678\n",
      "Epoch: 096, Loss: 2.5239,AUC: 0.9499,AP: 0.9688\n",
      "Epoch: 097, Loss: 2.5280,AUC: 0.9500,AP: 0.9685\n",
      "Epoch: 098, Loss: 2.4997,AUC: 0.9504,AP: 0.9694\n",
      "Epoch: 099, Loss: 2.5268,AUC: 0.9473,AP: 0.9668\n",
      "Epoch: 100, Loss: 2.4871,AUC: 0.9497,AP: 0.9685\n",
      "Epoch: 101, Loss: 2.5442,AUC: 0.9463,AP: 0.9664\n",
      "Epoch: 102, Loss: 2.5173,AUC: 0.9479,AP: 0.9677\n",
      "Epoch: 103, Loss: 2.5119,AUC: 0.9468,AP: 0.9671\n",
      "Epoch: 104, Loss: 2.4966,AUC: 0.9474,AP: 0.9676\n",
      "Epoch: 105, Loss: 2.5291,AUC: 0.9467,AP: 0.9667\n",
      "Epoch: 106, Loss: 2.5144,AUC: 0.9466,AP: 0.9666\n",
      "Epoch: 107, Loss: 2.5180,AUC: 0.9444,AP: 0.9649\n",
      "Epoch: 108, Loss: 2.4962,AUC: 0.9478,AP: 0.9669\n",
      "Epoch: 109, Loss: 2.4812,AUC: 0.9469,AP: 0.9668\n",
      "Epoch: 110, Loss: 2.5066,AUC: 0.9475,AP: 0.9669\n",
      "Epoch: 111, Loss: 2.5081,AUC: 0.9447,AP: 0.9654\n",
      "Epoch: 112, Loss: 2.4964,AUC: 0.9418,AP: 0.9632\n",
      "Epoch: 113, Loss: 2.5293,AUC: 0.9443,AP: 0.9648\n",
      "Epoch: 114, Loss: 2.4929,AUC: 0.9449,AP: 0.9650\n",
      "Epoch: 115, Loss: 2.4834,AUC: 0.9476,AP: 0.9669\n",
      "Epoch: 116, Loss: 2.4824,AUC: 0.9454,AP: 0.9658\n",
      "Epoch: 117, Loss: 2.4746,AUC: 0.9442,AP: 0.9651\n",
      "Epoch: 118, Loss: 2.4749,AUC: 0.9432,AP: 0.9647\n",
      "Epoch: 119, Loss: 2.4663,AUC: 0.9406,AP: 0.9629\n",
      "Epoch: 120, Loss: 2.4688,AUC: 0.9409,AP: 0.9631\n",
      "Epoch: 121, Loss: 2.5039,AUC: 0.9386,AP: 0.9616\n",
      "Epoch: 122, Loss: 2.4802,AUC: 0.9428,AP: 0.9631\n",
      "Epoch: 123, Loss: 2.4934,AUC: 0.9419,AP: 0.9633\n",
      "Epoch: 124, Loss: 2.5077,AUC: 0.9433,AP: 0.9638\n",
      "Epoch: 125, Loss: 2.4941,AUC: 0.9424,AP: 0.9635\n",
      "Epoch: 126, Loss: 2.5086,AUC: 0.9413,AP: 0.9623\n",
      "Epoch: 127, Loss: 2.4874,AUC: 0.9376,AP: 0.9610\n",
      "Epoch: 128, Loss: 2.4566,AUC: 0.9368,AP: 0.9606\n",
      "Epoch: 129, Loss: 2.4955,AUC: 0.9377,AP: 0.9610\n",
      "Epoch: 130, Loss: 2.5073,AUC: 0.9396,AP: 0.9623\n",
      "Epoch: 131, Loss: 2.4714,AUC: 0.9464,AP: 0.9663\n",
      "Epoch: 132, Loss: 2.4885,AUC: 0.9423,AP: 0.9639\n",
      "Epoch: 133, Loss: 2.4763,AUC: 0.9444,AP: 0.9656\n",
      "Epoch: 134, Loss: 2.4822,AUC: 0.9424,AP: 0.9642\n",
      "Epoch: 135, Loss: 2.5359,AUC: 0.9418,AP: 0.9641\n",
      "Epoch: 136, Loss: 2.5051,AUC: 0.9428,AP: 0.9645\n",
      "Epoch: 137, Loss: 2.4770,AUC: 0.9439,AP: 0.9650\n",
      "Epoch: 138, Loss: 2.4487,AUC: 0.9435,AP: 0.9649\n",
      "Epoch: 139, Loss: 2.4987,AUC: 0.9405,AP: 0.9635\n",
      "Epoch: 140, Loss: 2.4719,AUC: 0.9402,AP: 0.9625\n",
      "Epoch: 141, Loss: 2.4941,AUC: 0.9408,AP: 0.9640\n",
      "Epoch: 142, Loss: 2.4930,AUC: 0.9373,AP: 0.9616\n",
      "Epoch: 143, Loss: 2.4851,AUC: 0.9389,AP: 0.9627\n",
      "Epoch: 144, Loss: 2.5091,AUC: 0.9400,AP: 0.9630\n",
      "Epoch: 145, Loss: 2.4666,AUC: 0.9404,AP: 0.9634\n",
      "Epoch: 146, Loss: 2.4537,AUC: 0.9411,AP: 0.9645\n",
      "Epoch: 147, Loss: 2.5037,AUC: 0.9383,AP: 0.9630\n",
      "Epoch: 148, Loss: 2.4694,AUC: 0.9381,AP: 0.9625\n",
      "Epoch: 149, Loss: 2.4741,AUC: 0.9376,AP: 0.9618\n",
      "Epoch: 150, Loss: 2.4728,AUC: 0.9366,AP: 0.9619\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m4.pt')\n",
    "\n",
    "data.x = torch.randn((900, 100))\n",
    "train_data, val_data, test_data = RandomLinkSplit(num_val=0.0,\n",
    "                                                  num_test=0.1,\n",
    "                                                  is_undirected=True)(data)\n",
    "\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    "    num_workers=6, persistent_workers=True,\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        batch.edge_index = batch.edge_index % 900\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z4 = model(train_data.x, train_data.edge_index)\n",
    "z4.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 5.1830,AUC: 0.7893,AP: 0.7621\n",
      "Epoch: 002, Loss: 2.8774,AUC: 0.8879,AP: 0.8671\n",
      "Epoch: 003, Loss: 2.3904,AUC: 0.8999,AP: 0.8858\n",
      "Epoch: 004, Loss: 2.1730,AUC: 0.9034,AP: 0.8996\n",
      "Epoch: 005, Loss: 2.0577,AUC: 0.9072,AP: 0.9075\n",
      "Epoch: 006, Loss: 2.0629,AUC: 0.9102,AP: 0.9114\n",
      "Epoch: 007, Loss: 2.0658,AUC: 0.9063,AP: 0.9102\n",
      "Epoch: 008, Loss: 1.9709,AUC: 0.9090,AP: 0.9107\n",
      "Epoch: 009, Loss: 1.9070,AUC: 0.9144,AP: 0.9140\n",
      "Epoch: 010, Loss: 1.9540,AUC: 0.9129,AP: 0.9137\n",
      "Epoch: 011, Loss: 1.8977,AUC: 0.9105,AP: 0.9184\n",
      "Epoch: 012, Loss: 1.8991,AUC: 0.9135,AP: 0.9279\n",
      "Epoch: 013, Loss: 1.8641,AUC: 0.9116,AP: 0.9265\n",
      "Epoch: 014, Loss: 1.7728,AUC: 0.9125,AP: 0.9288\n",
      "Epoch: 015, Loss: 1.8545,AUC: 0.9124,AP: 0.9304\n",
      "Epoch: 016, Loss: 1.8082,AUC: 0.9075,AP: 0.9277\n",
      "Epoch: 017, Loss: 1.8366,AUC: 0.9084,AP: 0.9286\n",
      "Epoch: 018, Loss: 1.8287,AUC: 0.9080,AP: 0.9288\n",
      "Epoch: 019, Loss: 1.8158,AUC: 0.9059,AP: 0.9293\n",
      "Epoch: 020, Loss: 1.8177,AUC: 0.9066,AP: 0.9312\n",
      "Epoch: 021, Loss: 1.7836,AUC: 0.9037,AP: 0.9279\n",
      "Epoch: 022, Loss: 1.7978,AUC: 0.9017,AP: 0.9285\n",
      "Epoch: 023, Loss: 1.7699,AUC: 0.9036,AP: 0.9320\n",
      "Epoch: 024, Loss: 1.7947,AUC: 0.9070,AP: 0.9355\n",
      "Epoch: 025, Loss: 1.7793,AUC: 0.9078,AP: 0.9347\n",
      "Epoch: 026, Loss: 1.7381,AUC: 0.9089,AP: 0.9352\n",
      "Epoch: 027, Loss: 1.8232,AUC: 0.9094,AP: 0.9352\n",
      "Epoch: 028, Loss: 1.7940,AUC: 0.9062,AP: 0.9340\n",
      "Epoch: 029, Loss: 1.7539,AUC: 0.9072,AP: 0.9337\n",
      "Epoch: 030, Loss: 1.7552,AUC: 0.9085,AP: 0.9334\n",
      "Epoch: 031, Loss: 1.7805,AUC: 0.9084,AP: 0.9352\n",
      "Epoch: 032, Loss: 1.7583,AUC: 0.9106,AP: 0.9373\n",
      "Epoch: 033, Loss: 1.7306,AUC: 0.9111,AP: 0.9394\n",
      "Epoch: 034, Loss: 1.7264,AUC: 0.9073,AP: 0.9390\n",
      "Epoch: 035, Loss: 1.7155,AUC: 0.9082,AP: 0.9402\n",
      "Epoch: 036, Loss: 1.7353,AUC: 0.9097,AP: 0.9405\n",
      "Epoch: 037, Loss: 1.7538,AUC: 0.9050,AP: 0.9386\n",
      "Epoch: 038, Loss: 1.7257,AUC: 0.9066,AP: 0.9398\n",
      "Epoch: 039, Loss: 1.6704,AUC: 0.9096,AP: 0.9415\n",
      "Epoch: 040, Loss: 1.7291,AUC: 0.9066,AP: 0.9396\n",
      "Epoch: 041, Loss: 1.6833,AUC: 0.9086,AP: 0.9409\n",
      "Epoch: 042, Loss: 1.6818,AUC: 0.9114,AP: 0.9425\n",
      "Epoch: 043, Loss: 1.7415,AUC: 0.9086,AP: 0.9400\n",
      "Epoch: 044, Loss: 1.6839,AUC: 0.9084,AP: 0.9397\n",
      "Epoch: 045, Loss: 1.7093,AUC: 0.9136,AP: 0.9428\n",
      "Epoch: 046, Loss: 1.6871,AUC: 0.9119,AP: 0.9422\n",
      "Epoch: 047, Loss: 1.7035,AUC: 0.9099,AP: 0.9411\n",
      "Epoch: 048, Loss: 1.6876,AUC: 0.9110,AP: 0.9413\n",
      "Epoch: 049, Loss: 1.6674,AUC: 0.9100,AP: 0.9411\n",
      "Epoch: 050, Loss: 1.6703,AUC: 0.9078,AP: 0.9401\n",
      "Epoch: 051, Loss: 1.6821,AUC: 0.9092,AP: 0.9407\n",
      "Epoch: 052, Loss: 1.7001,AUC: 0.9122,AP: 0.9433\n",
      "Epoch: 053, Loss: 1.6518,AUC: 0.9112,AP: 0.9427\n",
      "Epoch: 054, Loss: 1.6721,AUC: 0.9096,AP: 0.9414\n",
      "Epoch: 055, Loss: 1.6469,AUC: 0.9105,AP: 0.9411\n",
      "Epoch: 056, Loss: 1.6764,AUC: 0.9120,AP: 0.9421\n",
      "Epoch: 057, Loss: 1.6771,AUC: 0.9106,AP: 0.9419\n",
      "Epoch: 058, Loss: 1.6414,AUC: 0.9097,AP: 0.9411\n",
      "Epoch: 059, Loss: 1.6456,AUC: 0.9110,AP: 0.9410\n",
      "Epoch: 060, Loss: 1.6286,AUC: 0.9104,AP: 0.9400\n",
      "Epoch: 061, Loss: 1.6654,AUC: 0.9084,AP: 0.9401\n",
      "Epoch: 062, Loss: 1.6618,AUC: 0.9051,AP: 0.9380\n",
      "Epoch: 063, Loss: 1.6452,AUC: 0.9081,AP: 0.9399\n",
      "Epoch: 064, Loss: 1.6585,AUC: 0.9094,AP: 0.9401\n",
      "Epoch: 065, Loss: 1.6708,AUC: 0.9071,AP: 0.9385\n",
      "Epoch: 066, Loss: 1.6485,AUC: 0.9052,AP: 0.9375\n",
      "Epoch: 067, Loss: 1.6432,AUC: 0.9008,AP: 0.9348\n",
      "Epoch: 068, Loss: 1.6622,AUC: 0.9031,AP: 0.9361\n",
      "Epoch: 069, Loss: 1.6743,AUC: 0.9037,AP: 0.9388\n",
      "Epoch: 070, Loss: 1.6928,AUC: 0.8986,AP: 0.9351\n",
      "Epoch: 071, Loss: 1.6139,AUC: 0.9019,AP: 0.9366\n",
      "Epoch: 072, Loss: 1.6846,AUC: 0.9057,AP: 0.9393\n",
      "Epoch: 073, Loss: 1.6576,AUC: 0.8967,AP: 0.9337\n",
      "Epoch: 074, Loss: 1.6690,AUC: 0.8987,AP: 0.9349\n",
      "Epoch: 075, Loss: 1.6303,AUC: 0.9007,AP: 0.9363\n",
      "Epoch: 076, Loss: 1.6495,AUC: 0.9048,AP: 0.9394\n",
      "Epoch: 077, Loss: 1.6302,AUC: 0.9024,AP: 0.9374\n",
      "Epoch: 078, Loss: 1.6528,AUC: 0.9011,AP: 0.9377\n",
      "Epoch: 079, Loss: 1.6535,AUC: 0.9020,AP: 0.9388\n",
      "Epoch: 080, Loss: 1.6366,AUC: 0.8989,AP: 0.9361\n",
      "Epoch: 081, Loss: 1.6991,AUC: 0.8938,AP: 0.9324\n",
      "Epoch: 082, Loss: 1.6388,AUC: 0.8915,AP: 0.9317\n",
      "Epoch: 083, Loss: 1.6503,AUC: 0.8899,AP: 0.9309\n",
      "Epoch: 084, Loss: 1.6391,AUC: 0.8891,AP: 0.9303\n",
      "Epoch: 085, Loss: 1.6413,AUC: 0.8928,AP: 0.9320\n",
      "Epoch: 086, Loss: 1.6229,AUC: 0.8902,AP: 0.9304\n",
      "Epoch: 087, Loss: 1.6354,AUC: 0.8878,AP: 0.9297\n",
      "Epoch: 088, Loss: 1.6546,AUC: 0.8892,AP: 0.9308\n",
      "Epoch: 089, Loss: 1.6514,AUC: 0.8925,AP: 0.9327\n",
      "Epoch: 090, Loss: 1.6791,AUC: 0.8898,AP: 0.9312\n",
      "Epoch: 091, Loss: 1.6287,AUC: 0.8913,AP: 0.9323\n",
      "Epoch: 092, Loss: 1.6567,AUC: 0.8903,AP: 0.9320\n",
      "Epoch: 093, Loss: 1.6476,AUC: 0.8921,AP: 0.9330\n",
      "Epoch: 094, Loss: 1.6136,AUC: 0.8981,AP: 0.9364\n",
      "Epoch: 095, Loss: 1.6472,AUC: 0.8974,AP: 0.9367\n",
      "Epoch: 096, Loss: 1.6842,AUC: 0.8940,AP: 0.9343\n",
      "Epoch: 097, Loss: 1.6368,AUC: 0.9035,AP: 0.9401\n",
      "Epoch: 098, Loss: 1.6053,AUC: 0.9042,AP: 0.9406\n",
      "Epoch: 099, Loss: 1.6422,AUC: 0.9009,AP: 0.9382\n",
      "Epoch: 100, Loss: 1.6448,AUC: 0.8959,AP: 0.9349\n",
      "Epoch: 101, Loss: 1.6441,AUC: 0.9011,AP: 0.9383\n",
      "Epoch: 102, Loss: 1.6550,AUC: 0.9004,AP: 0.9376\n",
      "Epoch: 103, Loss: 1.6439,AUC: 0.9011,AP: 0.9386\n",
      "Epoch: 104, Loss: 1.6021,AUC: 0.9046,AP: 0.9402\n",
      "Epoch: 105, Loss: 1.6136,AUC: 0.9046,AP: 0.9406\n",
      "Epoch: 106, Loss: 1.6098,AUC: 0.9025,AP: 0.9388\n",
      "Epoch: 107, Loss: 1.6124,AUC: 0.9095,AP: 0.9429\n",
      "Epoch: 108, Loss: 1.6320,AUC: 0.9013,AP: 0.9376\n",
      "Epoch: 109, Loss: 1.5977,AUC: 0.9014,AP: 0.9380\n",
      "Epoch: 110, Loss: 1.6232,AUC: 0.9061,AP: 0.9398\n",
      "Epoch: 111, Loss: 1.6154,AUC: 0.9060,AP: 0.9400\n",
      "Epoch: 112, Loss: 1.6764,AUC: 0.9015,AP: 0.9369\n",
      "Epoch: 113, Loss: 1.6138,AUC: 0.9044,AP: 0.9384\n",
      "Epoch: 114, Loss: 1.6062,AUC: 0.9015,AP: 0.9355\n",
      "Epoch: 115, Loss: 1.6045,AUC: 0.8991,AP: 0.9350\n",
      "Epoch: 116, Loss: 1.6200,AUC: 0.9024,AP: 0.9383\n",
      "Epoch: 117, Loss: 1.6287,AUC: 0.9009,AP: 0.9370\n",
      "Epoch: 118, Loss: 1.6252,AUC: 0.8981,AP: 0.9365\n",
      "Epoch: 119, Loss: 1.5874,AUC: 0.9038,AP: 0.9407\n",
      "Epoch: 120, Loss: 1.6363,AUC: 0.9004,AP: 0.9384\n",
      "Epoch: 121, Loss: 1.6240,AUC: 0.9002,AP: 0.9381\n",
      "Epoch: 122, Loss: 1.5975,AUC: 0.9043,AP: 0.9409\n",
      "Epoch: 123, Loss: 1.5979,AUC: 0.9064,AP: 0.9414\n",
      "Epoch: 124, Loss: 1.6086,AUC: 0.9039,AP: 0.9391\n",
      "Epoch: 125, Loss: 1.6268,AUC: 0.9038,AP: 0.9388\n",
      "Epoch: 126, Loss: 1.6273,AUC: 0.9005,AP: 0.9373\n",
      "Epoch: 127, Loss: 1.6294,AUC: 0.8914,AP: 0.9324\n",
      "Epoch: 128, Loss: 1.6208,AUC: 0.8933,AP: 0.9336\n",
      "Epoch: 129, Loss: 1.6293,AUC: 0.8970,AP: 0.9357\n",
      "Epoch: 130, Loss: 1.5721,AUC: 0.8921,AP: 0.9340\n",
      "Epoch: 131, Loss: 1.6171,AUC: 0.8954,AP: 0.9357\n",
      "Epoch: 132, Loss: 1.6395,AUC: 0.8969,AP: 0.9361\n",
      "Epoch: 133, Loss: 1.5965,AUC: 0.8900,AP: 0.9320\n",
      "Epoch: 134, Loss: 1.5991,AUC: 0.8945,AP: 0.9344\n",
      "Epoch: 135, Loss: 1.5894,AUC: 0.9003,AP: 0.9373\n",
      "Epoch: 136, Loss: 1.6006,AUC: 0.8946,AP: 0.9345\n",
      "Epoch: 137, Loss: 1.5607,AUC: 0.8987,AP: 0.9360\n",
      "Epoch: 138, Loss: 1.6267,AUC: 0.8998,AP: 0.9368\n",
      "Epoch: 139, Loss: 1.6340,AUC: 0.8927,AP: 0.9332\n",
      "Epoch: 140, Loss: 1.6192,AUC: 0.8892,AP: 0.9310\n",
      "Epoch: 141, Loss: 1.6176,AUC: 0.9010,AP: 0.9370\n",
      "Epoch: 142, Loss: 1.6380,AUC: 0.8901,AP: 0.9316\n",
      "Epoch: 143, Loss: 1.6260,AUC: 0.8999,AP: 0.9368\n",
      "Epoch: 144, Loss: 1.6275,AUC: 0.8996,AP: 0.9371\n",
      "Epoch: 145, Loss: 1.6120,AUC: 0.8925,AP: 0.9331\n",
      "Epoch: 146, Loss: 1.6158,AUC: 0.8956,AP: 0.9351\n",
      "Epoch: 147, Loss: 1.5902,AUC: 0.9040,AP: 0.9399\n",
      "Epoch: 148, Loss: 1.6223,AUC: 0.8948,AP: 0.9344\n",
      "Epoch: 149, Loss: 1.5910,AUC: 0.8887,AP: 0.9310\n",
      "Epoch: 150, Loss: 1.6358,AUC: 0.8945,AP: 0.9346\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m5.pt')\n",
    "\n",
    "data.x = torch.randn((900, 100))\n",
    "train_data, val_data, test_data = RandomLinkSplit(num_val=0.0,\n",
    "                                                  num_test=0.1,\n",
    "                                                  is_undirected=True)(data)\n",
    "\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    "    num_workers=6, persistent_workers=True,\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        batch.edge_index = batch.edge_index % 900\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 128])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z5 = model(train_data.x, train_data.edge_index)\n",
    "z5.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([900, 640])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_1_5 = torch.cat([z1, z2, z3, z4, z5], 1)\n",
    "z_1_5.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# torch.save(z_1_5, '/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/z_1_5_gs.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 584])\n",
      "AUC: 0.9541,AP: 0.9711\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/fb_snapshots/m6.pt')\n",
    "data = data.to(device)\n",
    "data.num_nodes = 899\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.0,\n",
    "                            is_undirected=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(train_data.edge_index.shape)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    data = train_data\n",
    "    z = z_1_5  # 输出潜在空间的嵌入向量\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "AUC, AP = test()\n",
    "print(f'AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
