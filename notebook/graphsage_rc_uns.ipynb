{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.2746,AUC: 0.9203,AP: 0.9486\n",
      "Epoch: 002, Loss: 0.1741,AUC: 0.9283,AP: 0.9534\n",
      "Epoch: 003, Loss: 0.1662,AUC: 0.9312,AP: 0.9550\n",
      "Epoch: 004, Loss: 0.1638,AUC: 0.9375,AP: 0.9587\n",
      "Epoch: 005, Loss: 0.1631,AUC: 0.9379,AP: 0.9591\n",
      "Epoch: 006, Loss: 0.1629,AUC: 0.9409,AP: 0.9611\n",
      "Epoch: 007, Loss: 0.1597,AUC: 0.9391,AP: 0.9604\n",
      "Epoch: 008, Loss: 0.1595,AUC: 0.9429,AP: 0.9627\n",
      "Epoch: 009, Loss: 0.1587,AUC: 0.9425,AP: 0.9628\n",
      "Epoch: 010, Loss: 0.1582,AUC: 0.9458,AP: 0.9649\n",
      "Epoch: 011, Loss: 0.1577,AUC: 0.9425,AP: 0.9634\n",
      "Epoch: 012, Loss: 0.1564,AUC: 0.9431,AP: 0.9638\n",
      "Epoch: 013, Loss: 0.1552,AUC: 0.9494,AP: 0.9679\n",
      "Epoch: 014, Loss: 0.1557,AUC: 0.9458,AP: 0.9661\n",
      "Epoch: 015, Loss: 0.1546,AUC: 0.9445,AP: 0.9653\n",
      "Epoch: 016, Loss: 0.1534,AUC: 0.9455,AP: 0.9660\n",
      "Epoch: 017, Loss: 0.1524,AUC: 0.9436,AP: 0.9654\n",
      "Epoch: 018, Loss: 0.1538,AUC: 0.9466,AP: 0.9671\n",
      "Epoch: 019, Loss: 0.1537,AUC: 0.9470,AP: 0.9672\n",
      "Epoch: 020, Loss: 0.1538,AUC: 0.9469,AP: 0.9673\n",
      "Epoch: 021, Loss: 0.1518,AUC: 0.9477,AP: 0.9678\n",
      "Epoch: 022, Loss: 0.1531,AUC: 0.9500,AP: 0.9692\n",
      "Epoch: 023, Loss: 0.1535,AUC: 0.9497,AP: 0.9692\n",
      "Epoch: 024, Loss: 0.1531,AUC: 0.9496,AP: 0.9693\n",
      "Epoch: 025, Loss: 0.1513,AUC: 0.9460,AP: 0.9676\n",
      "Epoch: 026, Loss: 0.1511,AUC: 0.9468,AP: 0.9683\n",
      "Epoch: 027, Loss: 0.1511,AUC: 0.9479,AP: 0.9689\n",
      "Epoch: 028, Loss: 0.1523,AUC: 0.9487,AP: 0.9694\n",
      "Epoch: 029, Loss: 0.1500,AUC: 0.9482,AP: 0.9690\n",
      "Epoch: 030, Loss: 0.1521,AUC: 0.9502,AP: 0.9701\n",
      "Epoch: 031, Loss: 0.1515,AUC: 0.9486,AP: 0.9695\n",
      "Epoch: 032, Loss: 0.1495,AUC: 0.9492,AP: 0.9699\n",
      "Epoch: 033, Loss: 0.1501,AUC: 0.9470,AP: 0.9687\n",
      "Epoch: 034, Loss: 0.1500,AUC: 0.9474,AP: 0.9689\n",
      "Epoch: 035, Loss: 0.1498,AUC: 0.9481,AP: 0.9694\n",
      "Epoch: 036, Loss: 0.1515,AUC: 0.9503,AP: 0.9706\n",
      "Epoch: 037, Loss: 0.1502,AUC: 0.9453,AP: 0.9680\n",
      "Epoch: 038, Loss: 0.1491,AUC: 0.9477,AP: 0.9693\n",
      "Epoch: 039, Loss: 0.1496,AUC: 0.9430,AP: 0.9668\n",
      "Epoch: 040, Loss: 0.1492,AUC: 0.9449,AP: 0.9679\n",
      "Epoch: 041, Loss: 0.1495,AUC: 0.9459,AP: 0.9685\n",
      "Epoch: 042, Loss: 0.1485,AUC: 0.9468,AP: 0.9689\n",
      "Epoch: 043, Loss: 0.1499,AUC: 0.9493,AP: 0.9700\n",
      "Epoch: 044, Loss: 0.1501,AUC: 0.9458,AP: 0.9682\n",
      "Epoch: 045, Loss: 0.1482,AUC: 0.9477,AP: 0.9691\n",
      "Epoch: 046, Loss: 0.1490,AUC: 0.9461,AP: 0.9683\n",
      "Epoch: 047, Loss: 0.1486,AUC: 0.9460,AP: 0.9683\n",
      "Epoch: 048, Loss: 0.1504,AUC: 0.9486,AP: 0.9697\n",
      "Epoch: 049, Loss: 0.1482,AUC: 0.9476,AP: 0.9691\n",
      "Epoch: 050, Loss: 0.1492,AUC: 0.9482,AP: 0.9695\n",
      "Epoch: 051, Loss: 0.1481,AUC: 0.9512,AP: 0.9710\n",
      "Epoch: 052, Loss: 0.1484,AUC: 0.9501,AP: 0.9703\n",
      "Epoch: 053, Loss: 0.1484,AUC: 0.9506,AP: 0.9706\n",
      "Epoch: 054, Loss: 0.1476,AUC: 0.9490,AP: 0.9699\n",
      "Epoch: 055, Loss: 0.1487,AUC: 0.9497,AP: 0.9704\n",
      "Epoch: 056, Loss: 0.1479,AUC: 0.9492,AP: 0.9701\n",
      "Epoch: 057, Loss: 0.1475,AUC: 0.9506,AP: 0.9711\n",
      "Epoch: 058, Loss: 0.1472,AUC: 0.9415,AP: 0.9669\n",
      "Epoch: 059, Loss: 0.1486,AUC: 0.9461,AP: 0.9691\n",
      "Epoch: 060, Loss: 0.1474,AUC: 0.9408,AP: 0.9664\n",
      "Epoch: 061, Loss: 0.1478,AUC: 0.9399,AP: 0.9660\n",
      "Epoch: 062, Loss: 0.1489,AUC: 0.9507,AP: 0.9710\n",
      "Epoch: 063, Loss: 0.1479,AUC: 0.9487,AP: 0.9701\n",
      "Epoch: 064, Loss: 0.1477,AUC: 0.9500,AP: 0.9707\n",
      "Epoch: 065, Loss: 0.1479,AUC: 0.9493,AP: 0.9704\n",
      "Epoch: 066, Loss: 0.1479,AUC: 0.9440,AP: 0.9680\n",
      "Epoch: 067, Loss: 0.1479,AUC: 0.9435,AP: 0.9676\n",
      "Epoch: 068, Loss: 0.1483,AUC: 0.9477,AP: 0.9695\n",
      "Epoch: 069, Loss: 0.1465,AUC: 0.9493,AP: 0.9700\n",
      "Epoch: 070, Loss: 0.1479,AUC: 0.9452,AP: 0.9681\n",
      "Epoch: 071, Loss: 0.1473,AUC: 0.9497,AP: 0.9703\n",
      "Epoch: 072, Loss: 0.1475,AUC: 0.9517,AP: 0.9716\n",
      "Epoch: 073, Loss: 0.1469,AUC: 0.9488,AP: 0.9700\n",
      "Epoch: 074, Loss: 0.1470,AUC: 0.9473,AP: 0.9693\n",
      "Epoch: 075, Loss: 0.1469,AUC: 0.9484,AP: 0.9698\n",
      "Epoch: 076, Loss: 0.1476,AUC: 0.9502,AP: 0.9709\n",
      "Epoch: 077, Loss: 0.1474,AUC: 0.9467,AP: 0.9693\n",
      "Epoch: 078, Loss: 0.1473,AUC: 0.9493,AP: 0.9706\n",
      "Epoch: 079, Loss: 0.1469,AUC: 0.9510,AP: 0.9714\n",
      "Epoch: 080, Loss: 0.1482,AUC: 0.9475,AP: 0.9698\n",
      "Epoch: 081, Loss: 0.1471,AUC: 0.9495,AP: 0.9709\n",
      "Epoch: 082, Loss: 0.1479,AUC: 0.9430,AP: 0.9680\n",
      "Epoch: 083, Loss: 0.1469,AUC: 0.9467,AP: 0.9700\n",
      "Epoch: 084, Loss: 0.1465,AUC: 0.9461,AP: 0.9699\n",
      "Epoch: 085, Loss: 0.1467,AUC: 0.9499,AP: 0.9715\n",
      "Epoch: 086, Loss: 0.1462,AUC: 0.9429,AP: 0.9682\n",
      "Epoch: 087, Loss: 0.1469,AUC: 0.9448,AP: 0.9692\n",
      "Epoch: 088, Loss: 0.1470,AUC: 0.9477,AP: 0.9704\n",
      "Epoch: 089, Loss: 0.1465,AUC: 0.9465,AP: 0.9694\n",
      "Epoch: 090, Loss: 0.1466,AUC: 0.9462,AP: 0.9693\n",
      "Epoch: 091, Loss: 0.1474,AUC: 0.9466,AP: 0.9692\n",
      "Epoch: 092, Loss: 0.1462,AUC: 0.9508,AP: 0.9713\n",
      "Epoch: 093, Loss: 0.1466,AUC: 0.9468,AP: 0.9692\n",
      "Epoch: 094, Loss: 0.1461,AUC: 0.9500,AP: 0.9712\n",
      "Epoch: 095, Loss: 0.1471,AUC: 0.9471,AP: 0.9699\n",
      "Epoch: 096, Loss: 0.1454,AUC: 0.9487,AP: 0.9708\n",
      "Epoch: 097, Loss: 0.1459,AUC: 0.9481,AP: 0.9702\n",
      "Epoch: 098, Loss: 0.1476,AUC: 0.9493,AP: 0.9712\n",
      "Epoch: 099, Loss: 0.1457,AUC: 0.9499,AP: 0.9717\n",
      "Epoch: 100, Loss: 0.1458,AUC: 0.9487,AP: 0.9711\n",
      "Epoch: 101, Loss: 0.1459,AUC: 0.9473,AP: 0.9705\n",
      "Epoch: 102, Loss: 0.1464,AUC: 0.9550,AP: 0.9749\n",
      "Epoch: 103, Loss: 0.1463,AUC: 0.9446,AP: 0.9691\n",
      "Epoch: 104, Loss: 0.1463,AUC: 0.9467,AP: 0.9705\n",
      "Epoch: 105, Loss: 0.1466,AUC: 0.9438,AP: 0.9686\n",
      "Epoch: 106, Loss: 0.1464,AUC: 0.9417,AP: 0.9674\n",
      "Epoch: 107, Loss: 0.1461,AUC: 0.9428,AP: 0.9678\n",
      "Epoch: 108, Loss: 0.1470,AUC: 0.9461,AP: 0.9697\n",
      "Epoch: 109, Loss: 0.1460,AUC: 0.9501,AP: 0.9723\n",
      "Epoch: 110, Loss: 0.1468,AUC: 0.9467,AP: 0.9709\n",
      "Epoch: 111, Loss: 0.1472,AUC: 0.9476,AP: 0.9712\n",
      "Epoch: 112, Loss: 0.1458,AUC: 0.9479,AP: 0.9711\n",
      "Epoch: 113, Loss: 0.1463,AUC: 0.9448,AP: 0.9699\n",
      "Epoch: 114, Loss: 0.1463,AUC: 0.9449,AP: 0.9702\n",
      "Epoch: 115, Loss: 0.1462,AUC: 0.9431,AP: 0.9690\n",
      "Epoch: 116, Loss: 0.1469,AUC: 0.9457,AP: 0.9704\n",
      "Epoch: 117, Loss: 0.1461,AUC: 0.9467,AP: 0.9709\n",
      "Epoch: 118, Loss: 0.1465,AUC: 0.9468,AP: 0.9705\n",
      "Epoch: 119, Loss: 0.1457,AUC: 0.9508,AP: 0.9731\n",
      "Epoch: 120, Loss: 0.1461,AUC: 0.9543,AP: 0.9749\n",
      "Epoch: 121, Loss: 0.1456,AUC: 0.9511,AP: 0.9735\n",
      "Epoch: 122, Loss: 0.1469,AUC: 0.9496,AP: 0.9728\n",
      "Epoch: 123, Loss: 0.1464,AUC: 0.9514,AP: 0.9738\n",
      "Epoch: 124, Loss: 0.1462,AUC: 0.9529,AP: 0.9743\n",
      "Epoch: 125, Loss: 0.1459,AUC: 0.9512,AP: 0.9729\n",
      "Epoch: 126, Loss: 0.1461,AUC: 0.9510,AP: 0.9730\n",
      "Epoch: 127, Loss: 0.1473,AUC: 0.9486,AP: 0.9718\n",
      "Epoch: 128, Loss: 0.1463,AUC: 0.9504,AP: 0.9730\n",
      "Epoch: 129, Loss: 0.1458,AUC: 0.9510,AP: 0.9733\n",
      "Epoch: 130, Loss: 0.1468,AUC: 0.9448,AP: 0.9699\n",
      "Epoch: 131, Loss: 0.1460,AUC: 0.9446,AP: 0.9694\n",
      "Epoch: 132, Loss: 0.1459,AUC: 0.9422,AP: 0.9679\n",
      "Epoch: 133, Loss: 0.1460,AUC: 0.9414,AP: 0.9680\n",
      "Epoch: 134, Loss: 0.1452,AUC: 0.9437,AP: 0.9695\n",
      "Epoch: 135, Loss: 0.1459,AUC: 0.9422,AP: 0.9689\n",
      "Epoch: 136, Loss: 0.1453,AUC: 0.9462,AP: 0.9708\n",
      "Epoch: 137, Loss: 0.1461,AUC: 0.9422,AP: 0.9685\n",
      "Epoch: 138, Loss: 0.1453,AUC: 0.9427,AP: 0.9688\n",
      "Epoch: 139, Loss: 0.1459,AUC: 0.9389,AP: 0.9667\n",
      "Epoch: 140, Loss: 0.1453,AUC: 0.9442,AP: 0.9691\n",
      "Epoch: 141, Loss: 0.1458,AUC: 0.9456,AP: 0.9697\n",
      "Epoch: 142, Loss: 0.1458,AUC: 0.9396,AP: 0.9667\n",
      "Epoch: 143, Loss: 0.1458,AUC: 0.9467,AP: 0.9702\n",
      "Epoch: 144, Loss: 0.1458,AUC: 0.9450,AP: 0.9696\n",
      "Epoch: 145, Loss: 0.1458,AUC: 0.9499,AP: 0.9720\n",
      "Epoch: 146, Loss: 0.1455,AUC: 0.9508,AP: 0.9729\n",
      "Epoch: 147, Loss: 0.1458,AUC: 0.9469,AP: 0.9704\n",
      "Epoch: 148, Loss: 0.1460,AUC: 0.9394,AP: 0.9666\n",
      "Epoch: 149, Loss: 0.1457,AUC: 0.9463,AP: 0.9708\n",
      "Epoch: 150, Loss: 0.1452,AUC: 0.9455,AP: 0.9705\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/f1.pt')\n",
    "data['edge_index'] = torch.tensor(data['edge_index'])\n",
    "data['edge_attr'] = torch.tensor(data['edge_attr'])\n",
    "data['t_start'] = torch.tensor(data['t_start'])\n",
    "data['num_nodes'] = 27045\n",
    "data['x'] = torch.randn((27045, 100))\n",
    "data = data.to(device, 'x', 'edge_index', 'edge_attr')\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=False)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "assert train_data.edge_index.max() < train_data.num_nodes  # !!!!!!!!\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    in_channels=train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([27045, 128])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = model(train_data.x, train_data.edge_index)\n",
    "z1.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.3251,AUC: 0.9138,AP: 0.9483\n",
      "Epoch: 002, Loss: 0.1999,AUC: 0.9209,AP: 0.9516\n",
      "Epoch: 003, Loss: 0.1874,AUC: 0.9256,AP: 0.9543\n",
      "Epoch: 004, Loss: 0.1852,AUC: 0.9298,AP: 0.9570\n",
      "Epoch: 005, Loss: 0.1839,AUC: 0.9312,AP: 0.9583\n",
      "Epoch: 006, Loss: 0.1838,AUC: 0.9313,AP: 0.9586\n",
      "Epoch: 007, Loss: 0.1795,AUC: 0.9335,AP: 0.9599\n",
      "Epoch: 008, Loss: 0.1808,AUC: 0.9319,AP: 0.9591\n",
      "Epoch: 009, Loss: 0.1805,AUC: 0.9325,AP: 0.9596\n",
      "Epoch: 010, Loss: 0.1798,AUC: 0.9300,AP: 0.9586\n",
      "Epoch: 011, Loss: 0.1772,AUC: 0.9309,AP: 0.9593\n",
      "Epoch: 012, Loss: 0.1771,AUC: 0.9314,AP: 0.9598\n",
      "Epoch: 013, Loss: 0.1769,AUC: 0.9337,AP: 0.9611\n",
      "Epoch: 014, Loss: 0.1750,AUC: 0.9343,AP: 0.9617\n",
      "Epoch: 015, Loss: 0.1742,AUC: 0.9368,AP: 0.9630\n",
      "Epoch: 016, Loss: 0.1763,AUC: 0.9373,AP: 0.9632\n",
      "Epoch: 017, Loss: 0.1745,AUC: 0.9341,AP: 0.9616\n",
      "Epoch: 018, Loss: 0.1749,AUC: 0.9340,AP: 0.9617\n",
      "Epoch: 019, Loss: 0.1732,AUC: 0.9363,AP: 0.9628\n",
      "Epoch: 020, Loss: 0.1728,AUC: 0.9362,AP: 0.9629\n",
      "Epoch: 021, Loss: 0.1742,AUC: 0.9387,AP: 0.9644\n",
      "Epoch: 022, Loss: 0.1724,AUC: 0.9366,AP: 0.9635\n",
      "Epoch: 023, Loss: 0.1714,AUC: 0.9362,AP: 0.9636\n",
      "Epoch: 024, Loss: 0.1722,AUC: 0.9356,AP: 0.9634\n",
      "Epoch: 025, Loss: 0.1722,AUC: 0.9371,AP: 0.9642\n",
      "Epoch: 026, Loss: 0.1717,AUC: 0.9370,AP: 0.9643\n",
      "Epoch: 027, Loss: 0.1721,AUC: 0.9383,AP: 0.9651\n",
      "Epoch: 028, Loss: 0.1704,AUC: 0.9355,AP: 0.9637\n",
      "Epoch: 029, Loss: 0.1712,AUC: 0.9370,AP: 0.9645\n",
      "Epoch: 030, Loss: 0.1714,AUC: 0.9373,AP: 0.9646\n",
      "Epoch: 031, Loss: 0.1718,AUC: 0.9368,AP: 0.9644\n",
      "Epoch: 032, Loss: 0.1704,AUC: 0.9347,AP: 0.9635\n",
      "Epoch: 033, Loss: 0.1697,AUC: 0.9351,AP: 0.9638\n",
      "Epoch: 034, Loss: 0.1700,AUC: 0.9342,AP: 0.9634\n",
      "Epoch: 035, Loss: 0.1696,AUC: 0.9367,AP: 0.9647\n",
      "Epoch: 036, Loss: 0.1691,AUC: 0.9403,AP: 0.9664\n",
      "Epoch: 037, Loss: 0.1703,AUC: 0.9361,AP: 0.9645\n",
      "Epoch: 038, Loss: 0.1694,AUC: 0.9370,AP: 0.9649\n",
      "Epoch: 039, Loss: 0.1689,AUC: 0.9393,AP: 0.9661\n",
      "Epoch: 040, Loss: 0.1686,AUC: 0.9366,AP: 0.9648\n",
      "Epoch: 041, Loss: 0.1677,AUC: 0.9375,AP: 0.9652\n",
      "Epoch: 042, Loss: 0.1680,AUC: 0.9362,AP: 0.9647\n",
      "Epoch: 043, Loss: 0.1684,AUC: 0.9369,AP: 0.9652\n",
      "Epoch: 044, Loss: 0.1679,AUC: 0.9361,AP: 0.9648\n",
      "Epoch: 045, Loss: 0.1677,AUC: 0.9359,AP: 0.9647\n",
      "Epoch: 046, Loss: 0.1675,AUC: 0.9363,AP: 0.9650\n",
      "Epoch: 047, Loss: 0.1669,AUC: 0.9372,AP: 0.9655\n",
      "Epoch: 048, Loss: 0.1679,AUC: 0.9377,AP: 0.9656\n",
      "Epoch: 049, Loss: 0.1666,AUC: 0.9379,AP: 0.9656\n",
      "Epoch: 050, Loss: 0.1669,AUC: 0.9346,AP: 0.9640\n",
      "Epoch: 051, Loss: 0.1681,AUC: 0.9416,AP: 0.9677\n",
      "Epoch: 052, Loss: 0.1672,AUC: 0.9382,AP: 0.9658\n",
      "Epoch: 053, Loss: 0.1672,AUC: 0.9424,AP: 0.9680\n",
      "Epoch: 054, Loss: 0.1677,AUC: 0.9413,AP: 0.9673\n",
      "Epoch: 055, Loss: 0.1676,AUC: 0.9382,AP: 0.9658\n",
      "Epoch: 056, Loss: 0.1679,AUC: 0.9378,AP: 0.9657\n",
      "Epoch: 057, Loss: 0.1672,AUC: 0.9384,AP: 0.9660\n",
      "Epoch: 058, Loss: 0.1671,AUC: 0.9387,AP: 0.9662\n",
      "Epoch: 059, Loss: 0.1667,AUC: 0.9398,AP: 0.9671\n",
      "Epoch: 060, Loss: 0.1664,AUC: 0.9408,AP: 0.9674\n",
      "Epoch: 061, Loss: 0.1662,AUC: 0.9398,AP: 0.9669\n",
      "Epoch: 062, Loss: 0.1673,AUC: 0.9396,AP: 0.9670\n",
      "Epoch: 063, Loss: 0.1660,AUC: 0.9414,AP: 0.9679\n",
      "Epoch: 064, Loss: 0.1670,AUC: 0.9374,AP: 0.9659\n",
      "Epoch: 065, Loss: 0.1670,AUC: 0.9371,AP: 0.9660\n",
      "Epoch: 066, Loss: 0.1661,AUC: 0.9378,AP: 0.9663\n",
      "Epoch: 067, Loss: 0.1661,AUC: 0.9381,AP: 0.9664\n",
      "Epoch: 068, Loss: 0.1663,AUC: 0.9366,AP: 0.9655\n",
      "Epoch: 069, Loss: 0.1664,AUC: 0.9388,AP: 0.9669\n",
      "Epoch: 070, Loss: 0.1663,AUC: 0.9386,AP: 0.9668\n",
      "Epoch: 071, Loss: 0.1655,AUC: 0.9361,AP: 0.9650\n",
      "Epoch: 072, Loss: 0.1665,AUC: 0.9385,AP: 0.9667\n",
      "Epoch: 073, Loss: 0.1673,AUC: 0.9365,AP: 0.9657\n",
      "Epoch: 074, Loss: 0.1662,AUC: 0.9366,AP: 0.9656\n",
      "Epoch: 075, Loss: 0.1659,AUC: 0.9339,AP: 0.9640\n",
      "Epoch: 076, Loss: 0.1657,AUC: 0.9359,AP: 0.9649\n",
      "Epoch: 077, Loss: 0.1666,AUC: 0.9354,AP: 0.9649\n",
      "Epoch: 078, Loss: 0.1652,AUC: 0.9355,AP: 0.9649\n",
      "Epoch: 079, Loss: 0.1662,AUC: 0.9352,AP: 0.9646\n",
      "Epoch: 080, Loss: 0.1652,AUC: 0.9349,AP: 0.9643\n",
      "Epoch: 081, Loss: 0.1671,AUC: 0.9321,AP: 0.9631\n",
      "Epoch: 082, Loss: 0.1654,AUC: 0.9315,AP: 0.9629\n",
      "Epoch: 083, Loss: 0.1661,AUC: 0.9358,AP: 0.9648\n",
      "Epoch: 084, Loss: 0.1642,AUC: 0.9334,AP: 0.9634\n",
      "Epoch: 085, Loss: 0.1647,AUC: 0.9354,AP: 0.9642\n",
      "Epoch: 086, Loss: 0.1653,AUC: 0.9357,AP: 0.9645\n",
      "Epoch: 087, Loss: 0.1649,AUC: 0.9334,AP: 0.9633\n",
      "Epoch: 088, Loss: 0.1651,AUC: 0.9303,AP: 0.9620\n",
      "Epoch: 089, Loss: 0.1651,AUC: 0.9286,AP: 0.9612\n",
      "Epoch: 090, Loss: 0.1655,AUC: 0.9324,AP: 0.9631\n",
      "Epoch: 091, Loss: 0.1655,AUC: 0.9344,AP: 0.9641\n",
      "Epoch: 092, Loss: 0.1655,AUC: 0.9279,AP: 0.9610\n",
      "Epoch: 093, Loss: 0.1658,AUC: 0.9299,AP: 0.9621\n",
      "Epoch: 094, Loss: 0.1655,AUC: 0.9323,AP: 0.9632\n",
      "Epoch: 095, Loss: 0.1659,AUC: 0.9352,AP: 0.9648\n",
      "Epoch: 096, Loss: 0.1649,AUC: 0.9378,AP: 0.9663\n",
      "Epoch: 097, Loss: 0.1647,AUC: 0.9338,AP: 0.9643\n",
      "Epoch: 098, Loss: 0.1663,AUC: 0.9292,AP: 0.9619\n",
      "Epoch: 099, Loss: 0.1656,AUC: 0.9300,AP: 0.9623\n",
      "Epoch: 100, Loss: 0.1649,AUC: 0.9303,AP: 0.9622\n",
      "Epoch: 101, Loss: 0.1647,AUC: 0.9309,AP: 0.9628\n",
      "Epoch: 102, Loss: 0.1651,AUC: 0.9285,AP: 0.9616\n",
      "Epoch: 103, Loss: 0.1643,AUC: 0.9323,AP: 0.9635\n",
      "Epoch: 104, Loss: 0.1655,AUC: 0.9379,AP: 0.9663\n",
      "Epoch: 105, Loss: 0.1647,AUC: 0.9355,AP: 0.9652\n",
      "Epoch: 106, Loss: 0.1646,AUC: 0.9362,AP: 0.9658\n",
      "Epoch: 107, Loss: 0.1650,AUC: 0.9341,AP: 0.9644\n",
      "Epoch: 108, Loss: 0.1650,AUC: 0.9369,AP: 0.9666\n",
      "Epoch: 109, Loss: 0.1650,AUC: 0.9401,AP: 0.9681\n",
      "Epoch: 110, Loss: 0.1658,AUC: 0.9359,AP: 0.9658\n",
      "Epoch: 111, Loss: 0.1655,AUC: 0.9368,AP: 0.9659\n",
      "Epoch: 112, Loss: 0.1652,AUC: 0.9353,AP: 0.9652\n",
      "Epoch: 113, Loss: 0.1651,AUC: 0.9360,AP: 0.9655\n",
      "Epoch: 114, Loss: 0.1652,AUC: 0.9371,AP: 0.9662\n",
      "Epoch: 115, Loss: 0.1648,AUC: 0.9355,AP: 0.9654\n",
      "Epoch: 116, Loss: 0.1643,AUC: 0.9336,AP: 0.9645\n",
      "Epoch: 117, Loss: 0.1641,AUC: 0.9357,AP: 0.9658\n",
      "Epoch: 118, Loss: 0.1642,AUC: 0.9370,AP: 0.9666\n",
      "Epoch: 119, Loss: 0.1654,AUC: 0.9365,AP: 0.9664\n",
      "Epoch: 120, Loss: 0.1649,AUC: 0.9368,AP: 0.9664\n",
      "Epoch: 121, Loss: 0.1649,AUC: 0.9356,AP: 0.9659\n",
      "Epoch: 122, Loss: 0.1651,AUC: 0.9356,AP: 0.9657\n",
      "Epoch: 123, Loss: 0.1641,AUC: 0.9342,AP: 0.9649\n",
      "Epoch: 124, Loss: 0.1650,AUC: 0.9392,AP: 0.9676\n",
      "Epoch: 125, Loss: 0.1641,AUC: 0.9393,AP: 0.9674\n",
      "Epoch: 126, Loss: 0.1646,AUC: 0.9369,AP: 0.9659\n",
      "Epoch: 127, Loss: 0.1650,AUC: 0.9396,AP: 0.9676\n",
      "Epoch: 128, Loss: 0.1645,AUC: 0.9358,AP: 0.9657\n",
      "Epoch: 129, Loss: 0.1648,AUC: 0.9362,AP: 0.9660\n",
      "Epoch: 130, Loss: 0.1655,AUC: 0.9367,AP: 0.9664\n",
      "Epoch: 131, Loss: 0.1656,AUC: 0.9358,AP: 0.9658\n",
      "Epoch: 132, Loss: 0.1646,AUC: 0.9365,AP: 0.9661\n",
      "Epoch: 133, Loss: 0.1644,AUC: 0.9369,AP: 0.9663\n",
      "Epoch: 134, Loss: 0.1648,AUC: 0.9338,AP: 0.9648\n",
      "Epoch: 135, Loss: 0.1648,AUC: 0.9364,AP: 0.9661\n",
      "Epoch: 136, Loss: 0.1647,AUC: 0.9383,AP: 0.9673\n",
      "Epoch: 137, Loss: 0.1643,AUC: 0.9383,AP: 0.9675\n",
      "Epoch: 138, Loss: 0.1650,AUC: 0.9381,AP: 0.9674\n",
      "Epoch: 139, Loss: 0.1646,AUC: 0.9351,AP: 0.9655\n",
      "Epoch: 140, Loss: 0.1647,AUC: 0.9331,AP: 0.9644\n",
      "Epoch: 141, Loss: 0.1648,AUC: 0.9391,AP: 0.9672\n",
      "Epoch: 142, Loss: 0.1645,AUC: 0.9391,AP: 0.9673\n",
      "Epoch: 143, Loss: 0.1647,AUC: 0.9403,AP: 0.9678\n",
      "Epoch: 144, Loss: 0.1640,AUC: 0.9379,AP: 0.9666\n",
      "Epoch: 145, Loss: 0.1645,AUC: 0.9398,AP: 0.9676\n",
      "Epoch: 146, Loss: 0.1647,AUC: 0.9343,AP: 0.9645\n",
      "Epoch: 147, Loss: 0.1640,AUC: 0.9364,AP: 0.9657\n",
      "Epoch: 148, Loss: 0.1642,AUC: 0.9316,AP: 0.9634\n",
      "Epoch: 149, Loss: 0.1649,AUC: 0.9323,AP: 0.9639\n",
      "Epoch: 150, Loss: 0.1635,AUC: 0.9340,AP: 0.9647\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/f2.pt')\n",
    "data['edge_index'] = torch.tensor(data['edge_index'])\n",
    "data['edge_attr'] = torch.tensor(data['edge_attr'])\n",
    "data['t_start'] = torch.tensor(data['t_start'])\n",
    "data['num_nodes'] = 27045\n",
    "data['x'] = torch.randn((27045, 100))\n",
    "data = data.to(device, 'x', 'edge_index', 'edge_attr')\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=False)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "assert train_data.edge_index.max() < train_data.num_nodes  # !!!!!!!!\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    in_channels=train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([27045, 128])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = model(train_data.x, train_data.edge_index)\n",
    "z2.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.4099,AUC: 0.9108,AP: 0.9456\n",
      "Epoch: 002, Loss: 0.2508,AUC: 0.9194,AP: 0.9495\n",
      "Epoch: 003, Loss: 0.2436,AUC: 0.9197,AP: 0.9495\n",
      "Epoch: 004, Loss: 0.2402,AUC: 0.9256,AP: 0.9528\n",
      "Epoch: 005, Loss: 0.2371,AUC: 0.9253,AP: 0.9530\n",
      "Epoch: 006, Loss: 0.2350,AUC: 0.9283,AP: 0.9548\n",
      "Epoch: 007, Loss: 0.2322,AUC: 0.9324,AP: 0.9572\n",
      "Epoch: 008, Loss: 0.2318,AUC: 0.9293,AP: 0.9560\n",
      "Epoch: 009, Loss: 0.2321,AUC: 0.9316,AP: 0.9573\n",
      "Epoch: 010, Loss: 0.2305,AUC: 0.9320,AP: 0.9578\n",
      "Epoch: 011, Loss: 0.2280,AUC: 0.9326,AP: 0.9586\n",
      "Epoch: 012, Loss: 0.2277,AUC: 0.9286,AP: 0.9566\n",
      "Epoch: 013, Loss: 0.2280,AUC: 0.9343,AP: 0.9598\n",
      "Epoch: 014, Loss: 0.2253,AUC: 0.9355,AP: 0.9607\n",
      "Epoch: 015, Loss: 0.2256,AUC: 0.9340,AP: 0.9603\n",
      "Epoch: 016, Loss: 0.2257,AUC: 0.9353,AP: 0.9611\n",
      "Epoch: 017, Loss: 0.2251,AUC: 0.9391,AP: 0.9632\n",
      "Epoch: 018, Loss: 0.2239,AUC: 0.9363,AP: 0.9619\n",
      "Epoch: 019, Loss: 0.2253,AUC: 0.9369,AP: 0.9625\n",
      "Epoch: 020, Loss: 0.2242,AUC: 0.9329,AP: 0.9605\n",
      "Epoch: 021, Loss: 0.2240,AUC: 0.9342,AP: 0.9611\n",
      "Epoch: 022, Loss: 0.2231,AUC: 0.9360,AP: 0.9622\n",
      "Epoch: 023, Loss: 0.2232,AUC: 0.9390,AP: 0.9640\n",
      "Epoch: 024, Loss: 0.2228,AUC: 0.9365,AP: 0.9628\n",
      "Epoch: 025, Loss: 0.2216,AUC: 0.9371,AP: 0.9633\n",
      "Epoch: 026, Loss: 0.2207,AUC: 0.9356,AP: 0.9626\n",
      "Epoch: 027, Loss: 0.2216,AUC: 0.9360,AP: 0.9629\n",
      "Epoch: 028, Loss: 0.2222,AUC: 0.9341,AP: 0.9618\n",
      "Epoch: 029, Loss: 0.2213,AUC: 0.9344,AP: 0.9623\n",
      "Epoch: 030, Loss: 0.2208,AUC: 0.9375,AP: 0.9643\n",
      "Epoch: 031, Loss: 0.2197,AUC: 0.9364,AP: 0.9640\n",
      "Epoch: 032, Loss: 0.2193,AUC: 0.9375,AP: 0.9647\n",
      "Epoch: 033, Loss: 0.2200,AUC: 0.9387,AP: 0.9652\n",
      "Epoch: 034, Loss: 0.2189,AUC: 0.9368,AP: 0.9644\n",
      "Epoch: 035, Loss: 0.2193,AUC: 0.9365,AP: 0.9644\n",
      "Epoch: 036, Loss: 0.2186,AUC: 0.9356,AP: 0.9639\n",
      "Epoch: 037, Loss: 0.2177,AUC: 0.9351,AP: 0.9634\n",
      "Epoch: 038, Loss: 0.2177,AUC: 0.9393,AP: 0.9655\n",
      "Epoch: 039, Loss: 0.2177,AUC: 0.9366,AP: 0.9644\n",
      "Epoch: 040, Loss: 0.2185,AUC: 0.9356,AP: 0.9637\n",
      "Epoch: 041, Loss: 0.2181,AUC: 0.9365,AP: 0.9645\n",
      "Epoch: 042, Loss: 0.2176,AUC: 0.9343,AP: 0.9636\n",
      "Epoch: 043, Loss: 0.2189,AUC: 0.9373,AP: 0.9651\n",
      "Epoch: 044, Loss: 0.2173,AUC: 0.9343,AP: 0.9635\n",
      "Epoch: 045, Loss: 0.2175,AUC: 0.9364,AP: 0.9645\n",
      "Epoch: 046, Loss: 0.2171,AUC: 0.9336,AP: 0.9632\n",
      "Epoch: 047, Loss: 0.2168,AUC: 0.9321,AP: 0.9625\n",
      "Epoch: 048, Loss: 0.2164,AUC: 0.9371,AP: 0.9647\n",
      "Epoch: 049, Loss: 0.2171,AUC: 0.9392,AP: 0.9658\n",
      "Epoch: 050, Loss: 0.2158,AUC: 0.9388,AP: 0.9656\n",
      "Epoch: 051, Loss: 0.2159,AUC: 0.9350,AP: 0.9636\n",
      "Epoch: 052, Loss: 0.2176,AUC: 0.9354,AP: 0.9639\n",
      "Epoch: 053, Loss: 0.2165,AUC: 0.9362,AP: 0.9646\n",
      "Epoch: 054, Loss: 0.2167,AUC: 0.9356,AP: 0.9645\n",
      "Epoch: 055, Loss: 0.2158,AUC: 0.9391,AP: 0.9662\n",
      "Epoch: 056, Loss: 0.2151,AUC: 0.9409,AP: 0.9672\n",
      "Epoch: 057, Loss: 0.2167,AUC: 0.9410,AP: 0.9673\n",
      "Epoch: 058, Loss: 0.2153,AUC: 0.9386,AP: 0.9660\n",
      "Epoch: 059, Loss: 0.2151,AUC: 0.9424,AP: 0.9682\n",
      "Epoch: 060, Loss: 0.2155,AUC: 0.9375,AP: 0.9655\n",
      "Epoch: 061, Loss: 0.2171,AUC: 0.9396,AP: 0.9664\n",
      "Epoch: 062, Loss: 0.2161,AUC: 0.9432,AP: 0.9684\n",
      "Epoch: 063, Loss: 0.2164,AUC: 0.9351,AP: 0.9643\n",
      "Epoch: 064, Loss: 0.2152,AUC: 0.9381,AP: 0.9660\n",
      "Epoch: 065, Loss: 0.2157,AUC: 0.9382,AP: 0.9663\n",
      "Epoch: 066, Loss: 0.2152,AUC: 0.9351,AP: 0.9643\n",
      "Epoch: 067, Loss: 0.2142,AUC: 0.9366,AP: 0.9652\n",
      "Epoch: 068, Loss: 0.2149,AUC: 0.9363,AP: 0.9651\n",
      "Epoch: 069, Loss: 0.2155,AUC: 0.9390,AP: 0.9665\n",
      "Epoch: 070, Loss: 0.2154,AUC: 0.9387,AP: 0.9663\n",
      "Epoch: 071, Loss: 0.2145,AUC: 0.9412,AP: 0.9677\n",
      "Epoch: 072, Loss: 0.2147,AUC: 0.9421,AP: 0.9680\n",
      "Epoch: 073, Loss: 0.2145,AUC: 0.9375,AP: 0.9654\n",
      "Epoch: 074, Loss: 0.2147,AUC: 0.9396,AP: 0.9670\n",
      "Epoch: 075, Loss: 0.2147,AUC: 0.9386,AP: 0.9664\n",
      "Epoch: 076, Loss: 0.2147,AUC: 0.9355,AP: 0.9650\n",
      "Epoch: 077, Loss: 0.2138,AUC: 0.9388,AP: 0.9664\n",
      "Epoch: 078, Loss: 0.2141,AUC: 0.9384,AP: 0.9661\n",
      "Epoch: 079, Loss: 0.2143,AUC: 0.9376,AP: 0.9660\n",
      "Epoch: 080, Loss: 0.2142,AUC: 0.9363,AP: 0.9651\n",
      "Epoch: 081, Loss: 0.2142,AUC: 0.9366,AP: 0.9654\n",
      "Epoch: 082, Loss: 0.2140,AUC: 0.9316,AP: 0.9627\n",
      "Epoch: 083, Loss: 0.2145,AUC: 0.9384,AP: 0.9664\n",
      "Epoch: 084, Loss: 0.2147,AUC: 0.9379,AP: 0.9658\n",
      "Epoch: 085, Loss: 0.2134,AUC: 0.9366,AP: 0.9652\n",
      "Epoch: 086, Loss: 0.2127,AUC: 0.9390,AP: 0.9669\n",
      "Epoch: 087, Loss: 0.2131,AUC: 0.9356,AP: 0.9654\n",
      "Epoch: 088, Loss: 0.2149,AUC: 0.9369,AP: 0.9662\n",
      "Epoch: 089, Loss: 0.2138,AUC: 0.9348,AP: 0.9647\n",
      "Epoch: 090, Loss: 0.2143,AUC: 0.9373,AP: 0.9663\n",
      "Epoch: 091, Loss: 0.2137,AUC: 0.9344,AP: 0.9649\n",
      "Epoch: 092, Loss: 0.2139,AUC: 0.9346,AP: 0.9650\n",
      "Epoch: 093, Loss: 0.2148,AUC: 0.9313,AP: 0.9633\n",
      "Epoch: 094, Loss: 0.2147,AUC: 0.9328,AP: 0.9640\n",
      "Epoch: 095, Loss: 0.2130,AUC: 0.9324,AP: 0.9636\n",
      "Epoch: 096, Loss: 0.2141,AUC: 0.9338,AP: 0.9643\n",
      "Epoch: 097, Loss: 0.2145,AUC: 0.9326,AP: 0.9639\n",
      "Epoch: 098, Loss: 0.2148,AUC: 0.9338,AP: 0.9648\n",
      "Epoch: 099, Loss: 0.2136,AUC: 0.9350,AP: 0.9656\n",
      "Epoch: 100, Loss: 0.2129,AUC: 0.9345,AP: 0.9652\n",
      "Epoch: 101, Loss: 0.2133,AUC: 0.9347,AP: 0.9653\n",
      "Epoch: 102, Loss: 0.2128,AUC: 0.9364,AP: 0.9662\n",
      "Epoch: 103, Loss: 0.2134,AUC: 0.9361,AP: 0.9660\n",
      "Epoch: 104, Loss: 0.2128,AUC: 0.9390,AP: 0.9674\n",
      "Epoch: 105, Loss: 0.2138,AUC: 0.9381,AP: 0.9665\n",
      "Epoch: 106, Loss: 0.2130,AUC: 0.9342,AP: 0.9643\n",
      "Epoch: 107, Loss: 0.2138,AUC: 0.9330,AP: 0.9641\n",
      "Epoch: 108, Loss: 0.2129,AUC: 0.9351,AP: 0.9651\n",
      "Epoch: 109, Loss: 0.2138,AUC: 0.9364,AP: 0.9657\n",
      "Epoch: 110, Loss: 0.2131,AUC: 0.9312,AP: 0.9633\n",
      "Epoch: 111, Loss: 0.2133,AUC: 0.9333,AP: 0.9641\n",
      "Epoch: 112, Loss: 0.2133,AUC: 0.9361,AP: 0.9655\n",
      "Epoch: 113, Loss: 0.2130,AUC: 0.9324,AP: 0.9634\n",
      "Epoch: 114, Loss: 0.2130,AUC: 0.9390,AP: 0.9671\n",
      "Epoch: 115, Loss: 0.2117,AUC: 0.9393,AP: 0.9675\n",
      "Epoch: 116, Loss: 0.2132,AUC: 0.9365,AP: 0.9659\n",
      "Epoch: 117, Loss: 0.2126,AUC: 0.9374,AP: 0.9663\n",
      "Epoch: 118, Loss: 0.2122,AUC: 0.9346,AP: 0.9646\n",
      "Epoch: 119, Loss: 0.2142,AUC: 0.9304,AP: 0.9625\n",
      "Epoch: 120, Loss: 0.2134,AUC: 0.9372,AP: 0.9661\n",
      "Epoch: 121, Loss: 0.2136,AUC: 0.9315,AP: 0.9633\n",
      "Epoch: 122, Loss: 0.2128,AUC: 0.9369,AP: 0.9659\n",
      "Epoch: 123, Loss: 0.2142,AUC: 0.9344,AP: 0.9646\n",
      "Epoch: 124, Loss: 0.2128,AUC: 0.9340,AP: 0.9648\n",
      "Epoch: 125, Loss: 0.2133,AUC: 0.9338,AP: 0.9647\n",
      "Epoch: 126, Loss: 0.2137,AUC: 0.9308,AP: 0.9628\n",
      "Epoch: 127, Loss: 0.2126,AUC: 0.9361,AP: 0.9655\n",
      "Epoch: 128, Loss: 0.2137,AUC: 0.9371,AP: 0.9662\n",
      "Epoch: 129, Loss: 0.2144,AUC: 0.9371,AP: 0.9666\n",
      "Epoch: 130, Loss: 0.2130,AUC: 0.9364,AP: 0.9665\n",
      "Epoch: 131, Loss: 0.2130,AUC: 0.9326,AP: 0.9647\n",
      "Epoch: 132, Loss: 0.2134,AUC: 0.9322,AP: 0.9645\n",
      "Epoch: 133, Loss: 0.2132,AUC: 0.9348,AP: 0.9655\n",
      "Epoch: 134, Loss: 0.2132,AUC: 0.9341,AP: 0.9651\n",
      "Epoch: 135, Loss: 0.2125,AUC: 0.9352,AP: 0.9656\n",
      "Epoch: 136, Loss: 0.2125,AUC: 0.9352,AP: 0.9658\n",
      "Epoch: 137, Loss: 0.2125,AUC: 0.9359,AP: 0.9661\n",
      "Epoch: 138, Loss: 0.2114,AUC: 0.9327,AP: 0.9643\n",
      "Epoch: 139, Loss: 0.2129,AUC: 0.9328,AP: 0.9643\n",
      "Epoch: 140, Loss: 0.2133,AUC: 0.9319,AP: 0.9637\n",
      "Epoch: 141, Loss: 0.2129,AUC: 0.9342,AP: 0.9650\n",
      "Epoch: 142, Loss: 0.2120,AUC: 0.9376,AP: 0.9671\n",
      "Epoch: 143, Loss: 0.2134,AUC: 0.9349,AP: 0.9655\n",
      "Epoch: 144, Loss: 0.2124,AUC: 0.9335,AP: 0.9649\n",
      "Epoch: 145, Loss: 0.2127,AUC: 0.9346,AP: 0.9654\n",
      "Epoch: 146, Loss: 0.2133,AUC: 0.9350,AP: 0.9657\n",
      "Epoch: 147, Loss: 0.2129,AUC: 0.9374,AP: 0.9666\n",
      "Epoch: 148, Loss: 0.2124,AUC: 0.9347,AP: 0.9652\n",
      "Epoch: 149, Loss: 0.2138,AUC: 0.9370,AP: 0.9664\n",
      "Epoch: 150, Loss: 0.2135,AUC: 0.9391,AP: 0.9678\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/f3.pt')\n",
    "data['edge_index'] = torch.tensor(data['edge_index'])\n",
    "data['edge_attr'] = torch.tensor(data['edge_attr'])\n",
    "data['t_start'] = torch.tensor(data['t_start'])\n",
    "data['num_nodes'] = 27045\n",
    "data['x'] = torch.randn((27045, 100))\n",
    "data = data.to(device, 'x', 'edge_index', 'edge_attr')\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=False)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "assert train_data.edge_index.max() < train_data.num_nodes  # !!!!!!!!\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    in_channels=train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([27045, 128])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3 = model(train_data.x, train_data.edge_index)\n",
    "z3.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.3318,AUC: 0.9002,AP: 0.9377\n",
      "Epoch: 002, Loss: 0.2130,AUC: 0.9049,AP: 0.9398\n",
      "Epoch: 003, Loss: 0.2066,AUC: 0.9087,AP: 0.9417\n",
      "Epoch: 004, Loss: 0.2010,AUC: 0.9131,AP: 0.9441\n",
      "Epoch: 005, Loss: 0.2009,AUC: 0.9089,AP: 0.9424\n",
      "Epoch: 006, Loss: 0.2011,AUC: 0.9079,AP: 0.9421\n",
      "Epoch: 007, Loss: 0.1993,AUC: 0.9100,AP: 0.9434\n",
      "Epoch: 008, Loss: 0.1983,AUC: 0.9124,AP: 0.9451\n",
      "Epoch: 009, Loss: 0.1960,AUC: 0.9145,AP: 0.9467\n",
      "Epoch: 010, Loss: 0.1952,AUC: 0.9105,AP: 0.9452\n",
      "Epoch: 011, Loss: 0.1957,AUC: 0.9098,AP: 0.9452\n",
      "Epoch: 012, Loss: 0.1932,AUC: 0.9078,AP: 0.9444\n",
      "Epoch: 013, Loss: 0.1930,AUC: 0.9116,AP: 0.9464\n",
      "Epoch: 014, Loss: 0.1915,AUC: 0.9141,AP: 0.9478\n",
      "Epoch: 015, Loss: 0.1917,AUC: 0.9107,AP: 0.9466\n",
      "Epoch: 016, Loss: 0.1909,AUC: 0.9149,AP: 0.9487\n",
      "Epoch: 017, Loss: 0.1913,AUC: 0.9179,AP: 0.9504\n",
      "Epoch: 018, Loss: 0.1903,AUC: 0.9141,AP: 0.9488\n",
      "Epoch: 019, Loss: 0.1903,AUC: 0.9128,AP: 0.9484\n",
      "Epoch: 020, Loss: 0.1902,AUC: 0.9201,AP: 0.9521\n",
      "Epoch: 021, Loss: 0.1888,AUC: 0.9154,AP: 0.9499\n",
      "Epoch: 022, Loss: 0.1893,AUC: 0.9154,AP: 0.9501\n",
      "Epoch: 023, Loss: 0.1877,AUC: 0.9200,AP: 0.9527\n",
      "Epoch: 024, Loss: 0.1883,AUC: 0.9227,AP: 0.9544\n",
      "Epoch: 025, Loss: 0.1884,AUC: 0.9210,AP: 0.9538\n",
      "Epoch: 026, Loss: 0.1879,AUC: 0.9202,AP: 0.9538\n",
      "Epoch: 027, Loss: 0.1861,AUC: 0.9188,AP: 0.9532\n",
      "Epoch: 028, Loss: 0.1867,AUC: 0.9202,AP: 0.9540\n",
      "Epoch: 029, Loss: 0.1869,AUC: 0.9202,AP: 0.9541\n",
      "Epoch: 030, Loss: 0.1854,AUC: 0.9202,AP: 0.9539\n",
      "Epoch: 031, Loss: 0.1860,AUC: 0.9166,AP: 0.9524\n",
      "Epoch: 032, Loss: 0.1866,AUC: 0.9176,AP: 0.9532\n",
      "Epoch: 033, Loss: 0.1853,AUC: 0.9196,AP: 0.9543\n",
      "Epoch: 034, Loss: 0.1854,AUC: 0.9216,AP: 0.9551\n",
      "Epoch: 035, Loss: 0.1848,AUC: 0.9203,AP: 0.9543\n",
      "Epoch: 036, Loss: 0.1845,AUC: 0.9210,AP: 0.9549\n",
      "Epoch: 037, Loss: 0.1841,AUC: 0.9211,AP: 0.9553\n",
      "Epoch: 038, Loss: 0.1847,AUC: 0.9250,AP: 0.9572\n",
      "Epoch: 039, Loss: 0.1831,AUC: 0.9233,AP: 0.9565\n",
      "Epoch: 040, Loss: 0.1831,AUC: 0.9250,AP: 0.9573\n",
      "Epoch: 041, Loss: 0.1844,AUC: 0.9240,AP: 0.9568\n",
      "Epoch: 042, Loss: 0.1837,AUC: 0.9271,AP: 0.9584\n",
      "Epoch: 043, Loss: 0.1828,AUC: 0.9245,AP: 0.9569\n",
      "Epoch: 044, Loss: 0.1836,AUC: 0.9248,AP: 0.9572\n",
      "Epoch: 045, Loss: 0.1820,AUC: 0.9249,AP: 0.9573\n",
      "Epoch: 046, Loss: 0.1827,AUC: 0.9225,AP: 0.9561\n",
      "Epoch: 047, Loss: 0.1829,AUC: 0.9242,AP: 0.9571\n",
      "Epoch: 048, Loss: 0.1851,AUC: 0.9223,AP: 0.9559\n",
      "Epoch: 049, Loss: 0.1828,AUC: 0.9230,AP: 0.9564\n",
      "Epoch: 050, Loss: 0.1831,AUC: 0.9169,AP: 0.9533\n",
      "Epoch: 051, Loss: 0.1827,AUC: 0.9216,AP: 0.9558\n",
      "Epoch: 052, Loss: 0.1833,AUC: 0.9202,AP: 0.9554\n",
      "Epoch: 053, Loss: 0.1829,AUC: 0.9176,AP: 0.9541\n",
      "Epoch: 054, Loss: 0.1825,AUC: 0.9205,AP: 0.9558\n",
      "Epoch: 055, Loss: 0.1827,AUC: 0.9242,AP: 0.9578\n",
      "Epoch: 056, Loss: 0.1813,AUC: 0.9222,AP: 0.9567\n",
      "Epoch: 057, Loss: 0.1833,AUC: 0.9211,AP: 0.9561\n",
      "Epoch: 058, Loss: 0.1820,AUC: 0.9200,AP: 0.9553\n",
      "Epoch: 059, Loss: 0.1811,AUC: 0.9215,AP: 0.9563\n",
      "Epoch: 060, Loss: 0.1811,AUC: 0.9214,AP: 0.9562\n",
      "Epoch: 061, Loss: 0.1824,AUC: 0.9174,AP: 0.9542\n",
      "Epoch: 062, Loss: 0.1814,AUC: 0.9235,AP: 0.9578\n",
      "Epoch: 063, Loss: 0.1821,AUC: 0.9232,AP: 0.9577\n",
      "Epoch: 064, Loss: 0.1809,AUC: 0.9233,AP: 0.9579\n",
      "Epoch: 065, Loss: 0.1821,AUC: 0.9224,AP: 0.9576\n",
      "Epoch: 066, Loss: 0.1809,AUC: 0.9227,AP: 0.9581\n",
      "Epoch: 067, Loss: 0.1810,AUC: 0.9206,AP: 0.9570\n",
      "Epoch: 068, Loss: 0.1814,AUC: 0.9213,AP: 0.9568\n",
      "Epoch: 069, Loss: 0.1805,AUC: 0.9194,AP: 0.9558\n",
      "Epoch: 070, Loss: 0.1809,AUC: 0.9233,AP: 0.9576\n",
      "Epoch: 071, Loss: 0.1800,AUC: 0.9211,AP: 0.9564\n",
      "Epoch: 072, Loss: 0.1802,AUC: 0.9208,AP: 0.9565\n",
      "Epoch: 073, Loss: 0.1810,AUC: 0.9218,AP: 0.9569\n",
      "Epoch: 074, Loss: 0.1809,AUC: 0.9239,AP: 0.9581\n",
      "Epoch: 075, Loss: 0.1815,AUC: 0.9156,AP: 0.9534\n",
      "Epoch: 076, Loss: 0.1811,AUC: 0.9222,AP: 0.9568\n",
      "Epoch: 077, Loss: 0.1816,AUC: 0.9240,AP: 0.9577\n",
      "Epoch: 078, Loss: 0.1796,AUC: 0.9245,AP: 0.9584\n",
      "Epoch: 079, Loss: 0.1810,AUC: 0.9236,AP: 0.9579\n",
      "Epoch: 080, Loss: 0.1806,AUC: 0.9210,AP: 0.9564\n",
      "Epoch: 081, Loss: 0.1807,AUC: 0.9192,AP: 0.9555\n",
      "Epoch: 082, Loss: 0.1805,AUC: 0.9188,AP: 0.9552\n",
      "Epoch: 083, Loss: 0.1808,AUC: 0.9243,AP: 0.9583\n",
      "Epoch: 084, Loss: 0.1802,AUC: 0.9234,AP: 0.9583\n",
      "Epoch: 085, Loss: 0.1800,AUC: 0.9196,AP: 0.9559\n",
      "Epoch: 086, Loss: 0.1816,AUC: 0.9202,AP: 0.9563\n",
      "Epoch: 087, Loss: 0.1814,AUC: 0.9234,AP: 0.9578\n",
      "Epoch: 088, Loss: 0.1803,AUC: 0.9184,AP: 0.9550\n",
      "Epoch: 089, Loss: 0.1802,AUC: 0.9184,AP: 0.9556\n",
      "Epoch: 090, Loss: 0.1815,AUC: 0.9203,AP: 0.9569\n",
      "Epoch: 091, Loss: 0.1808,AUC: 0.9209,AP: 0.9572\n",
      "Epoch: 092, Loss: 0.1801,AUC: 0.9211,AP: 0.9570\n",
      "Epoch: 093, Loss: 0.1797,AUC: 0.9199,AP: 0.9565\n",
      "Epoch: 094, Loss: 0.1810,AUC: 0.9250,AP: 0.9590\n",
      "Epoch: 095, Loss: 0.1800,AUC: 0.9195,AP: 0.9561\n",
      "Epoch: 096, Loss: 0.1792,AUC: 0.9212,AP: 0.9569\n",
      "Epoch: 097, Loss: 0.1800,AUC: 0.9201,AP: 0.9564\n",
      "Epoch: 098, Loss: 0.1797,AUC: 0.9229,AP: 0.9581\n",
      "Epoch: 099, Loss: 0.1803,AUC: 0.9196,AP: 0.9565\n",
      "Epoch: 100, Loss: 0.1800,AUC: 0.9191,AP: 0.9561\n",
      "Epoch: 101, Loss: 0.1805,AUC: 0.9199,AP: 0.9564\n",
      "Epoch: 102, Loss: 0.1800,AUC: 0.9240,AP: 0.9580\n",
      "Epoch: 103, Loss: 0.1805,AUC: 0.9179,AP: 0.9550\n",
      "Epoch: 104, Loss: 0.1794,AUC: 0.9176,AP: 0.9549\n",
      "Epoch: 105, Loss: 0.1800,AUC: 0.9142,AP: 0.9534\n",
      "Epoch: 106, Loss: 0.1799,AUC: 0.9175,AP: 0.9550\n",
      "Epoch: 107, Loss: 0.1798,AUC: 0.9213,AP: 0.9570\n",
      "Epoch: 108, Loss: 0.1790,AUC: 0.9171,AP: 0.9554\n",
      "Epoch: 109, Loss: 0.1805,AUC: 0.9117,AP: 0.9529\n",
      "Epoch: 110, Loss: 0.1800,AUC: 0.9156,AP: 0.9547\n",
      "Epoch: 111, Loss: 0.1791,AUC: 0.9165,AP: 0.9547\n",
      "Epoch: 112, Loss: 0.1786,AUC: 0.9185,AP: 0.9561\n",
      "Epoch: 113, Loss: 0.1793,AUC: 0.9242,AP: 0.9595\n",
      "Epoch: 114, Loss: 0.1793,AUC: 0.9199,AP: 0.9571\n",
      "Epoch: 115, Loss: 0.1800,AUC: 0.9152,AP: 0.9548\n",
      "Epoch: 116, Loss: 0.1793,AUC: 0.9157,AP: 0.9549\n",
      "Epoch: 117, Loss: 0.1794,AUC: 0.9143,AP: 0.9542\n",
      "Epoch: 118, Loss: 0.1791,AUC: 0.9179,AP: 0.9561\n",
      "Epoch: 119, Loss: 0.1791,AUC: 0.9165,AP: 0.9556\n",
      "Epoch: 120, Loss: 0.1784,AUC: 0.9117,AP: 0.9529\n",
      "Epoch: 121, Loss: 0.1794,AUC: 0.9147,AP: 0.9544\n",
      "Epoch: 122, Loss: 0.1800,AUC: 0.9134,AP: 0.9538\n",
      "Epoch: 123, Loss: 0.1802,AUC: 0.9151,AP: 0.9546\n",
      "Epoch: 124, Loss: 0.1796,AUC: 0.9168,AP: 0.9554\n",
      "Epoch: 125, Loss: 0.1793,AUC: 0.9220,AP: 0.9581\n",
      "Epoch: 126, Loss: 0.1791,AUC: 0.9201,AP: 0.9573\n",
      "Epoch: 127, Loss: 0.1792,AUC: 0.9170,AP: 0.9553\n",
      "Epoch: 128, Loss: 0.1795,AUC: 0.9150,AP: 0.9542\n",
      "Epoch: 129, Loss: 0.1796,AUC: 0.9139,AP: 0.9535\n",
      "Epoch: 130, Loss: 0.1795,AUC: 0.9178,AP: 0.9555\n",
      "Epoch: 131, Loss: 0.1799,AUC: 0.9208,AP: 0.9572\n",
      "Epoch: 132, Loss: 0.1804,AUC: 0.9179,AP: 0.9558\n",
      "Epoch: 133, Loss: 0.1792,AUC: 0.9176,AP: 0.9557\n",
      "Epoch: 134, Loss: 0.1795,AUC: 0.9124,AP: 0.9530\n",
      "Epoch: 135, Loss: 0.1802,AUC: 0.9090,AP: 0.9510\n",
      "Epoch: 136, Loss: 0.1804,AUC: 0.9119,AP: 0.9526\n",
      "Epoch: 137, Loss: 0.1793,AUC: 0.9139,AP: 0.9532\n",
      "Epoch: 138, Loss: 0.1797,AUC: 0.9191,AP: 0.9561\n",
      "Epoch: 139, Loss: 0.1793,AUC: 0.9156,AP: 0.9543\n",
      "Epoch: 140, Loss: 0.1795,AUC: 0.9110,AP: 0.9523\n",
      "Epoch: 141, Loss: 0.1800,AUC: 0.9119,AP: 0.9527\n",
      "Epoch: 142, Loss: 0.1793,AUC: 0.9117,AP: 0.9526\n",
      "Epoch: 143, Loss: 0.1803,AUC: 0.9073,AP: 0.9501\n",
      "Epoch: 144, Loss: 0.1791,AUC: 0.9135,AP: 0.9536\n",
      "Epoch: 145, Loss: 0.1791,AUC: 0.9150,AP: 0.9545\n",
      "Epoch: 146, Loss: 0.1791,AUC: 0.9136,AP: 0.9537\n",
      "Epoch: 147, Loss: 0.1790,AUC: 0.9153,AP: 0.9541\n",
      "Epoch: 148, Loss: 0.1790,AUC: 0.9124,AP: 0.9523\n",
      "Epoch: 149, Loss: 0.1789,AUC: 0.9231,AP: 0.9584\n",
      "Epoch: 150, Loss: 0.1790,AUC: 0.9242,AP: 0.9590\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/f4.pt')\n",
    "data['edge_index'] = torch.tensor(data['edge_index'])\n",
    "data['edge_attr'] = torch.tensor(data['edge_attr'])\n",
    "data['t_start'] = torch.tensor(data['t_start'])\n",
    "data['num_nodes'] = 27045\n",
    "data['x'] = torch.randn((27045, 100))\n",
    "data = data.to(device, 'x', 'edge_index', 'edge_attr')\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=False)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "assert train_data.edge_index.max() < train_data.num_nodes  # !!!!!!!!\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    in_channels=train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([27045, 128])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z4 = model(train_data.x, train_data.edge_index)\n",
    "z4.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.3782,AUC: 0.9128,AP: 0.9477\n",
      "Epoch: 002, Loss: 0.2197,AUC: 0.9235,AP: 0.9531\n",
      "Epoch: 003, Loss: 0.2096,AUC: 0.9263,AP: 0.9543\n",
      "Epoch: 004, Loss: 0.2078,AUC: 0.9293,AP: 0.9562\n",
      "Epoch: 005, Loss: 0.2048,AUC: 0.9304,AP: 0.9571\n",
      "Epoch: 006, Loss: 0.2013,AUC: 0.9333,AP: 0.9588\n",
      "Epoch: 007, Loss: 0.2016,AUC: 0.9317,AP: 0.9583\n",
      "Epoch: 008, Loss: 0.1991,AUC: 0.9344,AP: 0.9599\n",
      "Epoch: 009, Loss: 0.1996,AUC: 0.9337,AP: 0.9600\n",
      "Epoch: 010, Loss: 0.1974,AUC: 0.9332,AP: 0.9598\n",
      "Epoch: 011, Loss: 0.1961,AUC: 0.9352,AP: 0.9612\n",
      "Epoch: 012, Loss: 0.1977,AUC: 0.9349,AP: 0.9613\n",
      "Epoch: 013, Loss: 0.1957,AUC: 0.9353,AP: 0.9616\n",
      "Epoch: 014, Loss: 0.1968,AUC: 0.9382,AP: 0.9631\n",
      "Epoch: 015, Loss: 0.1947,AUC: 0.9377,AP: 0.9630\n",
      "Epoch: 016, Loss: 0.1943,AUC: 0.9400,AP: 0.9643\n",
      "Epoch: 017, Loss: 0.1947,AUC: 0.9393,AP: 0.9640\n",
      "Epoch: 018, Loss: 0.1922,AUC: 0.9393,AP: 0.9642\n",
      "Epoch: 019, Loss: 0.1932,AUC: 0.9383,AP: 0.9639\n",
      "Epoch: 020, Loss: 0.1923,AUC: 0.9400,AP: 0.9649\n",
      "Epoch: 021, Loss: 0.1915,AUC: 0.9356,AP: 0.9630\n",
      "Epoch: 022, Loss: 0.1922,AUC: 0.9368,AP: 0.9637\n",
      "Epoch: 023, Loss: 0.1916,AUC: 0.9387,AP: 0.9646\n",
      "Epoch: 024, Loss: 0.1927,AUC: 0.9383,AP: 0.9646\n",
      "Epoch: 025, Loss: 0.1908,AUC: 0.9389,AP: 0.9651\n",
      "Epoch: 026, Loss: 0.1908,AUC: 0.9375,AP: 0.9644\n",
      "Epoch: 027, Loss: 0.1903,AUC: 0.9404,AP: 0.9659\n",
      "Epoch: 028, Loss: 0.1912,AUC: 0.9373,AP: 0.9644\n",
      "Epoch: 029, Loss: 0.1900,AUC: 0.9412,AP: 0.9664\n",
      "Epoch: 030, Loss: 0.1889,AUC: 0.9415,AP: 0.9665\n",
      "Epoch: 031, Loss: 0.1902,AUC: 0.9428,AP: 0.9673\n",
      "Epoch: 032, Loss: 0.1893,AUC: 0.9411,AP: 0.9665\n",
      "Epoch: 033, Loss: 0.1885,AUC: 0.9427,AP: 0.9674\n",
      "Epoch: 034, Loss: 0.1890,AUC: 0.9430,AP: 0.9674\n",
      "Epoch: 035, Loss: 0.1894,AUC: 0.9426,AP: 0.9673\n",
      "Epoch: 036, Loss: 0.1875,AUC: 0.9440,AP: 0.9681\n",
      "Epoch: 037, Loss: 0.1874,AUC: 0.9412,AP: 0.9667\n",
      "Epoch: 038, Loss: 0.1882,AUC: 0.9436,AP: 0.9680\n",
      "Epoch: 039, Loss: 0.1886,AUC: 0.9452,AP: 0.9689\n",
      "Epoch: 040, Loss: 0.1882,AUC: 0.9447,AP: 0.9687\n",
      "Epoch: 041, Loss: 0.1877,AUC: 0.9451,AP: 0.9687\n",
      "Epoch: 042, Loss: 0.1875,AUC: 0.9472,AP: 0.9698\n",
      "Epoch: 043, Loss: 0.1880,AUC: 0.9444,AP: 0.9683\n",
      "Epoch: 044, Loss: 0.1877,AUC: 0.9452,AP: 0.9687\n",
      "Epoch: 045, Loss: 0.1865,AUC: 0.9437,AP: 0.9678\n",
      "Epoch: 046, Loss: 0.1873,AUC: 0.9423,AP: 0.9672\n",
      "Epoch: 047, Loss: 0.1869,AUC: 0.9434,AP: 0.9680\n",
      "Epoch: 048, Loss: 0.1863,AUC: 0.9425,AP: 0.9674\n",
      "Epoch: 049, Loss: 0.1870,AUC: 0.9416,AP: 0.9670\n",
      "Epoch: 050, Loss: 0.1862,AUC: 0.9398,AP: 0.9660\n",
      "Epoch: 051, Loss: 0.1867,AUC: 0.9418,AP: 0.9671\n",
      "Epoch: 052, Loss: 0.1863,AUC: 0.9397,AP: 0.9661\n",
      "Epoch: 053, Loss: 0.1860,AUC: 0.9432,AP: 0.9678\n",
      "Epoch: 054, Loss: 0.1870,AUC: 0.9395,AP: 0.9662\n",
      "Epoch: 055, Loss: 0.1868,AUC: 0.9395,AP: 0.9664\n",
      "Epoch: 056, Loss: 0.1864,AUC: 0.9397,AP: 0.9664\n",
      "Epoch: 057, Loss: 0.1867,AUC: 0.9420,AP: 0.9676\n",
      "Epoch: 058, Loss: 0.1853,AUC: 0.9444,AP: 0.9687\n",
      "Epoch: 059, Loss: 0.1873,AUC: 0.9412,AP: 0.9673\n",
      "Epoch: 060, Loss: 0.1856,AUC: 0.9440,AP: 0.9688\n",
      "Epoch: 061, Loss: 0.1851,AUC: 0.9427,AP: 0.9681\n",
      "Epoch: 062, Loss: 0.1859,AUC: 0.9432,AP: 0.9682\n",
      "Epoch: 063, Loss: 0.1854,AUC: 0.9459,AP: 0.9697\n",
      "Epoch: 064, Loss: 0.1853,AUC: 0.9436,AP: 0.9685\n",
      "Epoch: 065, Loss: 0.1856,AUC: 0.9478,AP: 0.9706\n",
      "Epoch: 066, Loss: 0.1856,AUC: 0.9416,AP: 0.9673\n",
      "Epoch: 067, Loss: 0.1856,AUC: 0.9402,AP: 0.9671\n",
      "Epoch: 068, Loss: 0.1841,AUC: 0.9442,AP: 0.9693\n",
      "Epoch: 069, Loss: 0.1858,AUC: 0.9410,AP: 0.9676\n",
      "Epoch: 070, Loss: 0.1856,AUC: 0.9399,AP: 0.9669\n",
      "Epoch: 071, Loss: 0.1860,AUC: 0.9380,AP: 0.9659\n",
      "Epoch: 072, Loss: 0.1846,AUC: 0.9386,AP: 0.9660\n",
      "Epoch: 073, Loss: 0.1858,AUC: 0.9385,AP: 0.9661\n",
      "Epoch: 074, Loss: 0.1839,AUC: 0.9442,AP: 0.9692\n",
      "Epoch: 075, Loss: 0.1854,AUC: 0.9430,AP: 0.9687\n",
      "Epoch: 076, Loss: 0.1850,AUC: 0.9447,AP: 0.9696\n",
      "Epoch: 077, Loss: 0.1850,AUC: 0.9450,AP: 0.9697\n",
      "Epoch: 078, Loss: 0.1840,AUC: 0.9452,AP: 0.9696\n",
      "Epoch: 079, Loss: 0.1852,AUC: 0.9472,AP: 0.9704\n",
      "Epoch: 080, Loss: 0.1847,AUC: 0.9474,AP: 0.9706\n",
      "Epoch: 081, Loss: 0.1840,AUC: 0.9457,AP: 0.9697\n",
      "Epoch: 082, Loss: 0.1859,AUC: 0.9451,AP: 0.9694\n",
      "Epoch: 083, Loss: 0.1841,AUC: 0.9484,AP: 0.9709\n",
      "Epoch: 084, Loss: 0.1842,AUC: 0.9470,AP: 0.9704\n",
      "Epoch: 085, Loss: 0.1849,AUC: 0.9462,AP: 0.9700\n",
      "Epoch: 086, Loss: 0.1848,AUC: 0.9466,AP: 0.9704\n",
      "Epoch: 087, Loss: 0.1839,AUC: 0.9436,AP: 0.9688\n",
      "Epoch: 088, Loss: 0.1844,AUC: 0.9422,AP: 0.9681\n",
      "Epoch: 089, Loss: 0.1841,AUC: 0.9420,AP: 0.9683\n",
      "Epoch: 090, Loss: 0.1834,AUC: 0.9419,AP: 0.9684\n",
      "Epoch: 091, Loss: 0.1839,AUC: 0.9429,AP: 0.9690\n",
      "Epoch: 092, Loss: 0.1845,AUC: 0.9422,AP: 0.9685\n",
      "Epoch: 093, Loss: 0.1840,AUC: 0.9424,AP: 0.9686\n",
      "Epoch: 094, Loss: 0.1838,AUC: 0.9467,AP: 0.9709\n",
      "Epoch: 095, Loss: 0.1840,AUC: 0.9455,AP: 0.9703\n",
      "Epoch: 096, Loss: 0.1835,AUC: 0.9438,AP: 0.9696\n",
      "Epoch: 097, Loss: 0.1834,AUC: 0.9428,AP: 0.9687\n",
      "Epoch: 098, Loss: 0.1839,AUC: 0.9485,AP: 0.9717\n",
      "Epoch: 099, Loss: 0.1831,AUC: 0.9474,AP: 0.9711\n",
      "Epoch: 100, Loss: 0.1847,AUC: 0.9426,AP: 0.9686\n",
      "Epoch: 101, Loss: 0.1831,AUC: 0.9396,AP: 0.9673\n",
      "Epoch: 102, Loss: 0.1838,AUC: 0.9441,AP: 0.9694\n",
      "Epoch: 103, Loss: 0.1832,AUC: 0.9437,AP: 0.9695\n",
      "Epoch: 104, Loss: 0.1841,AUC: 0.9444,AP: 0.9701\n",
      "Epoch: 105, Loss: 0.1846,AUC: 0.9458,AP: 0.9706\n",
      "Epoch: 106, Loss: 0.1828,AUC: 0.9482,AP: 0.9720\n",
      "Epoch: 107, Loss: 0.1836,AUC: 0.9458,AP: 0.9705\n",
      "Epoch: 108, Loss: 0.1832,AUC: 0.9464,AP: 0.9710\n",
      "Epoch: 109, Loss: 0.1836,AUC: 0.9436,AP: 0.9694\n",
      "Epoch: 110, Loss: 0.1840,AUC: 0.9492,AP: 0.9724\n",
      "Epoch: 111, Loss: 0.1834,AUC: 0.9457,AP: 0.9707\n",
      "Epoch: 112, Loss: 0.1840,AUC: 0.9478,AP: 0.9715\n",
      "Epoch: 113, Loss: 0.1830,AUC: 0.9526,AP: 0.9741\n",
      "Epoch: 114, Loss: 0.1839,AUC: 0.9518,AP: 0.9738\n",
      "Epoch: 115, Loss: 0.1845,AUC: 0.9474,AP: 0.9713\n",
      "Epoch: 116, Loss: 0.1828,AUC: 0.9480,AP: 0.9717\n",
      "Epoch: 117, Loss: 0.1834,AUC: 0.9520,AP: 0.9739\n",
      "Epoch: 118, Loss: 0.1832,AUC: 0.9498,AP: 0.9724\n",
      "Epoch: 119, Loss: 0.1821,AUC: 0.9540,AP: 0.9749\n",
      "Epoch: 120, Loss: 0.1830,AUC: 0.9504,AP: 0.9729\n",
      "Epoch: 121, Loss: 0.1837,AUC: 0.9455,AP: 0.9701\n",
      "Epoch: 122, Loss: 0.1830,AUC: 0.9460,AP: 0.9706\n",
      "Epoch: 123, Loss: 0.1834,AUC: 0.9486,AP: 0.9722\n",
      "Epoch: 124, Loss: 0.1841,AUC: 0.9476,AP: 0.9716\n",
      "Epoch: 125, Loss: 0.1840,AUC: 0.9475,AP: 0.9714\n",
      "Epoch: 126, Loss: 0.1843,AUC: 0.9440,AP: 0.9696\n",
      "Epoch: 127, Loss: 0.1826,AUC: 0.9465,AP: 0.9712\n",
      "Epoch: 128, Loss: 0.1834,AUC: 0.9475,AP: 0.9715\n",
      "Epoch: 129, Loss: 0.1828,AUC: 0.9458,AP: 0.9710\n",
      "Epoch: 130, Loss: 0.1839,AUC: 0.9445,AP: 0.9705\n",
      "Epoch: 131, Loss: 0.1839,AUC: 0.9445,AP: 0.9704\n",
      "Epoch: 132, Loss: 0.1834,AUC: 0.9436,AP: 0.9699\n",
      "Epoch: 133, Loss: 0.1828,AUC: 0.9421,AP: 0.9691\n",
      "Epoch: 134, Loss: 0.1831,AUC: 0.9406,AP: 0.9681\n",
      "Epoch: 135, Loss: 0.1829,AUC: 0.9424,AP: 0.9690\n",
      "Epoch: 136, Loss: 0.1835,AUC: 0.9415,AP: 0.9685\n",
      "Epoch: 137, Loss: 0.1828,AUC: 0.9457,AP: 0.9705\n",
      "Epoch: 138, Loss: 0.1828,AUC: 0.9498,AP: 0.9726\n",
      "Epoch: 139, Loss: 0.1830,AUC: 0.9487,AP: 0.9721\n",
      "Epoch: 140, Loss: 0.1829,AUC: 0.9459,AP: 0.9707\n",
      "Epoch: 141, Loss: 0.1828,AUC: 0.9493,AP: 0.9724\n",
      "Epoch: 142, Loss: 0.1833,AUC: 0.9504,AP: 0.9730\n",
      "Epoch: 143, Loss: 0.1835,AUC: 0.9451,AP: 0.9698\n",
      "Epoch: 144, Loss: 0.1823,AUC: 0.9479,AP: 0.9714\n",
      "Epoch: 145, Loss: 0.1821,AUC: 0.9472,AP: 0.9707\n",
      "Epoch: 146, Loss: 0.1832,AUC: 0.9469,AP: 0.9707\n",
      "Epoch: 147, Loss: 0.1840,AUC: 0.9476,AP: 0.9708\n",
      "Epoch: 148, Loss: 0.1826,AUC: 0.9504,AP: 0.9723\n",
      "Epoch: 149, Loss: 0.1826,AUC: 0.9475,AP: 0.9708\n",
      "Epoch: 150, Loss: 0.1823,AUC: 0.9490,AP: 0.9720\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/f5.pt')\n",
    "data['edge_index'] = torch.tensor(data['edge_index'])\n",
    "data['edge_attr'] = torch.tensor(data['edge_attr'])\n",
    "data['t_start'] = torch.tensor(data['t_start'])\n",
    "data['num_nodes'] = 27045\n",
    "data['x'] = torch.randn((27045, 100))\n",
    "data = data.to(device, 'x', 'edge_index', 'edge_attr')\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=False)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "assert train_data.edge_index.max() < train_data.num_nodes  # !!!!!!!!\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    in_channels=train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([27045, 128])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z5 = model(train_data.x, train_data.edge_index)\n",
    "z5.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.2513,AUC: 0.9209,AP: 0.9487\n",
      "Epoch: 002, Loss: 0.1618,AUC: 0.9188,AP: 0.9479\n",
      "Epoch: 003, Loss: 0.1542,AUC: 0.9256,AP: 0.9518\n",
      "Epoch: 004, Loss: 0.1516,AUC: 0.9302,AP: 0.9548\n",
      "Epoch: 005, Loss: 0.1499,AUC: 0.9275,AP: 0.9536\n",
      "Epoch: 006, Loss: 0.1509,AUC: 0.9267,AP: 0.9535\n",
      "Epoch: 007, Loss: 0.1491,AUC: 0.9283,AP: 0.9549\n",
      "Epoch: 008, Loss: 0.1458,AUC: 0.9322,AP: 0.9574\n",
      "Epoch: 009, Loss: 0.1480,AUC: 0.9270,AP: 0.9549\n",
      "Epoch: 010, Loss: 0.1455,AUC: 0.9252,AP: 0.9541\n",
      "Epoch: 011, Loss: 0.1457,AUC: 0.9289,AP: 0.9563\n",
      "Epoch: 012, Loss: 0.1460,AUC: 0.9256,AP: 0.9548\n",
      "Epoch: 013, Loss: 0.1453,AUC: 0.9278,AP: 0.9561\n",
      "Epoch: 014, Loss: 0.1442,AUC: 0.9268,AP: 0.9558\n",
      "Epoch: 015, Loss: 0.1446,AUC: 0.9295,AP: 0.9576\n",
      "Epoch: 016, Loss: 0.1420,AUC: 0.9286,AP: 0.9573\n",
      "Epoch: 017, Loss: 0.1424,AUC: 0.9313,AP: 0.9590\n",
      "Epoch: 018, Loss: 0.1412,AUC: 0.9279,AP: 0.9571\n",
      "Epoch: 019, Loss: 0.1439,AUC: 0.9289,AP: 0.9577\n",
      "Epoch: 020, Loss: 0.1421,AUC: 0.9300,AP: 0.9585\n",
      "Epoch: 021, Loss: 0.1414,AUC: 0.9310,AP: 0.9592\n",
      "Epoch: 022, Loss: 0.1422,AUC: 0.9323,AP: 0.9601\n",
      "Epoch: 023, Loss: 0.1419,AUC: 0.9310,AP: 0.9595\n",
      "Epoch: 024, Loss: 0.1416,AUC: 0.9296,AP: 0.9589\n",
      "Epoch: 025, Loss: 0.1397,AUC: 0.9330,AP: 0.9607\n",
      "Epoch: 026, Loss: 0.1407,AUC: 0.9328,AP: 0.9608\n",
      "Epoch: 027, Loss: 0.1405,AUC: 0.9347,AP: 0.9619\n",
      "Epoch: 028, Loss: 0.1400,AUC: 0.9308,AP: 0.9599\n",
      "Epoch: 029, Loss: 0.1386,AUC: 0.9342,AP: 0.9617\n",
      "Epoch: 030, Loss: 0.1397,AUC: 0.9336,AP: 0.9616\n",
      "Epoch: 031, Loss: 0.1405,AUC: 0.9320,AP: 0.9609\n",
      "Epoch: 032, Loss: 0.1391,AUC: 0.9337,AP: 0.9617\n",
      "Epoch: 033, Loss: 0.1394,AUC: 0.9331,AP: 0.9615\n",
      "Epoch: 034, Loss: 0.1385,AUC: 0.9327,AP: 0.9613\n",
      "Epoch: 035, Loss: 0.1385,AUC: 0.9377,AP: 0.9639\n",
      "Epoch: 036, Loss: 0.1402,AUC: 0.9344,AP: 0.9624\n",
      "Epoch: 037, Loss: 0.1395,AUC: 0.9363,AP: 0.9634\n",
      "Epoch: 038, Loss: 0.1387,AUC: 0.9331,AP: 0.9620\n",
      "Epoch: 039, Loss: 0.1375,AUC: 0.9328,AP: 0.9619\n",
      "Epoch: 040, Loss: 0.1380,AUC: 0.9317,AP: 0.9613\n",
      "Epoch: 041, Loss: 0.1392,AUC: 0.9368,AP: 0.9640\n",
      "Epoch: 042, Loss: 0.1383,AUC: 0.9358,AP: 0.9634\n",
      "Epoch: 043, Loss: 0.1392,AUC: 0.9326,AP: 0.9620\n",
      "Epoch: 044, Loss: 0.1384,AUC: 0.9359,AP: 0.9635\n",
      "Epoch: 045, Loss: 0.1376,AUC: 0.9356,AP: 0.9633\n",
      "Epoch: 046, Loss: 0.1378,AUC: 0.9328,AP: 0.9620\n",
      "Epoch: 047, Loss: 0.1374,AUC: 0.9353,AP: 0.9635\n",
      "Epoch: 048, Loss: 0.1383,AUC: 0.9354,AP: 0.9635\n",
      "Epoch: 049, Loss: 0.1384,AUC: 0.9346,AP: 0.9632\n",
      "Epoch: 050, Loss: 0.1380,AUC: 0.9350,AP: 0.9633\n",
      "Epoch: 051, Loss: 0.1368,AUC: 0.9402,AP: 0.9661\n",
      "Epoch: 052, Loss: 0.1369,AUC: 0.9356,AP: 0.9636\n",
      "Epoch: 053, Loss: 0.1383,AUC: 0.9361,AP: 0.9638\n",
      "Epoch: 054, Loss: 0.1371,AUC: 0.9382,AP: 0.9649\n",
      "Epoch: 055, Loss: 0.1379,AUC: 0.9387,AP: 0.9651\n",
      "Epoch: 056, Loss: 0.1365,AUC: 0.9369,AP: 0.9640\n",
      "Epoch: 057, Loss: 0.1373,AUC: 0.9391,AP: 0.9653\n",
      "Epoch: 058, Loss: 0.1362,AUC: 0.9376,AP: 0.9642\n",
      "Epoch: 059, Loss: 0.1368,AUC: 0.9398,AP: 0.9654\n",
      "Epoch: 060, Loss: 0.1365,AUC: 0.9396,AP: 0.9654\n",
      "Epoch: 061, Loss: 0.1363,AUC: 0.9399,AP: 0.9657\n",
      "Epoch: 062, Loss: 0.1365,AUC: 0.9373,AP: 0.9645\n",
      "Epoch: 063, Loss: 0.1360,AUC: 0.9367,AP: 0.9643\n",
      "Epoch: 064, Loss: 0.1365,AUC: 0.9378,AP: 0.9650\n",
      "Epoch: 065, Loss: 0.1356,AUC: 0.9383,AP: 0.9651\n",
      "Epoch: 066, Loss: 0.1364,AUC: 0.9383,AP: 0.9653\n",
      "Epoch: 067, Loss: 0.1362,AUC: 0.9328,AP: 0.9624\n",
      "Epoch: 068, Loss: 0.1369,AUC: 0.9324,AP: 0.9620\n",
      "Epoch: 069, Loss: 0.1357,AUC: 0.9346,AP: 0.9632\n",
      "Epoch: 070, Loss: 0.1358,AUC: 0.9359,AP: 0.9639\n",
      "Epoch: 071, Loss: 0.1369,AUC: 0.9317,AP: 0.9618\n",
      "Epoch: 072, Loss: 0.1367,AUC: 0.9390,AP: 0.9652\n",
      "Epoch: 073, Loss: 0.1355,AUC: 0.9363,AP: 0.9638\n",
      "Epoch: 074, Loss: 0.1352,AUC: 0.9437,AP: 0.9683\n",
      "Epoch: 075, Loss: 0.1349,AUC: 0.9407,AP: 0.9667\n",
      "Epoch: 076, Loss: 0.1352,AUC: 0.9375,AP: 0.9651\n",
      "Epoch: 077, Loss: 0.1354,AUC: 0.9352,AP: 0.9639\n",
      "Epoch: 078, Loss: 0.1349,AUC: 0.9325,AP: 0.9626\n",
      "Epoch: 079, Loss: 0.1357,AUC: 0.9334,AP: 0.9631\n",
      "Epoch: 080, Loss: 0.1358,AUC: 0.9342,AP: 0.9633\n",
      "Epoch: 081, Loss: 0.1367,AUC: 0.9330,AP: 0.9627\n",
      "Epoch: 082, Loss: 0.1368,AUC: 0.9366,AP: 0.9647\n",
      "Epoch: 083, Loss: 0.1353,AUC: 0.9366,AP: 0.9647\n",
      "Epoch: 084, Loss: 0.1359,AUC: 0.9323,AP: 0.9624\n",
      "Epoch: 085, Loss: 0.1362,AUC: 0.9399,AP: 0.9663\n",
      "Epoch: 086, Loss: 0.1354,AUC: 0.9348,AP: 0.9636\n",
      "Epoch: 087, Loss: 0.1349,AUC: 0.9366,AP: 0.9645\n",
      "Epoch: 088, Loss: 0.1369,AUC: 0.9367,AP: 0.9647\n",
      "Epoch: 089, Loss: 0.1353,AUC: 0.9321,AP: 0.9624\n",
      "Epoch: 090, Loss: 0.1354,AUC: 0.9303,AP: 0.9617\n",
      "Epoch: 091, Loss: 0.1353,AUC: 0.9350,AP: 0.9641\n",
      "Epoch: 092, Loss: 0.1347,AUC: 0.9349,AP: 0.9643\n",
      "Epoch: 093, Loss: 0.1351,AUC: 0.9353,AP: 0.9648\n",
      "Epoch: 094, Loss: 0.1362,AUC: 0.9358,AP: 0.9649\n",
      "Epoch: 095, Loss: 0.1349,AUC: 0.9366,AP: 0.9651\n",
      "Epoch: 096, Loss: 0.1353,AUC: 0.9386,AP: 0.9661\n",
      "Epoch: 097, Loss: 0.1352,AUC: 0.9400,AP: 0.9667\n",
      "Epoch: 098, Loss: 0.1349,AUC: 0.9405,AP: 0.9669\n",
      "Epoch: 099, Loss: 0.1346,AUC: 0.9393,AP: 0.9660\n",
      "Epoch: 100, Loss: 0.1346,AUC: 0.9383,AP: 0.9654\n",
      "Epoch: 101, Loss: 0.1349,AUC: 0.9373,AP: 0.9652\n",
      "Epoch: 102, Loss: 0.1347,AUC: 0.9404,AP: 0.9670\n",
      "Epoch: 103, Loss: 0.1357,AUC: 0.9414,AP: 0.9675\n",
      "Epoch: 104, Loss: 0.1363,AUC: 0.9407,AP: 0.9672\n",
      "Epoch: 105, Loss: 0.1344,AUC: 0.9402,AP: 0.9668\n",
      "Epoch: 106, Loss: 0.1344,AUC: 0.9397,AP: 0.9668\n",
      "Epoch: 107, Loss: 0.1354,AUC: 0.9416,AP: 0.9677\n",
      "Epoch: 108, Loss: 0.1348,AUC: 0.9425,AP: 0.9681\n",
      "Epoch: 109, Loss: 0.1344,AUC: 0.9422,AP: 0.9678\n",
      "Epoch: 110, Loss: 0.1346,AUC: 0.9408,AP: 0.9671\n",
      "Epoch: 111, Loss: 0.1340,AUC: 0.9419,AP: 0.9681\n",
      "Epoch: 112, Loss: 0.1347,AUC: 0.9405,AP: 0.9672\n",
      "Epoch: 113, Loss: 0.1348,AUC: 0.9402,AP: 0.9673\n",
      "Epoch: 114, Loss: 0.1348,AUC: 0.9437,AP: 0.9692\n",
      "Epoch: 115, Loss: 0.1351,AUC: 0.9435,AP: 0.9693\n",
      "Epoch: 116, Loss: 0.1344,AUC: 0.9448,AP: 0.9700\n",
      "Epoch: 117, Loss: 0.1346,AUC: 0.9421,AP: 0.9681\n",
      "Epoch: 118, Loss: 0.1348,AUC: 0.9416,AP: 0.9678\n",
      "Epoch: 119, Loss: 0.1340,AUC: 0.9418,AP: 0.9681\n",
      "Epoch: 120, Loss: 0.1351,AUC: 0.9380,AP: 0.9659\n",
      "Epoch: 121, Loss: 0.1351,AUC: 0.9402,AP: 0.9670\n",
      "Epoch: 122, Loss: 0.1346,AUC: 0.9408,AP: 0.9675\n",
      "Epoch: 123, Loss: 0.1348,AUC: 0.9392,AP: 0.9665\n",
      "Epoch: 124, Loss: 0.1346,AUC: 0.9356,AP: 0.9645\n",
      "Epoch: 125, Loss: 0.1348,AUC: 0.9371,AP: 0.9654\n",
      "Epoch: 126, Loss: 0.1347,AUC: 0.9371,AP: 0.9651\n",
      "Epoch: 127, Loss: 0.1343,AUC: 0.9401,AP: 0.9672\n",
      "Epoch: 128, Loss: 0.1342,AUC: 0.9371,AP: 0.9656\n",
      "Epoch: 129, Loss: 0.1357,AUC: 0.9355,AP: 0.9647\n",
      "Epoch: 130, Loss: 0.1348,AUC: 0.9356,AP: 0.9647\n",
      "Epoch: 131, Loss: 0.1344,AUC: 0.9353,AP: 0.9646\n",
      "Epoch: 132, Loss: 0.1340,AUC: 0.9330,AP: 0.9638\n",
      "Epoch: 133, Loss: 0.1341,AUC: 0.9331,AP: 0.9640\n",
      "Epoch: 134, Loss: 0.1354,AUC: 0.9336,AP: 0.9641\n",
      "Epoch: 135, Loss: 0.1347,AUC: 0.9359,AP: 0.9649\n",
      "Epoch: 136, Loss: 0.1346,AUC: 0.9380,AP: 0.9658\n",
      "Epoch: 137, Loss: 0.1344,AUC: 0.9333,AP: 0.9636\n",
      "Epoch: 138, Loss: 0.1346,AUC: 0.9299,AP: 0.9620\n",
      "Epoch: 139, Loss: 0.1336,AUC: 0.9355,AP: 0.9654\n",
      "Epoch: 140, Loss: 0.1346,AUC: 0.9313,AP: 0.9632\n",
      "Epoch: 141, Loss: 0.1340,AUC: 0.9321,AP: 0.9638\n",
      "Epoch: 142, Loss: 0.1347,AUC: 0.9284,AP: 0.9615\n",
      "Epoch: 143, Loss: 0.1347,AUC: 0.9334,AP: 0.9641\n",
      "Epoch: 144, Loss: 0.1343,AUC: 0.9293,AP: 0.9618\n",
      "Epoch: 145, Loss: 0.1350,AUC: 0.9328,AP: 0.9632\n",
      "Epoch: 146, Loss: 0.1340,AUC: 0.9366,AP: 0.9653\n",
      "Epoch: 147, Loss: 0.1338,AUC: 0.9395,AP: 0.9672\n",
      "Epoch: 148, Loss: 0.1340,AUC: 0.9406,AP: 0.9677\n",
      "Epoch: 149, Loss: 0.1341,AUC: 0.9413,AP: 0.9684\n",
      "Epoch: 150, Loss: 0.1334,AUC: 0.9396,AP: 0.9669\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "pyg.seed_everything(3407)\n",
    "\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/f6.pt')\n",
    "data['edge_index'] = torch.tensor(data['edge_index'])\n",
    "data['edge_attr'] = torch.tensor(data['edge_attr'])\n",
    "data['t_start'] = torch.tensor(data['t_start'])\n",
    "data['num_nodes'] = 27045\n",
    "data['x'] = torch.randn((27045, 100))\n",
    "data = data.to(device, 'x', 'edge_index', 'edge_attr')\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.1,\n",
    "                            is_undirected=False)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "assert train_data.edge_index.max() < train_data.num_nodes  # !!!!!!!!\n",
    "train_data = train_data.cpu()  # 采样使用cpu\n",
    "loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    ")\n",
    "model = GraphSAGE(\n",
    "    in_channels=train_data.num_node_features,\n",
    "    hidden_channels=128,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        pred = (h[batch.edge_label_index[0]] * h[batch.edge_label_index[1]]).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    data = test_data.to(device)\n",
    "    z = model(data.x, data.edge_index)\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "for epoch in range(1, 151):\n",
    "    loss = train()\n",
    "    AUC, AP = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([27045, 128])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z6 = model(train_data.x, train_data.edge_index)\n",
    "z6.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([27045, 768])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_1_6 = torch.cat([z1, z2, z3, z4, z5, z6], 1)\n",
    "z_1_6.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "torch.save(z_1_6, '/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/z_1_6_gs.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8538])\n",
      "AUC: 0.8580,AP: 0.9205\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "# z_1_6=torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/z_1_6.pt')\n",
    "pyg.seed_everything(3407)\n",
    "device = 'cpu'\n",
    "\n",
    "data = torch.load('/home/chenjunfen/workspace/XZH/graphemb/dydata/rc_snapshots/f7.pt')\n",
    "data = data.to(device)\n",
    "data['edge_index'] = torch.tensor(data['edge_index'])\n",
    "data['edge_attr'] = torch.tensor(data['edge_attr'])\n",
    "data['t_start'] = torch.tensor(data['t_start'])\n",
    "data['num_nodes'] = 27045\n",
    "\n",
    "transform = RandomLinkSplit(num_val=0.0,\n",
    "                            num_test=0.0,\n",
    "                            is_undirected=False)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(train_data.edge_index.shape)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    data = train_data\n",
    "    z = z_1_6  # 输出潜在空间的嵌入向量\n",
    "    out = (z[data.edge_label_index[0]] * z[data.edge_label_index[1]]).sum(dim=-1)\n",
    "    out = out.view(-1).sigmoid()  # 重建edge_label与输入的边标签进行对比\n",
    "    AUC = roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    AP = average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    return AUC, AP\n",
    "\n",
    "\n",
    "AUC, AP = test()\n",
    "print(f'AUC: {AUC:.4f},AP: {AP:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
